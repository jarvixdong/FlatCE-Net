H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.022683821909496294
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L6_S9_Setup100_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L6_S9_Setup10_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}, 'trainer': {'optimizer': 'Adam', 'lr': 0.01, 'weight_decay': 0.001, 'lr_scheduler': 'Plateau', 'lr_gamma': 0.1, 'min_lr': 1e-06, 'patience': 20}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (53): ReLU(inplace=True)
      (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (56): ReLU(inplace=True)
      (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (59): ReLU(inplace=True)
      (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (62): ReLU(inplace=True)
      (63): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (65): ReLU(inplace=True)
      (66): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (68): ReLU(inplace=True)
      (69): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (71): ReLU(inplace=True)
      (72): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (73): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (74): ReLU(inplace=True)
      (75): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (76): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (77): ReLU(inplace=True)
      (78): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (79): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (80): ReLU(inplace=True)
      (81): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (82): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (83): ReLU(inplace=True)
      (84): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (85): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (86): ReLU(inplace=True)
      (87): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (88): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (89): ReLU(inplace=True)
      (90): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (91): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (92): ReLU(inplace=True)
      (93): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 12.73 MB
loss function:: L1Loss()
[2025-02-22 00:39:31] Epoch 1/100, Loss: 27.521139, Train_MMSE: 0.0472, NMMSE: 0.040811, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:40:54] Epoch 2/100, Loss: 27.621527, Train_MMSE: 0.047175, NMMSE: 0.040885, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:42:15] Epoch 3/100, Loss: 27.363623, Train_MMSE: 0.047139, NMMSE: 0.04074, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:43:38] Epoch 4/100, Loss: 27.453030, Train_MMSE: 0.046921, NMMSE: 0.040615, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:45:00] Epoch 5/100, Loss: 27.105631, Train_MMSE: 0.046566, NMMSE: 0.041332, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:46:21] Epoch 6/100, Loss: 27.249672, Train_MMSE: 0.046316, NMMSE: 0.041888, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:47:44] Epoch 7/100, Loss: 27.099800, Train_MMSE: 0.046105, NMMSE: 0.040907, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:49:07] Epoch 8/100, Loss: 27.091890, Train_MMSE: 0.045937, NMMSE: 0.040847, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:50:27] Epoch 9/100, Loss: 27.179104, Train_MMSE: 0.045841, NMMSE: 0.041863, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:51:49] Epoch 10/100, Loss: 27.232697, Train_MMSE: 0.045727, NMMSE: 0.039936, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:53:09] Epoch 11/100, Loss: 27.143671, Train_MMSE: 0.045632, NMMSE: 0.039993, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:54:31] Epoch 12/100, Loss: 26.874470, Train_MMSE: 0.045563, NMMSE: 0.040232, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:55:52] Epoch 13/100, Loss: 26.940208, Train_MMSE: 0.045496, NMMSE: 0.040894, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:57:14] Epoch 14/100, Loss: 26.984102, Train_MMSE: 0.045454, NMMSE: 0.040526, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:58:37] Epoch 15/100, Loss: 27.068951, Train_MMSE: 0.045387, NMMSE: 0.0405, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 00:59:59] Epoch 16/100, Loss: 26.759634, Train_MMSE: 0.045336, NMMSE: 0.040693, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:01:21] Epoch 17/100, Loss: 27.098934, Train_MMSE: 0.045221, NMMSE: 0.039808, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:02:43] Epoch 18/100, Loss: 26.632961, Train_MMSE: 0.045134, NMMSE: 0.040484, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:04:04] Epoch 19/100, Loss: 26.903631, Train_MMSE: 0.045046, NMMSE: 0.039721, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:05:26] Epoch 20/100, Loss: 26.960550, Train_MMSE: 0.044968, NMMSE: 0.040311, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:06:48] Epoch 21/100, Loss: 26.737730, Train_MMSE: 0.044901, NMMSE: 0.039963, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:08:09] Epoch 22/100, Loss: 26.840528, Train_MMSE: 0.044858, NMMSE: 0.039487, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:09:31] Epoch 23/100, Loss: 26.727495, Train_MMSE: 0.044819, NMMSE: 0.040754, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:10:52] Epoch 24/100, Loss: 26.710213, Train_MMSE: 0.044771, NMMSE: 0.040146, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 01:12:15] Epoch 25/100, Loss: 26.809320, Train_MMSE: 0.044763, NMMSE: 0.04089, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:13:38] Epoch 26/100, Loss: 26.677765, Train_MMSE: 0.044477, NMMSE: 0.03897, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:14:59] Epoch 27/100, Loss: 26.604778, Train_MMSE: 0.044416, NMMSE: 0.038996, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:16:21] Epoch 28/100, Loss: 26.575661, Train_MMSE: 0.044388, NMMSE: 0.038967, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:17:42] Epoch 29/100, Loss: 26.863703, Train_MMSE: 0.044362, NMMSE: 0.03894, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:19:01] Epoch 30/100, Loss: 26.702953, Train_MMSE: 0.044352, NMMSE: 0.039039, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:20:15] Epoch 31/100, Loss: 26.741577, Train_MMSE: 0.04433, NMMSE: 0.038987, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:21:27] Epoch 32/100, Loss: 26.917875, Train_MMSE: 0.044313, NMMSE: 0.038932, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:22:40] Epoch 33/100, Loss: 26.681387, Train_MMSE: 0.044297, NMMSE: 0.038914, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:23:54] Epoch 34/100, Loss: 26.490492, Train_MMSE: 0.044283, NMMSE: 0.038899, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:25:07] Epoch 35/100, Loss: 26.657566, Train_MMSE: 0.044271, NMMSE: 0.038928, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:26:20] Epoch 36/100, Loss: 26.595575, Train_MMSE: 0.044251, NMMSE: 0.038911, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:27:32] Epoch 37/100, Loss: 26.406221, Train_MMSE: 0.044234, NMMSE: 0.038926, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:28:37] Epoch 38/100, Loss: 26.516470, Train_MMSE: 0.044234, NMMSE: 0.038963, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:29:41] Epoch 39/100, Loss: 26.544031, Train_MMSE: 0.044212, NMMSE: 0.03891, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:30:44] Epoch 40/100, Loss: 26.543755, Train_MMSE: 0.044183, NMMSE: 0.038815, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:31:46] Epoch 41/100, Loss: 26.551348, Train_MMSE: 0.044159, NMMSE: 0.038867, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:32:49] Epoch 42/100, Loss: 26.468880, Train_MMSE: 0.044133, NMMSE: 0.038768, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:33:52] Epoch 43/100, Loss: 26.232868, Train_MMSE: 0.044086, NMMSE: 0.038719, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:34:55] Epoch 44/100, Loss: 26.600428, Train_MMSE: 0.044029, NMMSE: 0.039064, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:35:58] Epoch 45/100, Loss: 26.398445, Train_MMSE: 0.044008, NMMSE: 0.038695, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:37:01] Epoch 46/100, Loss: 26.708519, Train_MMSE: 0.043988, NMMSE: 0.03863, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:38:03] Epoch 47/100, Loss: 26.774973, Train_MMSE: 0.043955, NMMSE: 0.038713, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:38:44] Epoch 48/100, Loss: 26.881819, Train_MMSE: 0.043939, NMMSE: 0.038757, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:39:24] Epoch 49/100, Loss: 26.558657, Train_MMSE: 0.043915, NMMSE: 0.038675, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 01:40:04] Epoch 50/100, Loss: 26.674992, Train_MMSE: 0.043914, NMMSE: 0.038622, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:40:44] Epoch 51/100, Loss: 26.583611, Train_MMSE: 0.043817, NMMSE: 0.038532, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:41:24] Epoch 52/100, Loss: 26.356741, Train_MMSE: 0.043804, NMMSE: 0.038535, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:42:03] Epoch 53/100, Loss: 26.597172, Train_MMSE: 0.043805, NMMSE: 0.038552, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:42:43] Epoch 54/100, Loss: 26.568014, Train_MMSE: 0.043806, NMMSE: 0.038545, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:43:23] Epoch 55/100, Loss: 26.090349, Train_MMSE: 0.043799, NMMSE: 0.038536, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:44:03] Epoch 56/100, Loss: 26.389952, Train_MMSE: 0.043793, NMMSE: 0.038525, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:44:42] Epoch 57/100, Loss: 26.584986, Train_MMSE: 0.043791, NMMSE: 0.038525, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:45:21] Epoch 58/100, Loss: 26.490784, Train_MMSE: 0.043791, NMMSE: 0.038535, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:46:00] Epoch 59/100, Loss: 26.455513, Train_MMSE: 0.043783, NMMSE: 0.038542, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:46:40] Epoch 60/100, Loss: 26.191849, Train_MMSE: 0.043774, NMMSE: 0.038519, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:47:19] Epoch 61/100, Loss: 26.476688, Train_MMSE: 0.043781, NMMSE: 0.038526, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:47:58] Epoch 62/100, Loss: 26.571560, Train_MMSE: 0.043779, NMMSE: 0.038518, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:48:37] Epoch 63/100, Loss: 26.321522, Train_MMSE: 0.043773, NMMSE: 0.038516, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:49:17] Epoch 64/100, Loss: 26.550867, Train_MMSE: 0.043772, NMMSE: 0.038508, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:49:57] Epoch 65/100, Loss: 26.437531, Train_MMSE: 0.043769, NMMSE: 0.038516, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:50:36] Epoch 66/100, Loss: 26.564634, Train_MMSE: 0.043769, NMMSE: 0.038525, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:51:16] Epoch 67/100, Loss: 26.825129, Train_MMSE: 0.043766, NMMSE: 0.038505, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:51:54] Epoch 68/100, Loss: 26.501961, Train_MMSE: 0.04376, NMMSE: 0.038511, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:52:34] Epoch 69/100, Loss: 26.431042, Train_MMSE: 0.043759, NMMSE: 0.038504, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:53:13] Epoch 70/100, Loss: 26.449385, Train_MMSE: 0.043757, NMMSE: 0.038536, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:53:52] Epoch 71/100, Loss: 26.353205, Train_MMSE: 0.043755, NMMSE: 0.038518, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:54:30] Epoch 72/100, Loss: 26.516171, Train_MMSE: 0.043755, NMMSE: 0.03851, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:55:11] Epoch 73/100, Loss: 26.419107, Train_MMSE: 0.04375, NMMSE: 0.038526, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:55:50] Epoch 74/100, Loss: 26.541649, Train_MMSE: 0.043751, NMMSE: 0.038495, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 01:56:29] Epoch 75/100, Loss: 26.470148, Train_MMSE: 0.043746, NMMSE: 0.038493, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 01:57:07] Epoch 76/100, Loss: 26.666685, Train_MMSE: 0.043733, NMMSE: 0.038489, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 01:57:46] Epoch 77/100, Loss: 26.345804, Train_MMSE: 0.043732, NMMSE: 0.038486, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 01:58:25] Epoch 78/100, Loss: 26.487951, Train_MMSE: 0.043731, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 01:59:04] Epoch 79/100, Loss: 26.173544, Train_MMSE: 0.043735, NMMSE: 0.038486, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 01:59:44] Epoch 80/100, Loss: 26.552181, Train_MMSE: 0.043734, NMMSE: 0.038486, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:00:25] Epoch 81/100, Loss: 26.706179, Train_MMSE: 0.043735, NMMSE: 0.038488, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:01:05] Epoch 82/100, Loss: 26.363430, Train_MMSE: 0.043734, NMMSE: 0.038489, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:01:43] Epoch 83/100, Loss: 26.408400, Train_MMSE: 0.043732, NMMSE: 0.038489, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:02:22] Epoch 84/100, Loss: 26.486246, Train_MMSE: 0.043733, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:03:03] Epoch 85/100, Loss: 26.356949, Train_MMSE: 0.043732, NMMSE: 0.038488, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:03:42] Epoch 86/100, Loss: 26.573204, Train_MMSE: 0.043732, NMMSE: 0.038488, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:04:22] Epoch 87/100, Loss: 26.369867, Train_MMSE: 0.04373, NMMSE: 0.038485, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:05:02] Epoch 88/100, Loss: 26.576090, Train_MMSE: 0.043734, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:05:41] Epoch 89/100, Loss: 26.294262, Train_MMSE: 0.043729, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:06:21] Epoch 90/100, Loss: 26.672495, Train_MMSE: 0.043727, NMMSE: 0.038485, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:07:00] Epoch 91/100, Loss: 26.436817, Train_MMSE: 0.04373, NMMSE: 0.038488, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:07:40] Epoch 92/100, Loss: 26.627203, Train_MMSE: 0.043731, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:08:19] Epoch 93/100, Loss: 26.427734, Train_MMSE: 0.043728, NMMSE: 0.038487, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:08:58] Epoch 94/100, Loss: 26.213369, Train_MMSE: 0.04373, NMMSE: 0.038488, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:09:37] Epoch 95/100, Loss: 26.484734, Train_MMSE: 0.04373, NMMSE: 0.038484, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:10:18] Epoch 96/100, Loss: 26.308727, Train_MMSE: 0.043731, NMMSE: 0.038483, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:10:57] Epoch 97/100, Loss: 26.369654, Train_MMSE: 0.043726, NMMSE: 0.038484, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:11:39] Epoch 98/100, Loss: 26.478100, Train_MMSE: 0.043733, NMMSE: 0.038486, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:12:19] Epoch 99/100, Loss: 26.425459, Train_MMSE: 0.04373, NMMSE: 0.038483, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 02:12:57] Epoch 100/100, Loss: 26.364828, Train_MMSE: 0.043729, NMMSE: 0.038485, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
