H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.022683821909496294
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L6_S9_Setup100_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L6_S9_Setup10_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}, 'trainer': {'optimizer': 'Adam', 'lr': 0.01, 'weight_decay': 0.001, 'lr_scheduler': 'Plateau', 'lr_gamma': 0.1, 'min_lr': 1e-06, 'patience': 20}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
loss function:: L1Loss()
[2025-02-22 08:38:06] Epoch 1/250, Loss: 25.840519, Train_MMSE: 0.208915, NMMSE: 0.037679, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:38:21] Epoch 2/250, Loss: 22.723511, Train_MMSE: 0.035474, NMMSE: 0.029671, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:38:42] Epoch 3/250, Loss: 21.966284, Train_MMSE: 0.030862, NMMSE: 0.028326, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:39:13] Epoch 4/250, Loss: 21.724138, Train_MMSE: 0.029926, NMMSE: 0.027388, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:39:49] Epoch 5/250, Loss: 21.729876, Train_MMSE: 0.029453, NMMSE: 0.027051, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:40:34] Epoch 6/250, Loss: 21.694773, Train_MMSE: 0.029208, NMMSE: 0.027266, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:41:19] Epoch 7/250, Loss: 21.448521, Train_MMSE: 0.029078, NMMSE: 0.026749, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:42:08] Epoch 8/250, Loss: 21.445221, Train_MMSE: 0.028953, NMMSE: 0.026901, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:42:55] Epoch 9/250, Loss: 21.275703, Train_MMSE: 0.028841, NMMSE: 0.026204, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:43:43] Epoch 10/250, Loss: 21.517092, Train_MMSE: 0.029565, NMMSE: 0.026798, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:44:29] Epoch 11/250, Loss: 21.521433, Train_MMSE: 0.028707, NMMSE: 0.026297, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:45:17] Epoch 12/250, Loss: 21.417048, Train_MMSE: 0.028669, NMMSE: 0.025734, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:46:02] Epoch 13/250, Loss: 21.397161, Train_MMSE: 0.028613, NMMSE: 0.026322, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:46:50] Epoch 14/250, Loss: 21.689854, Train_MMSE: 0.028569, NMMSE: 0.026503, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:47:37] Epoch 15/250, Loss: 21.282906, Train_MMSE: 0.028756, NMMSE: 0.025941, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:48:23] Epoch 16/250, Loss: 21.767948, Train_MMSE: 0.028518, NMMSE: 0.02779, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:49:16] Epoch 17/250, Loss: 21.198421, Train_MMSE: 0.028524, NMMSE: 0.027931, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:50:04] Epoch 18/250, Loss: 21.527454, Train_MMSE: 0.028519, NMMSE: 0.026596, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:50:52] Epoch 19/250, Loss: 21.363476, Train_MMSE: 0.028506, NMMSE: 0.025772, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:51:39] Epoch 20/250, Loss: 21.434511, Train_MMSE: 0.028443, NMMSE: 0.02723, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:52:25] Epoch 21/250, Loss: 21.508615, Train_MMSE: 0.028508, NMMSE: 0.025742, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:53:14] Epoch 22/250, Loss: 21.522589, Train_MMSE: 0.028458, NMMSE: 0.026446, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:53:59] Epoch 23/250, Loss: 21.301737, Train_MMSE: 0.028428, NMMSE: 0.026233, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:54:49] Epoch 24/250, Loss: 21.397974, Train_MMSE: 0.028365, NMMSE: 0.027865, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:55:38] Epoch 25/250, Loss: 21.230743, Train_MMSE: 0.028423, NMMSE: 0.026253, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:56:26] Epoch 26/250, Loss: 21.314857, Train_MMSE: 0.028331, NMMSE: 0.027477, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:57:14] Epoch 27/250, Loss: 21.289139, Train_MMSE: 0.028395, NMMSE: 0.026508, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:58:02] Epoch 28/250, Loss: 22.115364, Train_MMSE: 0.031432, NMMSE: 0.028525, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:58:47] Epoch 29/250, Loss: 21.807962, Train_MMSE: 0.029243, NMMSE: 0.026833, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:59:41] Epoch 30/250, Loss: 21.587162, Train_MMSE: 0.028797, NMMSE: 0.026053, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:00:33] Epoch 31/250, Loss: 21.357141, Train_MMSE: 0.028576, NMMSE: 0.026067, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:01:23] Epoch 32/250, Loss: 21.283457, Train_MMSE: 0.028496, NMMSE: 0.02598, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:02:09] Epoch 33/250, Loss: 21.767883, Train_MMSE: 0.028415, NMMSE: 0.026564, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:02:55] Epoch 34/250, Loss: 21.264292, Train_MMSE: 0.028449, NMMSE: 0.025997, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:03:47] Epoch 35/250, Loss: 21.230131, Train_MMSE: 0.028396, NMMSE: 0.025631, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:04:35] Epoch 36/250, Loss: 21.426680, Train_MMSE: 0.028387, NMMSE: 0.026933, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:05:24] Epoch 37/250, Loss: 21.545595, Train_MMSE: 0.028361, NMMSE: 0.026754, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:06:11] Epoch 38/250, Loss: 21.297899, Train_MMSE: 0.028367, NMMSE: 0.027807, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:06:56] Epoch 39/250, Loss: 21.382029, Train_MMSE: 0.028409, NMMSE: 0.025798, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:07:43] Epoch 40/250, Loss: 21.139921, Train_MMSE: 0.028293, NMMSE: 0.025583, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:08:28] Epoch 41/250, Loss: 21.511227, Train_MMSE: 0.028369, NMMSE: 0.026602, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:09:14] Epoch 42/250, Loss: 21.972572, Train_MMSE: 0.028323, NMMSE: 0.02684, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:10:00] Epoch 43/250, Loss: 21.419088, Train_MMSE: 0.028281, NMMSE: 0.026067, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:10:48] Epoch 44/250, Loss: 21.499094, Train_MMSE: 0.028318, NMMSE: 0.025439, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:11:33] Epoch 45/250, Loss: 21.447077, Train_MMSE: 0.028251, NMMSE: 0.02693, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:12:22] Epoch 46/250, Loss: 21.506037, Train_MMSE: 0.028428, NMMSE: 0.026772, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:13:07] Epoch 47/250, Loss: 21.583046, Train_MMSE: 0.028344, NMMSE: 0.026494, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:13:55] Epoch 48/250, Loss: 21.251011, Train_MMSE: 0.028275, NMMSE: 0.026049, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:14:42] Epoch 49/250, Loss: 21.244467, Train_MMSE: 0.028307, NMMSE: 0.033804, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:15:27] Epoch 50/250, Loss: 21.308689, Train_MMSE: 0.028217, NMMSE: 0.026184, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:16:16] Epoch 51/250, Loss: 21.045797, Train_MMSE: 0.028218, NMMSE: 0.026049, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:17:05] Epoch 52/250, Loss: 21.255142, Train_MMSE: 0.028178, NMMSE: 0.026545, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:17:51] Epoch 53/250, Loss: 21.355934, Train_MMSE: 0.028335, NMMSE: 0.026114, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:18:41] Epoch 54/250, Loss: 21.171453, Train_MMSE: 0.028164, NMMSE: 0.033787, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:19:26] Epoch 55/250, Loss: 21.217461, Train_MMSE: 0.028237, NMMSE: 0.02607, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:20:15] Epoch 56/250, Loss: 21.425135, Train_MMSE: 0.02901, NMMSE: 0.027716, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:21:00] Epoch 57/250, Loss: 21.224615, Train_MMSE: 0.028344, NMMSE: 0.025896, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:21:46] Epoch 58/250, Loss: 21.379553, Train_MMSE: 0.02811, NMMSE: 0.025811, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:22:33] Epoch 59/250, Loss: 21.219374, Train_MMSE: 0.028213, NMMSE: 0.026089, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:23:18] Epoch 60/250, Loss: 21.253967, Train_MMSE: 0.028209, NMMSE: 0.02653, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:24:07] Epoch 61/250, Loss: 21.159836, Train_MMSE: 0.027423, NMMSE: 0.024489, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:24:55] Epoch 62/250, Loss: 20.954865, Train_MMSE: 0.027366, NMMSE: 0.024511, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:25:45] Epoch 63/250, Loss: 20.864317, Train_MMSE: 0.027336, NMMSE: 0.024493, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:26:29] Epoch 64/250, Loss: 20.828897, Train_MMSE: 0.027323, NMMSE: 0.024493, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:27:15] Epoch 65/250, Loss: 20.951447, Train_MMSE: 0.027338, NMMSE: 0.024558, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:28:03] Epoch 66/250, Loss: 21.030687, Train_MMSE: 0.02734, NMMSE: 0.02451, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:28:52] Epoch 67/250, Loss: 20.995022, Train_MMSE: 0.02732, NMMSE: 0.024513, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:29:44] Epoch 68/250, Loss: 21.027231, Train_MMSE: 0.027314, NMMSE: 0.024515, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:30:33] Epoch 69/250, Loss: 20.993553, Train_MMSE: 0.027319, NMMSE: 0.024531, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:31:21] Epoch 70/250, Loss: 21.168251, Train_MMSE: 0.027307, NMMSE: 0.024703, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:32:10] Epoch 71/250, Loss: 20.917843, Train_MMSE: 0.027318, NMMSE: 0.024465, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:33:02] Epoch 72/250, Loss: 20.989756, Train_MMSE: 0.027306, NMMSE: 0.024493, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:33:49] Epoch 73/250, Loss: 21.042074, Train_MMSE: 0.027304, NMMSE: 0.024447, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:34:39] Epoch 74/250, Loss: 21.052660, Train_MMSE: 0.02731, NMMSE: 0.024634, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:35:24] Epoch 75/250, Loss: 21.174198, Train_MMSE: 0.027332, NMMSE: 0.024506, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:36:10] Epoch 76/250, Loss: 20.849165, Train_MMSE: 0.02731, NMMSE: 0.024574, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:36:59] Epoch 77/250, Loss: 20.914831, Train_MMSE: 0.027295, NMMSE: 0.024539, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:37:47] Epoch 78/250, Loss: 21.287313, Train_MMSE: 0.027298, NMMSE: 0.024485, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:38:33] Epoch 79/250, Loss: 21.125893, Train_MMSE: 0.02733, NMMSE: 0.024503, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:39:18] Epoch 80/250, Loss: 20.859684, Train_MMSE: 0.027295, NMMSE: 0.02446, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:40:07] Epoch 81/250, Loss: 20.915529, Train_MMSE: 0.027284, NMMSE: 0.024514, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:40:52] Epoch 82/250, Loss: 20.772942, Train_MMSE: 0.027313, NMMSE: 0.024573, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:41:40] Epoch 83/250, Loss: 20.868126, Train_MMSE: 0.027305, NMMSE: 0.024622, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:42:27] Epoch 84/250, Loss: 20.970726, Train_MMSE: 0.027277, NMMSE: 0.024577, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:43:15] Epoch 85/250, Loss: 20.855415, Train_MMSE: 0.027323, NMMSE: 0.024686, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:44:01] Epoch 86/250, Loss: 21.106951, Train_MMSE: 0.027303, NMMSE: 0.024692, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:44:50] Epoch 87/250, Loss: 21.038898, Train_MMSE: 0.027317, NMMSE: 0.026111, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:45:38] Epoch 88/250, Loss: 21.237095, Train_MMSE: 0.027305, NMMSE: 0.024514, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:46:28] Epoch 89/250, Loss: 20.978621, Train_MMSE: 0.0273, NMMSE: 0.024544, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:47:15] Epoch 90/250, Loss: 20.847294, Train_MMSE: 0.02728, NMMSE: 0.024471, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:48:06] Epoch 91/250, Loss: 20.932796, Train_MMSE: 0.027281, NMMSE: 0.027112, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:48:56] Epoch 92/250, Loss: 20.944780, Train_MMSE: 0.027292, NMMSE: 0.025514, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:49:43] Epoch 93/250, Loss: 21.097746, Train_MMSE: 0.027305, NMMSE: 0.026904, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:50:33] Epoch 94/250, Loss: 20.938379, Train_MMSE: 0.027284, NMMSE: 0.034317, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:51:19] Epoch 95/250, Loss: 20.805580, Train_MMSE: 0.027274, NMMSE: 0.030547, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:52:07] Epoch 96/250, Loss: 20.833117, Train_MMSE: 0.027276, NMMSE: 0.024684, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:52:54] Epoch 97/250, Loss: 21.112265, Train_MMSE: 0.027273, NMMSE: 0.024632, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:53:42] Epoch 98/250, Loss: 20.856689, Train_MMSE: 0.027272, NMMSE: 0.024915, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:54:31] Epoch 99/250, Loss: 20.927959, Train_MMSE: 0.02729, NMMSE: 0.024645, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:55:18] Epoch 100/250, Loss: 20.626543, Train_MMSE: 0.027295, NMMSE: 0.024492, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:56:04] Epoch 101/250, Loss: 20.958296, Train_MMSE: 0.029846, NMMSE: 0.024872, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:56:54] Epoch 102/250, Loss: 21.104771, Train_MMSE: 0.027351, NMMSE: 0.024676, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:57:42] Epoch 103/250, Loss: 21.141708, Train_MMSE: 0.027301, NMMSE: 0.024554, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:58:31] Epoch 104/250, Loss: 20.896509, Train_MMSE: 0.027282, NMMSE: 0.024611, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 09:59:20] Epoch 105/250, Loss: 20.946636, Train_MMSE: 0.02728, NMMSE: 0.02451, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:00:11] Epoch 106/250, Loss: 21.034826, Train_MMSE: 0.027289, NMMSE: 0.024583, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:00:59] Epoch 107/250, Loss: 20.958139, Train_MMSE: 0.027277, NMMSE: 0.024589, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:01:53] Epoch 108/250, Loss: 21.018961, Train_MMSE: 0.027283, NMMSE: 0.024701, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:02:41] Epoch 109/250, Loss: 20.883881, Train_MMSE: 0.027283, NMMSE: 0.024515, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:03:26] Epoch 110/250, Loss: 21.025715, Train_MMSE: 0.027278, NMMSE: 0.024555, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:04:12] Epoch 111/250, Loss: 20.795004, Train_MMSE: 0.027267, NMMSE: 0.024421, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:05:04] Epoch 112/250, Loss: 21.010609, Train_MMSE: 0.027302, NMMSE: 0.024458, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:05:50] Epoch 113/250, Loss: 20.812691, Train_MMSE: 0.027246, NMMSE: 0.024592, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:06:37] Epoch 114/250, Loss: 21.019573, Train_MMSE: 0.027295, NMMSE: 0.024544, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:07:24] Epoch 115/250, Loss: 21.129383, Train_MMSE: 0.027288, NMMSE: 0.024502, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:08:14] Epoch 116/250, Loss: 20.881372, Train_MMSE: 0.027268, NMMSE: 0.024574, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:09:01] Epoch 117/250, Loss: 20.867306, Train_MMSE: 0.027267, NMMSE: 0.024527, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:09:47] Epoch 118/250, Loss: 20.955130, Train_MMSE: 0.027267, NMMSE: 0.02456, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:10:32] Epoch 119/250, Loss: 20.966942, Train_MMSE: 0.027249, NMMSE: 0.024582, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:11:17] Epoch 120/250, Loss: 21.077686, Train_MMSE: 0.027292, NMMSE: 0.02454, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:12:06] Epoch 121/250, Loss: 20.913210, Train_MMSE: 0.02712, NMMSE: 0.024296, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:12:53] Epoch 122/250, Loss: 20.714085, Train_MMSE: 0.027104, NMMSE: 0.024309, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:13:40] Epoch 123/250, Loss: 20.878580, Train_MMSE: 0.027097, NMMSE: 0.024287, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:14:27] Epoch 124/250, Loss: 20.866594, Train_MMSE: 0.027095, NMMSE: 0.024286, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:15:12] Epoch 125/250, Loss: 20.836609, Train_MMSE: 0.027095, NMMSE: 0.024296, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:16:03] Epoch 126/250, Loss: 20.856131, Train_MMSE: 0.027095, NMMSE: 0.024285, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:16:51] Epoch 127/250, Loss: 20.893440, Train_MMSE: 0.027089, NMMSE: 0.02429, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:17:37] Epoch 128/250, Loss: 20.824366, Train_MMSE: 0.027093, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:18:25] Epoch 129/250, Loss: 20.819908, Train_MMSE: 0.027082, NMMSE: 0.024279, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:19:12] Epoch 130/250, Loss: 20.751078, Train_MMSE: 0.027092, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:19:58] Epoch 131/250, Loss: 20.774698, Train_MMSE: 0.027074, NMMSE: 0.024286, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:20:44] Epoch 132/250, Loss: 20.807083, Train_MMSE: 0.027089, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:21:31] Epoch 133/250, Loss: 20.882349, Train_MMSE: 0.027083, NMMSE: 0.024277, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:22:16] Epoch 134/250, Loss: 20.928995, Train_MMSE: 0.027077, NMMSE: 0.024287, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:23:01] Epoch 135/250, Loss: 20.899427, Train_MMSE: 0.027077, NMMSE: 0.024291, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:23:50] Epoch 136/250, Loss: 20.865458, Train_MMSE: 0.027095, NMMSE: 0.024292, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:24:37] Epoch 137/250, Loss: 20.737679, Train_MMSE: 0.027079, NMMSE: 0.024274, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:25:27] Epoch 138/250, Loss: 21.094866, Train_MMSE: 0.027079, NMMSE: 0.024293, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:26:14] Epoch 139/250, Loss: 20.792419, Train_MMSE: 0.027059, NMMSE: 0.024314, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:27:03] Epoch 140/250, Loss: 20.998152, Train_MMSE: 0.027081, NMMSE: 0.02428, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:27:49] Epoch 141/250, Loss: 20.811348, Train_MMSE: 0.027058, NMMSE: 0.024287, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:28:37] Epoch 142/250, Loss: 20.688232, Train_MMSE: 0.027091, NMMSE: 0.024284, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:29:25] Epoch 143/250, Loss: 20.744013, Train_MMSE: 0.027076, NMMSE: 0.02428, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:30:11] Epoch 144/250, Loss: 20.907711, Train_MMSE: 0.027053, NMMSE: 0.02429, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:30:58] Epoch 145/250, Loss: 20.785830, Train_MMSE: 0.027079, NMMSE: 0.024293, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:31:44] Epoch 146/250, Loss: 20.870161, Train_MMSE: 0.027061, NMMSE: 0.024285, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:32:30] Epoch 147/250, Loss: 20.722071, Train_MMSE: 0.02707, NMMSE: 0.024285, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:33:16] Epoch 148/250, Loss: 20.857895, Train_MMSE: 0.027079, NMMSE: 0.024277, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:34:03] Epoch 149/250, Loss: 20.762747, Train_MMSE: 0.027082, NMMSE: 0.024275, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:34:53] Epoch 150/250, Loss: 20.822428, Train_MMSE: 0.027068, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:35:40] Epoch 151/250, Loss: 20.766678, Train_MMSE: 0.027089, NMMSE: 0.024274, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:36:31] Epoch 152/250, Loss: 20.979609, Train_MMSE: 0.027074, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:37:18] Epoch 153/250, Loss: 20.755995, Train_MMSE: 0.027081, NMMSE: 0.024283, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:38:03] Epoch 154/250, Loss: 20.807655, Train_MMSE: 0.027085, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:38:52] Epoch 155/250, Loss: 20.809692, Train_MMSE: 0.027078, NMMSE: 0.024293, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:39:40] Epoch 156/250, Loss: 20.688705, Train_MMSE: 0.027078, NMMSE: 0.02428, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:40:26] Epoch 157/250, Loss: 20.989758, Train_MMSE: 0.027081, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:41:16] Epoch 158/250, Loss: 20.981159, Train_MMSE: 0.027074, NMMSE: 0.024283, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:42:02] Epoch 159/250, Loss: 20.917475, Train_MMSE: 0.027084, NMMSE: 0.02429, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:42:48] Epoch 160/250, Loss: 20.885563, Train_MMSE: 0.027058, NMMSE: 0.024281, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:43:38] Epoch 161/250, Loss: 20.896875, Train_MMSE: 0.027073, NMMSE: 0.02428, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:44:25] Epoch 162/250, Loss: 20.832621, Train_MMSE: 0.027075, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:45:13] Epoch 163/250, Loss: 20.677071, Train_MMSE: 0.027059, NMMSE: 0.024294, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:45:59] Epoch 164/250, Loss: 20.921543, Train_MMSE: 0.027071, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:46:48] Epoch 165/250, Loss: 20.728676, Train_MMSE: 0.027086, NMMSE: 0.024311, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:47:34] Epoch 166/250, Loss: 20.709461, Train_MMSE: 0.02706, NMMSE: 0.024276, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:48:28] Epoch 167/250, Loss: 20.881775, Train_MMSE: 0.027045, NMMSE: 0.024311, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:49:15] Epoch 168/250, Loss: 20.807743, Train_MMSE: 0.027079, NMMSE: 0.024283, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:50:03] Epoch 169/250, Loss: 20.984100, Train_MMSE: 0.027065, NMMSE: 0.024289, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:50:55] Epoch 170/250, Loss: 20.808584, Train_MMSE: 0.027062, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:51:41] Epoch 171/250, Loss: 20.889811, Train_MMSE: 0.027056, NMMSE: 0.02428, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:52:30] Epoch 172/250, Loss: 20.928341, Train_MMSE: 0.027091, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:53:19] Epoch 173/250, Loss: 20.999876, Train_MMSE: 0.027077, NMMSE: 0.024323, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:54:06] Epoch 174/250, Loss: 20.927223, Train_MMSE: 0.027088, NMMSE: 0.024285, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:54:53] Epoch 175/250, Loss: 20.790609, Train_MMSE: 0.027065, NMMSE: 0.024288, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:55:42] Epoch 176/250, Loss: 20.749201, Train_MMSE: 0.027072, NMMSE: 0.024277, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:56:36] Epoch 177/250, Loss: 20.745388, Train_MMSE: 0.027058, NMMSE: 0.024283, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:57:23] Epoch 178/250, Loss: 20.990198, Train_MMSE: 0.027059, NMMSE: 0.024278, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:58:12] Epoch 179/250, Loss: 20.765171, Train_MMSE: 0.027066, NMMSE: 0.024271, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 10:59:00] Epoch 180/250, Loss: 20.919239, Train_MMSE: 0.027059, NMMSE: 0.024282, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 10:59:49] Epoch 181/250, Loss: 20.720533, Train_MMSE: 0.027031, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:00:41] Epoch 182/250, Loss: 20.887854, Train_MMSE: 0.027036, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:01:32] Epoch 183/250, Loss: 20.826456, Train_MMSE: 0.027017, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:02:22] Epoch 184/250, Loss: 20.790623, Train_MMSE: 0.027028, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:03:09] Epoch 185/250, Loss: 20.798288, Train_MMSE: 0.027035, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:03:57] Epoch 186/250, Loss: 20.678751, Train_MMSE: 0.027038, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:04:44] Epoch 187/250, Loss: 20.755602, Train_MMSE: 0.027041, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:05:30] Epoch 188/250, Loss: 20.705393, Train_MMSE: 0.027047, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:06:15] Epoch 189/250, Loss: 20.874792, Train_MMSE: 0.027066, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:07:03] Epoch 190/250, Loss: 20.847786, Train_MMSE: 0.027045, NMMSE: 0.024265, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:07:56] Epoch 191/250, Loss: 20.686045, Train_MMSE: 0.027032, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:08:41] Epoch 192/250, Loss: 20.970436, Train_MMSE: 0.027036, NMMSE: 0.024261, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:09:27] Epoch 193/250, Loss: 21.031984, Train_MMSE: 0.027016, NMMSE: 0.02426, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:10:13] Epoch 194/250, Loss: 20.850290, Train_MMSE: 0.027039, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:10:59] Epoch 195/250, Loss: 20.822714, Train_MMSE: 0.02702, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:11:47] Epoch 196/250, Loss: 20.550531, Train_MMSE: 0.027028, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:12:33] Epoch 197/250, Loss: 20.780800, Train_MMSE: 0.027038, NMMSE: 0.024271, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:13:17] Epoch 198/250, Loss: 20.674892, Train_MMSE: 0.02705, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:14:04] Epoch 199/250, Loss: 20.770874, Train_MMSE: 0.027029, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:14:50] Epoch 200/250, Loss: 21.024508, Train_MMSE: 0.027044, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:15:34] Epoch 201/250, Loss: 20.935331, Train_MMSE: 0.027045, NMMSE: 0.024274, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:16:23] Epoch 202/250, Loss: 20.609941, Train_MMSE: 0.027038, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:17:08] Epoch 203/250, Loss: 20.707729, Train_MMSE: 0.027077, NMMSE: 0.024261, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:17:54] Epoch 204/250, Loss: 20.664244, Train_MMSE: 0.027063, NMMSE: 0.024275, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:18:44] Epoch 205/250, Loss: 20.935026, Train_MMSE: 0.027029, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:19:29] Epoch 206/250, Loss: 20.909271, Train_MMSE: 0.027042, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:20:22] Epoch 207/250, Loss: 20.801424, Train_MMSE: 0.02704, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:21:10] Epoch 208/250, Loss: 20.613592, Train_MMSE: 0.02704, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:21:58] Epoch 209/250, Loss: 20.949730, Train_MMSE: 0.027033, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:22:42] Epoch 210/250, Loss: 20.930859, Train_MMSE: 0.027026, NMMSE: 0.02426, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:23:28] Epoch 211/250, Loss: 20.923883, Train_MMSE: 0.027051, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:24:14] Epoch 212/250, Loss: 20.662703, Train_MMSE: 0.027022, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:24:59] Epoch 213/250, Loss: 20.711037, Train_MMSE: 0.027041, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:25:48] Epoch 214/250, Loss: 20.825335, Train_MMSE: 0.027048, NMMSE: 0.024269, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:26:34] Epoch 215/250, Loss: 21.018141, Train_MMSE: 0.027048, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:27:19] Epoch 216/250, Loss: 20.938385, Train_MMSE: 0.02703, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:28:08] Epoch 217/250, Loss: 20.680124, Train_MMSE: 0.027014, NMMSE: 0.024255, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:28:58] Epoch 218/250, Loss: 20.835737, Train_MMSE: 0.027034, NMMSE: 0.024285, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:29:44] Epoch 219/250, Loss: 20.865593, Train_MMSE: 0.027049, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:30:29] Epoch 220/250, Loss: 21.084377, Train_MMSE: 0.027064, NMMSE: 0.024256, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:31:14] Epoch 221/250, Loss: 20.752186, Train_MMSE: 0.027033, NMMSE: 0.024261, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:32:01] Epoch 222/250, Loss: 20.917086, Train_MMSE: 0.027018, NMMSE: 0.024267, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:32:47] Epoch 223/250, Loss: 20.796499, Train_MMSE: 0.027042, NMMSE: 0.024255, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:33:33] Epoch 224/250, Loss: 20.662943, Train_MMSE: 0.027052, NMMSE: 0.024266, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:34:19] Epoch 225/250, Loss: 21.001753, Train_MMSE: 0.027036, NMMSE: 0.024264, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:35:05] Epoch 226/250, Loss: 20.797874, Train_MMSE: 0.027037, NMMSE: 0.024266, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:35:51] Epoch 227/250, Loss: 20.912716, Train_MMSE: 0.027021, NMMSE: 0.024258, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:36:40] Epoch 228/250, Loss: 20.949841, Train_MMSE: 0.027039, NMMSE: 0.024257, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:37:29] Epoch 229/250, Loss: 20.778524, Train_MMSE: 0.027043, NMMSE: 0.024263, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:38:16] Epoch 230/250, Loss: 20.819016, Train_MMSE: 0.027042, NMMSE: 0.024269, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:39:04] Epoch 231/250, Loss: 21.003883, Train_MMSE: 0.027023, NMMSE: 0.024272, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 11:39:51] Epoch 232/250, Loss: 20.801559, Train_MMSE: 0.027044, NMMSE: 0.024259, LS_NMSE: 0.040811, Lr: 1e-05
