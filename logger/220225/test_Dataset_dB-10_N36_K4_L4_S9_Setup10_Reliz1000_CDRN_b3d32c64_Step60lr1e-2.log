H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.024458686477191807
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S9_Setup100_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S9_Setup10_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}, 'trainer': {'optimizer': 'Adam', 'lr': 0.01, 'weight_decay': 0.001, 'lr_scheduler': 'Plateau', 'lr_gamma': 0.1, 'min_lr': 1e-06, 'patience': 20}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (53): ReLU(inplace=True)
      (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (56): ReLU(inplace=True)
      (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (59): ReLU(inplace=True)
      (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (62): ReLU(inplace=True)
      (63): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (65): ReLU(inplace=True)
      (66): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (68): ReLU(inplace=True)
      (69): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (71): ReLU(inplace=True)
      (72): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (73): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (74): ReLU(inplace=True)
      (75): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (76): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (77): ReLU(inplace=True)
      (78): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (79): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (80): ReLU(inplace=True)
      (81): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (82): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (83): ReLU(inplace=True)
      (84): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (85): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (86): ReLU(inplace=True)
      (87): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (88): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (89): ReLU(inplace=True)
      (90): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (91): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (92): ReLU(inplace=True)
      (93): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 12.73 MB
loss function:: L1Loss()
[2025-02-22 08:29:15] Epoch 1/250, Loss: 28.120050, Train_MMSE: 0.047485, NMMSE: 0.040632, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:30:57] Epoch 2/250, Loss: 27.905689, Train_MMSE: 0.047473, NMMSE: 0.040619, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:32:39] Epoch 3/250, Loss: 27.970638, Train_MMSE: 0.047436, NMMSE: 0.040587, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:34:20] Epoch 4/250, Loss: 27.869972, Train_MMSE: 0.047374, NMMSE: 0.040542, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:36:01] Epoch 5/250, Loss: 27.826040, Train_MMSE: 0.047267, NMMSE: 0.040539, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:37:43] Epoch 6/250, Loss: 27.932840, Train_MMSE: 0.047056, NMMSE: 0.040578, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:39:25] Epoch 7/250, Loss: 27.858698, Train_MMSE: 0.046967, NMMSE: 0.040947, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:41:07] Epoch 8/250, Loss: 27.918459, Train_MMSE: 0.046915, NMMSE: 0.040441, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:42:49] Epoch 9/250, Loss: 27.965080, Train_MMSE: 0.046877, NMMSE: 0.04027, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:44:31] Epoch 10/250, Loss: 27.982536, Train_MMSE: 0.046854, NMMSE: 0.040846, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:46:12] Epoch 11/250, Loss: 27.721121, Train_MMSE: 0.04684, NMMSE: 0.040147, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:47:56] Epoch 12/250, Loss: 27.903055, Train_MMSE: 0.046819, NMMSE: 0.040135, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:49:38] Epoch 13/250, Loss: 27.754267, Train_MMSE: 0.0468, NMMSE: 0.040091, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:51:22] Epoch 14/250, Loss: 28.172951, Train_MMSE: 0.046785, NMMSE: 0.040243, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:53:06] Epoch 15/250, Loss: 27.808506, Train_MMSE: 0.046761, NMMSE: 0.040429, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:54:50] Epoch 16/250, Loss: 27.652632, Train_MMSE: 0.046726, NMMSE: 0.04165, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:56:32] Epoch 17/250, Loss: 27.806938, Train_MMSE: 0.046697, NMMSE: 0.04012, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:58:15] Epoch 18/250, Loss: 27.708597, Train_MMSE: 0.046672, NMMSE: 0.040336, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 08:59:57] Epoch 19/250, Loss: 28.144770, Train_MMSE: 0.046635, NMMSE: 0.040653, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:01:39] Epoch 20/250, Loss: 27.601940, Train_MMSE: 0.04662, NMMSE: 0.040216, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:03:20] Epoch 21/250, Loss: 27.777775, Train_MMSE: 0.046588, NMMSE: 0.039981, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:05:02] Epoch 22/250, Loss: 27.883621, Train_MMSE: 0.046574, NMMSE: 0.040374, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:06:44] Epoch 23/250, Loss: 28.097038, Train_MMSE: 0.046543, NMMSE: 0.041329, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:08:26] Epoch 24/250, Loss: 27.714594, Train_MMSE: 0.046522, NMMSE: 0.040417, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:10:10] Epoch 25/250, Loss: 27.823503, Train_MMSE: 0.046514, NMMSE: 0.040651, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:11:50] Epoch 26/250, Loss: 27.641867, Train_MMSE: 0.046471, NMMSE: 0.040652, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:13:33] Epoch 27/250, Loss: 27.696112, Train_MMSE: 0.046423, NMMSE: 0.040151, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:15:15] Epoch 28/250, Loss: 27.786047, Train_MMSE: 0.04638, NMMSE: 0.039834, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:16:58] Epoch 29/250, Loss: 27.791996, Train_MMSE: 0.046319, NMMSE: 0.040812, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:18:42] Epoch 30/250, Loss: 27.737392, Train_MMSE: 0.046284, NMMSE: 0.040161, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:20:24] Epoch 31/250, Loss: 27.861589, Train_MMSE: 0.046237, NMMSE: 0.03982, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:22:07] Epoch 32/250, Loss: 27.755140, Train_MMSE: 0.04619, NMMSE: 0.040615, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:23:49] Epoch 33/250, Loss: 27.469740, Train_MMSE: 0.046153, NMMSE: 0.039978, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:25:32] Epoch 34/250, Loss: 27.272427, Train_MMSE: 0.0461, NMMSE: 0.039769, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:27:11] Epoch 35/250, Loss: 27.434408, Train_MMSE: 0.046081, NMMSE: 0.039679, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:28:52] Epoch 36/250, Loss: 27.525049, Train_MMSE: 0.046028, NMMSE: 0.039716, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:30:34] Epoch 37/250, Loss: 27.646919, Train_MMSE: 0.046004, NMMSE: 0.043942, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:32:16] Epoch 38/250, Loss: 27.730509, Train_MMSE: 0.045967, NMMSE: 0.039993, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:33:59] Epoch 39/250, Loss: 27.577158, Train_MMSE: 0.045927, NMMSE: 0.041946, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:35:38] Epoch 40/250, Loss: 27.593636, Train_MMSE: 0.045907, NMMSE: 0.040001, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:37:19] Epoch 41/250, Loss: 27.656229, Train_MMSE: 0.045854, NMMSE: 0.039781, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:39:01] Epoch 42/250, Loss: 27.595640, Train_MMSE: 0.045845, NMMSE: 0.042918, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:40:41] Epoch 43/250, Loss: 27.588104, Train_MMSE: 0.045816, NMMSE: 0.041483, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:42:21] Epoch 44/250, Loss: 27.638935, Train_MMSE: 0.045804, NMMSE: 0.039753, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:44:01] Epoch 45/250, Loss: 27.753693, Train_MMSE: 0.045769, NMMSE: 0.040704, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:45:41] Epoch 46/250, Loss: 27.327602, Train_MMSE: 0.045736, NMMSE: 0.039317, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:47:22] Epoch 47/250, Loss: 27.455297, Train_MMSE: 0.045719, NMMSE: 0.04009, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:49:03] Epoch 48/250, Loss: 27.601488, Train_MMSE: 0.04572, NMMSE: 0.041066, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:50:43] Epoch 49/250, Loss: 27.567015, Train_MMSE: 0.045674, NMMSE: 0.039283, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:52:24] Epoch 50/250, Loss: 27.467842, Train_MMSE: 0.045673, NMMSE: 0.03919, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:54:05] Epoch 51/250, Loss: 27.304089, Train_MMSE: 0.045649, NMMSE: 0.039667, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:55:47] Epoch 52/250, Loss: 27.402584, Train_MMSE: 0.045611, NMMSE: 0.039511, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:57:29] Epoch 53/250, Loss: 27.567474, Train_MMSE: 0.045612, NMMSE: 0.039497, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 09:59:10] Epoch 54/250, Loss: 27.499367, Train_MMSE: 0.045591, NMMSE: 0.039183, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:00:52] Epoch 55/250, Loss: 27.392887, Train_MMSE: 0.04557, NMMSE: 0.0395, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:02:34] Epoch 56/250, Loss: 27.562616, Train_MMSE: 0.045565, NMMSE: 0.039316, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:04:17] Epoch 57/250, Loss: 27.604591, Train_MMSE: 0.045508, NMMSE: 0.03922, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:05:59] Epoch 58/250, Loss: 27.485548, Train_MMSE: 0.045506, NMMSE: 0.039206, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:07:42] Epoch 59/250, Loss: 27.689423, Train_MMSE: 0.045473, NMMSE: 0.039159, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 10:09:25] Epoch 60/250, Loss: 27.354240, Train_MMSE: 0.045439, NMMSE: 0.0391, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:11:06] Epoch 61/250, Loss: 27.333035, Train_MMSE: 0.045169, NMMSE: 0.038651, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:12:48] Epoch 62/250, Loss: 27.410898, Train_MMSE: 0.045107, NMMSE: 0.038657, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:14:30] Epoch 63/250, Loss: 27.401592, Train_MMSE: 0.045078, NMMSE: 0.038632, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:16:13] Epoch 64/250, Loss: 27.013861, Train_MMSE: 0.045056, NMMSE: 0.038648, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:17:54] Epoch 65/250, Loss: 27.189438, Train_MMSE: 0.045032, NMMSE: 0.038647, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:19:35] Epoch 66/250, Loss: 27.254770, Train_MMSE: 0.045, NMMSE: 0.03853, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:21:20] Epoch 67/250, Loss: 27.264622, Train_MMSE: 0.044966, NMMSE: 0.038593, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:23:02] Epoch 68/250, Loss: 27.272379, Train_MMSE: 0.044945, NMMSE: 0.038522, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:24:43] Epoch 69/250, Loss: 27.421381, Train_MMSE: 0.044929, NMMSE: 0.038534, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:26:26] Epoch 70/250, Loss: 27.369104, Train_MMSE: 0.044906, NMMSE: 0.038548, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:28:07] Epoch 71/250, Loss: 27.169708, Train_MMSE: 0.044888, NMMSE: 0.038471, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:29:49] Epoch 72/250, Loss: 27.239962, Train_MMSE: 0.044883, NMMSE: 0.038468, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:31:31] Epoch 73/250, Loss: 27.293262, Train_MMSE: 0.044851, NMMSE: 0.038395, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:33:13] Epoch 74/250, Loss: 27.393345, Train_MMSE: 0.044842, NMMSE: 0.038448, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:34:55] Epoch 75/250, Loss: 27.637247, Train_MMSE: 0.044819, NMMSE: 0.038402, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:36:37] Epoch 76/250, Loss: 27.192909, Train_MMSE: 0.044797, NMMSE: 0.038332, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:38:20] Epoch 77/250, Loss: 27.402451, Train_MMSE: 0.044791, NMMSE: 0.038431, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:40:02] Epoch 78/250, Loss: 27.258936, Train_MMSE: 0.044772, NMMSE: 0.038437, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:41:43] Epoch 79/250, Loss: 27.233864, Train_MMSE: 0.044747, NMMSE: 0.038324, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:43:25] Epoch 80/250, Loss: 27.327715, Train_MMSE: 0.044745, NMMSE: 0.038312, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:45:07] Epoch 81/250, Loss: 27.296595, Train_MMSE: 0.04472, NMMSE: 0.038299, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:46:49] Epoch 82/250, Loss: 27.409307, Train_MMSE: 0.044708, NMMSE: 0.038274, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:48:31] Epoch 83/250, Loss: 27.238438, Train_MMSE: 0.044687, NMMSE: 0.038294, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:50:13] Epoch 84/250, Loss: 27.073191, Train_MMSE: 0.044687, NMMSE: 0.03828, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:51:53] Epoch 85/250, Loss: 27.219841, Train_MMSE: 0.044681, NMMSE: 0.038353, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:53:36] Epoch 86/250, Loss: 27.531588, Train_MMSE: 0.044663, NMMSE: 0.038339, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:55:19] Epoch 87/250, Loss: 27.192041, Train_MMSE: 0.044654, NMMSE: 0.038234, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:57:01] Epoch 88/250, Loss: 27.222445, Train_MMSE: 0.044639, NMMSE: 0.038362, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 10:58:48] Epoch 89/250, Loss: 27.163942, Train_MMSE: 0.044641, NMMSE: 0.038222, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:00:32] Epoch 90/250, Loss: 27.130602, Train_MMSE: 0.044625, NMMSE: 0.038315, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:02:14] Epoch 91/250, Loss: 27.447727, Train_MMSE: 0.044614, NMMSE: 0.038228, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:03:57] Epoch 92/250, Loss: 26.934040, Train_MMSE: 0.044614, NMMSE: 0.03825, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:05:39] Epoch 93/250, Loss: 27.163103, Train_MMSE: 0.044599, NMMSE: 0.038207, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:07:20] Epoch 94/250, Loss: 26.985785, Train_MMSE: 0.044596, NMMSE: 0.038204, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:09:02] Epoch 95/250, Loss: 26.745903, Train_MMSE: 0.044572, NMMSE: 0.03816, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:10:45] Epoch 96/250, Loss: 26.974878, Train_MMSE: 0.044584, NMMSE: 0.03817, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:12:29] Epoch 97/250, Loss: 27.513638, Train_MMSE: 0.04456, NMMSE: 0.038173, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:14:14] Epoch 98/250, Loss: 27.001003, Train_MMSE: 0.04455, NMMSE: 0.038124, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:15:55] Epoch 99/250, Loss: 27.174545, Train_MMSE: 0.044538, NMMSE: 0.038182, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:17:37] Epoch 100/250, Loss: 27.165846, Train_MMSE: 0.044515, NMMSE: 0.038124, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:19:18] Epoch 101/250, Loss: 27.077967, Train_MMSE: 0.044505, NMMSE: 0.038127, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:20:59] Epoch 102/250, Loss: 27.137079, Train_MMSE: 0.044502, NMMSE: 0.038243, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:22:41] Epoch 103/250, Loss: 27.240889, Train_MMSE: 0.0445, NMMSE: 0.038152, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:24:21] Epoch 104/250, Loss: 27.120966, Train_MMSE: 0.04449, NMMSE: 0.038083, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:26:01] Epoch 105/250, Loss: 27.128584, Train_MMSE: 0.044483, NMMSE: 0.038107, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:27:44] Epoch 106/250, Loss: 26.951996, Train_MMSE: 0.044469, NMMSE: 0.038193, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:29:27] Epoch 107/250, Loss: 27.164986, Train_MMSE: 0.044473, NMMSE: 0.038081, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:31:09] Epoch 108/250, Loss: 27.289444, Train_MMSE: 0.044454, NMMSE: 0.038115, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:32:51] Epoch 109/250, Loss: 27.407484, Train_MMSE: 0.044437, NMMSE: 0.038133, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:34:34] Epoch 110/250, Loss: 27.164198, Train_MMSE: 0.04443, NMMSE: 0.038045, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:36:15] Epoch 111/250, Loss: 27.307594, Train_MMSE: 0.044421, NMMSE: 0.038084, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:37:58] Epoch 112/250, Loss: 27.215607, Train_MMSE: 0.044425, NMMSE: 0.038012, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:39:41] Epoch 113/250, Loss: 27.191633, Train_MMSE: 0.044405, NMMSE: 0.038026, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:41:24] Epoch 114/250, Loss: 27.140572, Train_MMSE: 0.044406, NMMSE: 0.038056, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:43:07] Epoch 115/250, Loss: 27.365328, Train_MMSE: 0.044403, NMMSE: 0.038469, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:44:50] Epoch 116/250, Loss: 27.208693, Train_MMSE: 0.044397, NMMSE: 0.038014, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:46:32] Epoch 117/250, Loss: 27.175394, Train_MMSE: 0.044392, NMMSE: 0.038055, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:48:14] Epoch 118/250, Loss: 27.428833, Train_MMSE: 0.044384, NMMSE: 0.038096, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:49:56] Epoch 119/250, Loss: 27.040403, Train_MMSE: 0.04437, NMMSE: 0.038048, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 11:51:39] Epoch 120/250, Loss: 27.361902, Train_MMSE: 0.044364, NMMSE: 0.037987, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 11:53:21] Epoch 121/250, Loss: 26.982210, Train_MMSE: 0.044288, NMMSE: 0.037906, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 11:55:03] Epoch 122/250, Loss: 27.139143, Train_MMSE: 0.044275, NMMSE: 0.037903, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 11:56:46] Epoch 123/250, Loss: 26.914059, Train_MMSE: 0.04427, NMMSE: 0.03792, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 11:58:29] Epoch 124/250, Loss: 27.082945, Train_MMSE: 0.044264, NMMSE: 0.037897, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:00:12] Epoch 125/250, Loss: 27.102081, Train_MMSE: 0.04426, NMMSE: 0.037899, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:01:54] Epoch 126/250, Loss: 27.084822, Train_MMSE: 0.044261, NMMSE: 0.037898, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:03:33] Epoch 127/250, Loss: 27.011618, Train_MMSE: 0.044263, NMMSE: 0.037902, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:05:14] Epoch 128/250, Loss: 27.370827, Train_MMSE: 0.044262, NMMSE: 0.037895, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:06:56] Epoch 129/250, Loss: 27.177732, Train_MMSE: 0.044256, NMMSE: 0.037886, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:08:37] Epoch 130/250, Loss: 26.870432, Train_MMSE: 0.044255, NMMSE: 0.037886, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:10:17] Epoch 131/250, Loss: 27.160471, Train_MMSE: 0.044254, NMMSE: 0.037889, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:11:59] Epoch 132/250, Loss: 27.096643, Train_MMSE: 0.044251, NMMSE: 0.03789, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:13:41] Epoch 133/250, Loss: 26.940733, Train_MMSE: 0.044252, NMMSE: 0.037883, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:15:20] Epoch 134/250, Loss: 27.232887, Train_MMSE: 0.044247, NMMSE: 0.037883, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:17:02] Epoch 135/250, Loss: 26.949671, Train_MMSE: 0.044247, NMMSE: 0.037877, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:18:43] Epoch 136/250, Loss: 26.883360, Train_MMSE: 0.044243, NMMSE: 0.037884, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:20:26] Epoch 137/250, Loss: 27.179008, Train_MMSE: 0.044242, NMMSE: 0.037874, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:22:07] Epoch 138/250, Loss: 26.936665, Train_MMSE: 0.044237, NMMSE: 0.037879, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:23:49] Epoch 139/250, Loss: 27.268103, Train_MMSE: 0.044242, NMMSE: 0.037881, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:25:32] Epoch 140/250, Loss: 26.972086, Train_MMSE: 0.044238, NMMSE: 0.037873, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:27:13] Epoch 141/250, Loss: 27.176451, Train_MMSE: 0.044234, NMMSE: 0.037877, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:28:56] Epoch 142/250, Loss: 27.353861, Train_MMSE: 0.044233, NMMSE: 0.037887, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:30:40] Epoch 143/250, Loss: 26.948395, Train_MMSE: 0.044227, NMMSE: 0.037871, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:32:23] Epoch 144/250, Loss: 26.789255, Train_MMSE: 0.044228, NMMSE: 0.037881, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:34:06] Epoch 145/250, Loss: 27.128433, Train_MMSE: 0.044225, NMMSE: 0.037873, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:35:48] Epoch 146/250, Loss: 27.133291, Train_MMSE: 0.04422, NMMSE: 0.037859, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:37:30] Epoch 147/250, Loss: 27.042723, Train_MMSE: 0.044221, NMMSE: 0.037874, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:39:12] Epoch 148/250, Loss: 27.057718, Train_MMSE: 0.04422, NMMSE: 0.03789, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:40:55] Epoch 149/250, Loss: 27.119055, Train_MMSE: 0.044221, NMMSE: 0.037866, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:42:37] Epoch 150/250, Loss: 27.059061, Train_MMSE: 0.044215, NMMSE: 0.03788, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:44:20] Epoch 151/250, Loss: 27.254484, Train_MMSE: 0.044211, NMMSE: 0.037867, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:46:02] Epoch 152/250, Loss: 27.225529, Train_MMSE: 0.044213, NMMSE: 0.037859, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:47:43] Epoch 153/250, Loss: 27.024975, Train_MMSE: 0.044216, NMMSE: 0.037848, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:49:23] Epoch 154/250, Loss: 27.202610, Train_MMSE: 0.044214, NMMSE: 0.037853, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:51:06] Epoch 155/250, Loss: 27.210602, Train_MMSE: 0.044204, NMMSE: 0.037866, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:52:48] Epoch 156/250, Loss: 27.156443, Train_MMSE: 0.04421, NMMSE: 0.037845, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:54:31] Epoch 157/250, Loss: 26.978016, Train_MMSE: 0.044206, NMMSE: 0.037846, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:56:14] Epoch 158/250, Loss: 27.273159, Train_MMSE: 0.044204, NMMSE: 0.037854, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:57:56] Epoch 159/250, Loss: 27.205399, Train_MMSE: 0.044202, NMMSE: 0.03785, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 12:59:38] Epoch 160/250, Loss: 27.051449, Train_MMSE: 0.044205, NMMSE: 0.037852, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:01:22] Epoch 161/250, Loss: 27.478487, Train_MMSE: 0.044203, NMMSE: 0.037851, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:03:04] Epoch 162/250, Loss: 27.132788, Train_MMSE: 0.044198, NMMSE: 0.037856, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:04:46] Epoch 163/250, Loss: 27.035496, Train_MMSE: 0.044196, NMMSE: 0.037844, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:06:29] Epoch 164/250, Loss: 27.124895, Train_MMSE: 0.044195, NMMSE: 0.037842, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:08:11] Epoch 165/250, Loss: 27.137989, Train_MMSE: 0.044195, NMMSE: 0.037846, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:09:53] Epoch 166/250, Loss: 27.129747, Train_MMSE: 0.044192, NMMSE: 0.03786, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:11:36] Epoch 167/250, Loss: 27.063124, Train_MMSE: 0.044193, NMMSE: 0.037865, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:13:18] Epoch 168/250, Loss: 27.409945, Train_MMSE: 0.044188, NMMSE: 0.03783, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:15:01] Epoch 169/250, Loss: 27.247004, Train_MMSE: 0.044189, NMMSE: 0.037845, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:16:44] Epoch 170/250, Loss: 27.270872, Train_MMSE: 0.044191, NMMSE: 0.037849, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:18:26] Epoch 171/250, Loss: 27.104799, Train_MMSE: 0.044185, NMMSE: 0.037835, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:20:08] Epoch 172/250, Loss: 27.135027, Train_MMSE: 0.044187, NMMSE: 0.037826, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:21:49] Epoch 173/250, Loss: 27.296385, Train_MMSE: 0.044184, NMMSE: 0.037869, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:23:32] Epoch 174/250, Loss: 27.109716, Train_MMSE: 0.044184, NMMSE: 0.037848, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:25:14] Epoch 175/250, Loss: 26.811543, Train_MMSE: 0.044182, NMMSE: 0.037831, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:26:57] Epoch 176/250, Loss: 27.199308, Train_MMSE: 0.04418, NMMSE: 0.037827, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:28:37] Epoch 177/250, Loss: 27.247091, Train_MMSE: 0.044177, NMMSE: 0.037832, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:30:18] Epoch 178/250, Loss: 27.196650, Train_MMSE: 0.044175, NMMSE: 0.037818, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:32:00] Epoch 179/250, Loss: 26.855383, Train_MMSE: 0.044177, NMMSE: 0.037827, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 13:33:42] Epoch 180/250, Loss: 27.146437, Train_MMSE: 0.044174, NMMSE: 0.037821, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:35:23] Epoch 181/250, Loss: 27.157509, Train_MMSE: 0.044152, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:37:04] Epoch 182/250, Loss: 27.031176, Train_MMSE: 0.044152, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:38:46] Epoch 183/250, Loss: 27.001932, Train_MMSE: 0.044151, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:40:28] Epoch 184/250, Loss: 26.837744, Train_MMSE: 0.044149, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:42:11] Epoch 185/250, Loss: 27.154219, Train_MMSE: 0.044149, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:43:53] Epoch 186/250, Loss: 27.318581, Train_MMSE: 0.044153, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:45:36] Epoch 187/250, Loss: 26.659504, Train_MMSE: 0.044154, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:47:19] Epoch 188/250, Loss: 27.075205, Train_MMSE: 0.044154, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:49:03] Epoch 189/250, Loss: 26.918217, Train_MMSE: 0.044154, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:50:46] Epoch 190/250, Loss: 27.112696, Train_MMSE: 0.04415, NMMSE: 0.037814, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:52:30] Epoch 191/250, Loss: 27.094278, Train_MMSE: 0.044154, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:54:14] Epoch 192/250, Loss: 27.190783, Train_MMSE: 0.044152, NMMSE: 0.037808, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:55:57] Epoch 193/250, Loss: 26.928720, Train_MMSE: 0.044153, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:57:41] Epoch 194/250, Loss: 26.779284, Train_MMSE: 0.044149, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 13:59:25] Epoch 195/250, Loss: 27.208738, Train_MMSE: 0.044152, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:01:07] Epoch 196/250, Loss: 27.134357, Train_MMSE: 0.044148, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:02:49] Epoch 197/250, Loss: 27.454487, Train_MMSE: 0.044149, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:04:32] Epoch 198/250, Loss: 27.007151, Train_MMSE: 0.044151, NMMSE: 0.037808, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:06:13] Epoch 199/250, Loss: 26.814861, Train_MMSE: 0.044151, NMMSE: 0.037808, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:07:57] Epoch 200/250, Loss: 27.269854, Train_MMSE: 0.044151, NMMSE: 0.037811, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:09:39] Epoch 201/250, Loss: 27.007936, Train_MMSE: 0.044151, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:11:21] Epoch 202/250, Loss: 27.083420, Train_MMSE: 0.044152, NMMSE: 0.037808, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:13:03] Epoch 203/250, Loss: 27.370615, Train_MMSE: 0.04415, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:14:47] Epoch 204/250, Loss: 26.853481, Train_MMSE: 0.044152, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:16:30] Epoch 205/250, Loss: 27.093683, Train_MMSE: 0.044149, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:18:14] Epoch 206/250, Loss: 26.941206, Train_MMSE: 0.044147, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:19:58] Epoch 207/250, Loss: 26.830708, Train_MMSE: 0.044151, NMMSE: 0.037809, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:21:41] Epoch 208/250, Loss: 26.998095, Train_MMSE: 0.044146, NMMSE: 0.037808, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:23:24] Epoch 209/250, Loss: 26.943796, Train_MMSE: 0.044152, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:25:07] Epoch 210/250, Loss: 26.783209, Train_MMSE: 0.044148, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:26:50] Epoch 211/250, Loss: 26.993500, Train_MMSE: 0.04415, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:28:32] Epoch 212/250, Loss: 26.966776, Train_MMSE: 0.04415, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:30:14] Epoch 213/250, Loss: 27.021667, Train_MMSE: 0.044147, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:31:56] Epoch 214/250, Loss: 27.211908, Train_MMSE: 0.044148, NMMSE: 0.03781, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:33:39] Epoch 215/250, Loss: 26.997702, Train_MMSE: 0.044146, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:35:21] Epoch 216/250, Loss: 27.045622, Train_MMSE: 0.044151, NMMSE: 0.037804, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:37:03] Epoch 217/250, Loss: 27.041420, Train_MMSE: 0.044147, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:38:47] Epoch 218/250, Loss: 26.886826, Train_MMSE: 0.044147, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:40:30] Epoch 219/250, Loss: 26.877821, Train_MMSE: 0.044147, NMMSE: 0.037802, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:42:11] Epoch 220/250, Loss: 27.001640, Train_MMSE: 0.04415, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:43:53] Epoch 221/250, Loss: 27.096373, Train_MMSE: 0.044148, NMMSE: 0.037804, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:45:34] Epoch 222/250, Loss: 27.146399, Train_MMSE: 0.044153, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:47:16] Epoch 223/250, Loss: 27.079102, Train_MMSE: 0.044151, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:48:56] Epoch 224/250, Loss: 27.218126, Train_MMSE: 0.044145, NMMSE: 0.037804, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:50:37] Epoch 225/250, Loss: 26.997061, Train_MMSE: 0.044149, NMMSE: 0.037803, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:52:17] Epoch 226/250, Loss: 27.041559, Train_MMSE: 0.044147, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:53:58] Epoch 227/250, Loss: 26.917048, Train_MMSE: 0.044151, NMMSE: 0.037804, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:55:39] Epoch 228/250, Loss: 26.949123, Train_MMSE: 0.044143, NMMSE: 0.037806, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:57:21] Epoch 229/250, Loss: 27.323967, Train_MMSE: 0.044141, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 14:59:01] Epoch 230/250, Loss: 27.199520, Train_MMSE: 0.044146, NMMSE: 0.037807, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:00:43] Epoch 231/250, Loss: 27.153528, Train_MMSE: 0.044143, NMMSE: 0.037803, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:02:24] Epoch 232/250, Loss: 27.130991, Train_MMSE: 0.044146, NMMSE: 0.037802, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:04:05] Epoch 233/250, Loss: 27.039751, Train_MMSE: 0.044144, NMMSE: 0.037802, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:05:46] Epoch 234/250, Loss: 27.122095, Train_MMSE: 0.04414, NMMSE: 0.037803, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:07:16] Epoch 235/250, Loss: 27.025354, Train_MMSE: 0.044141, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:08:37] Epoch 236/250, Loss: 26.948799, Train_MMSE: 0.044147, NMMSE: 0.037805, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:10:00] Epoch 237/250, Loss: 26.949011, Train_MMSE: 0.044144, NMMSE: 0.037802, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:11:23] Epoch 238/250, Loss: 26.991131, Train_MMSE: 0.044142, NMMSE: 0.037803, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:12:45] Epoch 239/250, Loss: 26.847906, Train_MMSE: 0.044142, NMMSE: 0.037801, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 15:14:08] Epoch 240/250, Loss: 27.243841, Train_MMSE: 0.044148, NMMSE: 0.037801, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:15:29] Epoch 241/250, Loss: 26.849499, Train_MMSE: 0.044136, NMMSE: 0.037801, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:16:52] Epoch 242/250, Loss: 27.043827, Train_MMSE: 0.044139, NMMSE: 0.0378, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:18:14] Epoch 243/250, Loss: 27.062246, Train_MMSE: 0.044134, NMMSE: 0.037802, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:19:35] Epoch 244/250, Loss: 27.132084, Train_MMSE: 0.044142, NMMSE: 0.037801, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:20:56] Epoch 245/250, Loss: 27.260353, Train_MMSE: 0.044142, NMMSE: 0.0378, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:22:17] Epoch 246/250, Loss: 26.989981, Train_MMSE: 0.044137, NMMSE: 0.037799, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:23:40] Epoch 247/250, Loss: 27.042595, Train_MMSE: 0.044142, NMMSE: 0.0378, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:25:02] Epoch 248/250, Loss: 26.954874, Train_MMSE: 0.044142, NMMSE: 0.037801, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:26:23] Epoch 249/250, Loss: 27.075312, Train_MMSE: 0.044139, NMMSE: 0.037799, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
[2025-02-22 15:27:35] Epoch 250/250, Loss: 26.931757, Train_MMSE: 0.04414, NMMSE: 0.037799, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
