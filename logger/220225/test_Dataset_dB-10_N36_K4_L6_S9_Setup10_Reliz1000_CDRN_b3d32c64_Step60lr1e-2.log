H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.022683821909496294
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L6_S9_Setup100_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L6_S9_Setup10_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}, 'trainer': {'optimizer': 'Adam', 'lr': 0.01, 'weight_decay': 0.001, 'lr_scheduler': 'Plateau', 'lr_gamma': 0.1, 'min_lr': 1e-06, 'patience': 20}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (52): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (53): ReLU(inplace=True)
      (54): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (55): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (56): ReLU(inplace=True)
      (57): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (58): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (59): ReLU(inplace=True)
      (60): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (61): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (62): ReLU(inplace=True)
      (63): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (65): ReLU(inplace=True)
      (66): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (67): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (68): ReLU(inplace=True)
      (69): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (70): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (71): ReLU(inplace=True)
      (72): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (73): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (74): ReLU(inplace=True)
      (75): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (76): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (77): ReLU(inplace=True)
      (78): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (79): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (80): ReLU(inplace=True)
      (81): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (82): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (83): ReLU(inplace=True)
      (84): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (85): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (86): ReLU(inplace=True)
      (87): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (88): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (89): ReLU(inplace=True)
      (90): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (91): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (92): ReLU(inplace=True)
      (93): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 12.73 MB
loss function:: L1Loss()
[2025-02-22 08:30:47] Epoch 1/250, Loss: 27.487680, Train_MMSE: 0.0472, NMMSE: 0.040811, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:32:30] Epoch 2/250, Loss: 27.536659, Train_MMSE: 0.047176, NMMSE: 0.040828, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:34:13] Epoch 3/250, Loss: 27.202902, Train_MMSE: 0.047109, NMMSE: 0.04073, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:35:54] Epoch 4/250, Loss: 27.585403, Train_MMSE: 0.046851, NMMSE: 0.040546, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:37:39] Epoch 5/250, Loss: 27.007090, Train_MMSE: 0.046402, NMMSE: 0.040286, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:39:19] Epoch 6/250, Loss: 26.913578, Train_MMSE: 0.046159, NMMSE: 0.040042, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:41:01] Epoch 7/250, Loss: 27.210428, Train_MMSE: 0.045814, NMMSE: 0.040564, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:42:43] Epoch 8/250, Loss: 27.005857, Train_MMSE: 0.045366, NMMSE: 0.040431, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:44:26] Epoch 9/250, Loss: 26.687349, Train_MMSE: 0.044761, NMMSE: 0.040336, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:46:10] Epoch 10/250, Loss: 26.433109, Train_MMSE: 0.044353, NMMSE: 0.039688, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:47:55] Epoch 11/250, Loss: 26.322996, Train_MMSE: 0.043569, NMMSE: 0.037652, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:49:36] Epoch 12/250, Loss: 26.072697, Train_MMSE: 0.04257, NMMSE: 0.037435, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:51:20] Epoch 13/250, Loss: 25.854141, Train_MMSE: 0.041957, NMMSE: 0.037037, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:53:04] Epoch 14/250, Loss: 25.773277, Train_MMSE: 0.041508, NMMSE: 0.036661, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:54:47] Epoch 15/250, Loss: 25.682615, Train_MMSE: 0.041137, NMMSE: 0.036032, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:56:30] Epoch 16/250, Loss: 25.584108, Train_MMSE: 0.040909, NMMSE: 0.036222, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:58:14] Epoch 17/250, Loss: 25.722507, Train_MMSE: 0.040733, NMMSE: 0.035705, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 08:59:57] Epoch 18/250, Loss: 25.387329, Train_MMSE: 0.040585, NMMSE: 0.035735, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:01:40] Epoch 19/250, Loss: 25.678396, Train_MMSE: 0.040444, NMMSE: 0.035745, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:03:22] Epoch 20/250, Loss: 25.355015, Train_MMSE: 0.040401, NMMSE: 0.035753, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:05:04] Epoch 21/250, Loss: 25.490412, Train_MMSE: 0.040311, NMMSE: 0.035752, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:06:46] Epoch 22/250, Loss: 25.267317, Train_MMSE: 0.040273, NMMSE: 0.035766, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:08:29] Epoch 23/250, Loss: 25.263832, Train_MMSE: 0.040164, NMMSE: 0.035383, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:10:11] Epoch 24/250, Loss: 25.408028, Train_MMSE: 0.040104, NMMSE: 0.03543, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:11:53] Epoch 25/250, Loss: 25.476740, Train_MMSE: 0.040096, NMMSE: 0.035191, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:13:35] Epoch 26/250, Loss: 25.561441, Train_MMSE: 0.04, NMMSE: 0.036144, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:15:17] Epoch 27/250, Loss: 25.371643, Train_MMSE: 0.039985, NMMSE: 0.035227, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:17:01] Epoch 28/250, Loss: 25.539618, Train_MMSE: 0.039941, NMMSE: 0.035558, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:18:45] Epoch 29/250, Loss: 25.411718, Train_MMSE: 0.039916, NMMSE: 0.035254, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:20:29] Epoch 30/250, Loss: 25.409693, Train_MMSE: 0.039885, NMMSE: 0.035338, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:22:12] Epoch 31/250, Loss: 25.268999, Train_MMSE: 0.039858, NMMSE: 0.035518, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:23:53] Epoch 32/250, Loss: 25.274456, Train_MMSE: 0.039858, NMMSE: 0.037449, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:25:36] Epoch 33/250, Loss: 25.135807, Train_MMSE: 0.039811, NMMSE: 0.035143, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:27:18] Epoch 34/250, Loss: 25.224106, Train_MMSE: 0.039808, NMMSE: 0.035072, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:29:00] Epoch 35/250, Loss: 25.392580, Train_MMSE: 0.039826, NMMSE: 0.035263, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:30:43] Epoch 36/250, Loss: 25.149641, Train_MMSE: 0.039785, NMMSE: 0.03527, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:32:26] Epoch 37/250, Loss: 25.314386, Train_MMSE: 0.039752, NMMSE: 0.035091, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:34:10] Epoch 38/250, Loss: 25.326124, Train_MMSE: 0.039727, NMMSE: 0.035277, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:35:51] Epoch 39/250, Loss: 25.232540, Train_MMSE: 0.039754, NMMSE: 0.035069, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:37:35] Epoch 40/250, Loss: 25.298124, Train_MMSE: 0.039711, NMMSE: 0.035452, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:39:18] Epoch 41/250, Loss: 25.198154, Train_MMSE: 0.039705, NMMSE: 0.035484, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:41:01] Epoch 42/250, Loss: 25.214115, Train_MMSE: 0.039747, NMMSE: 0.035042, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:42:44] Epoch 43/250, Loss: 25.447466, Train_MMSE: 0.039686, NMMSE: 0.034993, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:44:26] Epoch 44/250, Loss: 25.385256, Train_MMSE: 0.039671, NMMSE: 0.035169, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:46:13] Epoch 45/250, Loss: 25.290127, Train_MMSE: 0.039604, NMMSE: 0.035042, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:47:56] Epoch 46/250, Loss: 25.262684, Train_MMSE: 0.039662, NMMSE: 0.035525, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:49:40] Epoch 47/250, Loss: 25.088465, Train_MMSE: 0.039618, NMMSE: 0.035138, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:51:23] Epoch 48/250, Loss: 25.210255, Train_MMSE: 0.039595, NMMSE: 0.034915, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:53:07] Epoch 49/250, Loss: 25.316603, Train_MMSE: 0.039622, NMMSE: 0.035856, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:54:52] Epoch 50/250, Loss: 25.361538, Train_MMSE: 0.039549, NMMSE: 0.035373, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:56:36] Epoch 51/250, Loss: 25.145086, Train_MMSE: 0.039544, NMMSE: 0.036217, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 09:58:21] Epoch 52/250, Loss: 25.296021, Train_MMSE: 0.039517, NMMSE: 0.035148, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:00:05] Epoch 53/250, Loss: 25.109522, Train_MMSE: 0.039472, NMMSE: 0.035949, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:01:48] Epoch 54/250, Loss: 25.099485, Train_MMSE: 0.039444, NMMSE: 0.035422, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:03:33] Epoch 55/250, Loss: 25.085806, Train_MMSE: 0.039402, NMMSE: 0.034742, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:05:17] Epoch 56/250, Loss: 25.184191, Train_MMSE: 0.039363, NMMSE: 0.036108, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:07:00] Epoch 57/250, Loss: 25.138704, Train_MMSE: 0.039373, NMMSE: 0.034451, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:08:44] Epoch 58/250, Loss: 25.114620, Train_MMSE: 0.039281, NMMSE: 0.034533, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:10:28] Epoch 59/250, Loss: 25.130280, Train_MMSE: 0.039234, NMMSE: 0.034928, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 10:12:13] Epoch 60/250, Loss: 25.038401, Train_MMSE: 0.039176, NMMSE: 0.035062, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:13:58] Epoch 61/250, Loss: 24.986658, Train_MMSE: 0.038612, NMMSE: 0.033858, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:15:42] Epoch 62/250, Loss: 24.953970, Train_MMSE: 0.038503, NMMSE: 0.033821, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:17:26] Epoch 63/250, Loss: 24.988302, Train_MMSE: 0.03847, NMMSE: 0.033873, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:19:10] Epoch 64/250, Loss: 24.940516, Train_MMSE: 0.038434, NMMSE: 0.033781, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:20:54] Epoch 65/250, Loss: 24.855856, Train_MMSE: 0.038404, NMMSE: 0.033749, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:22:37] Epoch 66/250, Loss: 25.018631, Train_MMSE: 0.038373, NMMSE: 0.033713, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:24:20] Epoch 67/250, Loss: 24.768019, Train_MMSE: 0.038337, NMMSE: 0.033753, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:26:04] Epoch 68/250, Loss: 24.798582, Train_MMSE: 0.038222, NMMSE: 0.033525, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:27:47] Epoch 69/250, Loss: 24.866997, Train_MMSE: 0.038064, NMMSE: 0.033383, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:29:30] Epoch 70/250, Loss: 24.718914, Train_MMSE: 0.037935, NMMSE: 0.033461, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:31:12] Epoch 71/250, Loss: 24.673979, Train_MMSE: 0.037859, NMMSE: 0.033295, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:32:56] Epoch 72/250, Loss: 24.594484, Train_MMSE: 0.037792, NMMSE: 0.033204, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:34:39] Epoch 73/250, Loss: 24.681803, Train_MMSE: 0.037735, NMMSE: 0.033214, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:36:23] Epoch 74/250, Loss: 24.645281, Train_MMSE: 0.037694, NMMSE: 0.033112, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:38:06] Epoch 75/250, Loss: 24.505472, Train_MMSE: 0.037649, NMMSE: 0.033167, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:39:50] Epoch 76/250, Loss: 24.594812, Train_MMSE: 0.037624, NMMSE: 0.033076, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:41:33] Epoch 77/250, Loss: 24.642582, Train_MMSE: 0.037583, NMMSE: 0.033072, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:43:16] Epoch 78/250, Loss: 24.534777, Train_MMSE: 0.037575, NMMSE: 0.033095, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:44:59] Epoch 79/250, Loss: 24.526224, Train_MMSE: 0.037534, NMMSE: 0.033028, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:46:43] Epoch 80/250, Loss: 24.446075, Train_MMSE: 0.037515, NMMSE: 0.032986, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:48:28] Epoch 81/250, Loss: 24.424377, Train_MMSE: 0.037497, NMMSE: 0.033053, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:50:10] Epoch 82/250, Loss: 24.692989, Train_MMSE: 0.037477, NMMSE: 0.032961, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:51:53] Epoch 83/250, Loss: 24.478674, Train_MMSE: 0.037453, NMMSE: 0.033012, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:53:36] Epoch 84/250, Loss: 24.368855, Train_MMSE: 0.037446, NMMSE: 0.032999, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:55:19] Epoch 85/250, Loss: 24.568037, Train_MMSE: 0.037417, NMMSE: 0.032972, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:57:02] Epoch 86/250, Loss: 24.514318, Train_MMSE: 0.037421, NMMSE: 0.033042, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 10:58:46] Epoch 87/250, Loss: 24.756182, Train_MMSE: 0.037397, NMMSE: 0.032933, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:00:28] Epoch 88/250, Loss: 24.572718, Train_MMSE: 0.037389, NMMSE: 0.032935, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:02:11] Epoch 89/250, Loss: 24.547390, Train_MMSE: 0.03737, NMMSE: 0.032855, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:03:54] Epoch 90/250, Loss: 24.509123, Train_MMSE: 0.037344, NMMSE: 0.033016, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:05:36] Epoch 91/250, Loss: 24.639122, Train_MMSE: 0.037336, NMMSE: 0.032855, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:07:18] Epoch 92/250, Loss: 24.509104, Train_MMSE: 0.037331, NMMSE: 0.032894, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:09:01] Epoch 93/250, Loss: 24.454870, Train_MMSE: 0.037309, NMMSE: 0.032856, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:10:45] Epoch 94/250, Loss: 24.705011, Train_MMSE: 0.03729, NMMSE: 0.032808, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:12:28] Epoch 95/250, Loss: 24.486477, Train_MMSE: 0.037299, NMMSE: 0.032827, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:14:12] Epoch 96/250, Loss: 24.410620, Train_MMSE: 0.037275, NMMSE: 0.03292, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:15:56] Epoch 97/250, Loss: 24.237604, Train_MMSE: 0.037267, NMMSE: 0.032785, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:17:40] Epoch 98/250, Loss: 24.538576, Train_MMSE: 0.03726, NMMSE: 0.032767, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:19:23] Epoch 99/250, Loss: 24.624453, Train_MMSE: 0.037242, NMMSE: 0.032797, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:21:07] Epoch 100/250, Loss: 24.474430, Train_MMSE: 0.037226, NMMSE: 0.032786, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:22:49] Epoch 101/250, Loss: 24.480858, Train_MMSE: 0.037226, NMMSE: 0.032801, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:24:30] Epoch 102/250, Loss: 24.468143, Train_MMSE: 0.037215, NMMSE: 0.032773, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:26:13] Epoch 103/250, Loss: 24.547012, Train_MMSE: 0.03719, NMMSE: 0.032724, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:27:56] Epoch 104/250, Loss: 24.468918, Train_MMSE: 0.037188, NMMSE: 0.032756, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:29:36] Epoch 105/250, Loss: 24.290556, Train_MMSE: 0.037187, NMMSE: 0.032715, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:31:20] Epoch 106/250, Loss: 24.622442, Train_MMSE: 0.037166, NMMSE: 0.032827, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:33:04] Epoch 107/250, Loss: 24.535284, Train_MMSE: 0.03715, NMMSE: 0.032693, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:34:45] Epoch 108/250, Loss: 24.260675, Train_MMSE: 0.03715, NMMSE: 0.032681, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:36:29] Epoch 109/250, Loss: 24.293043, Train_MMSE: 0.037166, NMMSE: 0.032683, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:38:13] Epoch 110/250, Loss: 24.477360, Train_MMSE: 0.037137, NMMSE: 0.032718, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:39:57] Epoch 111/250, Loss: 24.559370, Train_MMSE: 0.03713, NMMSE: 0.03268, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:41:42] Epoch 112/250, Loss: 24.571629, Train_MMSE: 0.037119, NMMSE: 0.032671, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:43:25] Epoch 113/250, Loss: 24.441216, Train_MMSE: 0.037118, NMMSE: 0.032626, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:45:09] Epoch 114/250, Loss: 24.571381, Train_MMSE: 0.037092, NMMSE: 0.032675, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:46:51] Epoch 115/250, Loss: 24.529041, Train_MMSE: 0.0371, NMMSE: 0.032652, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:48:35] Epoch 116/250, Loss: 24.513947, Train_MMSE: 0.037094, NMMSE: 0.032703, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:50:18] Epoch 117/250, Loss: 24.587839, Train_MMSE: 0.037093, NMMSE: 0.032625, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:52:02] Epoch 118/250, Loss: 24.378798, Train_MMSE: 0.037084, NMMSE: 0.032666, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:53:44] Epoch 119/250, Loss: 24.421019, Train_MMSE: 0.03706, NMMSE: 0.032654, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 11:55:27] Epoch 120/250, Loss: 24.346550, Train_MMSE: 0.037063, NMMSE: 0.032659, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 11:57:09] Epoch 121/250, Loss: 24.511713, Train_MMSE: 0.036916, NMMSE: 0.032498, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 11:58:51] Epoch 122/250, Loss: 24.429014, Train_MMSE: 0.036906, NMMSE: 0.0325, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:00:34] Epoch 123/250, Loss: 24.194721, Train_MMSE: 0.036897, NMMSE: 0.032487, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:02:17] Epoch 124/250, Loss: 24.470102, Train_MMSE: 0.036898, NMMSE: 0.032494, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:03:59] Epoch 125/250, Loss: 24.261564, Train_MMSE: 0.036892, NMMSE: 0.03249, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:05:43] Epoch 126/250, Loss: 24.182024, Train_MMSE: 0.036889, NMMSE: 0.032488, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:07:26] Epoch 127/250, Loss: 24.294659, Train_MMSE: 0.036888, NMMSE: 0.032485, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:09:07] Epoch 128/250, Loss: 24.186729, Train_MMSE: 0.036885, NMMSE: 0.032483, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:10:51] Epoch 129/250, Loss: 24.216257, Train_MMSE: 0.036885, NMMSE: 0.032489, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:12:35] Epoch 130/250, Loss: 24.234674, Train_MMSE: 0.036879, NMMSE: 0.032482, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:14:17] Epoch 131/250, Loss: 24.373346, Train_MMSE: 0.036877, NMMSE: 0.032495, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:15:59] Epoch 132/250, Loss: 24.410286, Train_MMSE: 0.036872, NMMSE: 0.032467, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:17:44] Epoch 133/250, Loss: 24.197260, Train_MMSE: 0.03687, NMMSE: 0.032477, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:19:26] Epoch 134/250, Loss: 24.117239, Train_MMSE: 0.036868, NMMSE: 0.032465, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:21:10] Epoch 135/250, Loss: 24.411066, Train_MMSE: 0.036863, NMMSE: 0.032466, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:22:54] Epoch 136/250, Loss: 24.490513, Train_MMSE: 0.036864, NMMSE: 0.032464, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:24:39] Epoch 137/250, Loss: 24.402679, Train_MMSE: 0.03686, NMMSE: 0.032459, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:26:24] Epoch 138/250, Loss: 24.620012, Train_MMSE: 0.036861, NMMSE: 0.032478, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:28:06] Epoch 139/250, Loss: 24.431540, Train_MMSE: 0.036849, NMMSE: 0.032471, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:29:50] Epoch 140/250, Loss: 24.481617, Train_MMSE: 0.036843, NMMSE: 0.032456, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:31:33] Epoch 141/250, Loss: 24.241638, Train_MMSE: 0.036848, NMMSE: 0.032453, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:33:17] Epoch 142/250, Loss: 24.442636, Train_MMSE: 0.036836, NMMSE: 0.032443, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:35:00] Epoch 143/250, Loss: 24.336826, Train_MMSE: 0.036841, NMMSE: 0.032444, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:36:44] Epoch 144/250, Loss: 24.555592, Train_MMSE: 0.036832, NMMSE: 0.032452, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:38:28] Epoch 145/250, Loss: 24.311359, Train_MMSE: 0.036826, NMMSE: 0.032431, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:40:11] Epoch 146/250, Loss: 24.448870, Train_MMSE: 0.03682, NMMSE: 0.032439, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:41:55] Epoch 147/250, Loss: 24.381538, Train_MMSE: 0.036822, NMMSE: 0.032434, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:43:39] Epoch 148/250, Loss: 24.372097, Train_MMSE: 0.036815, NMMSE: 0.032441, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:45:21] Epoch 149/250, Loss: 24.481007, Train_MMSE: 0.036814, NMMSE: 0.032451, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:47:05] Epoch 150/250, Loss: 24.312958, Train_MMSE: 0.036814, NMMSE: 0.032436, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:48:47] Epoch 151/250, Loss: 24.338039, Train_MMSE: 0.036807, NMMSE: 0.032447, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:50:29] Epoch 152/250, Loss: 24.454861, Train_MMSE: 0.036808, NMMSE: 0.03242, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:52:13] Epoch 153/250, Loss: 24.468882, Train_MMSE: 0.036796, NMMSE: 0.032437, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:53:55] Epoch 154/250, Loss: 24.243990, Train_MMSE: 0.036795, NMMSE: 0.032409, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:55:38] Epoch 155/250, Loss: 24.314461, Train_MMSE: 0.036795, NMMSE: 0.032416, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:57:23] Epoch 156/250, Loss: 24.218618, Train_MMSE: 0.036792, NMMSE: 0.032416, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 12:59:05] Epoch 157/250, Loss: 24.193428, Train_MMSE: 0.036779, NMMSE: 0.032411, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:00:48] Epoch 158/250, Loss: 24.283960, Train_MMSE: 0.036783, NMMSE: 0.032409, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:02:29] Epoch 159/250, Loss: 24.512493, Train_MMSE: 0.036775, NMMSE: 0.03241, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:04:13] Epoch 160/250, Loss: 24.347721, Train_MMSE: 0.036775, NMMSE: 0.032398, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:05:55] Epoch 161/250, Loss: 24.327074, Train_MMSE: 0.036776, NMMSE: 0.032388, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:07:38] Epoch 162/250, Loss: 24.317450, Train_MMSE: 0.036765, NMMSE: 0.032395, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:09:20] Epoch 163/250, Loss: 24.229160, Train_MMSE: 0.036768, NMMSE: 0.032376, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:11:04] Epoch 164/250, Loss: 24.362375, Train_MMSE: 0.036766, NMMSE: 0.032395, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:12:47] Epoch 165/250, Loss: 24.356995, Train_MMSE: 0.036756, NMMSE: 0.032395, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:14:31] Epoch 166/250, Loss: 24.230965, Train_MMSE: 0.03675, NMMSE: 0.032374, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:16:14] Epoch 167/250, Loss: 24.361977, Train_MMSE: 0.036753, NMMSE: 0.032372, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:17:56] Epoch 168/250, Loss: 24.215532, Train_MMSE: 0.03675, NMMSE: 0.03237, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:19:39] Epoch 169/250, Loss: 24.486237, Train_MMSE: 0.03674, NMMSE: 0.032383, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:21:24] Epoch 170/250, Loss: 24.261435, Train_MMSE: 0.036733, NMMSE: 0.032358, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:23:07] Epoch 171/250, Loss: 24.426262, Train_MMSE: 0.036728, NMMSE: 0.03237, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:24:49] Epoch 172/250, Loss: 24.245457, Train_MMSE: 0.036735, NMMSE: 0.032384, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:26:33] Epoch 173/250, Loss: 24.405653, Train_MMSE: 0.03673, NMMSE: 0.032349, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:28:16] Epoch 174/250, Loss: 24.386679, Train_MMSE: 0.036725, NMMSE: 0.03237, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:30:01] Epoch 175/250, Loss: 24.199558, Train_MMSE: 0.036722, NMMSE: 0.032369, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:31:44] Epoch 176/250, Loss: 24.403086, Train_MMSE: 0.036717, NMMSE: 0.032349, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:33:28] Epoch 177/250, Loss: 24.246216, Train_MMSE: 0.03672, NMMSE: 0.032353, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:35:12] Epoch 178/250, Loss: 24.371298, Train_MMSE: 0.036714, NMMSE: 0.032357, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:36:55] Epoch 179/250, Loss: 24.231913, Train_MMSE: 0.036709, NMMSE: 0.032355, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 13:38:39] Epoch 180/250, Loss: 24.169228, Train_MMSE: 0.036712, NMMSE: 0.032345, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:40:22] Epoch 181/250, Loss: 24.049479, Train_MMSE: 0.03668, NMMSE: 0.032321, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:42:06] Epoch 182/250, Loss: 24.296553, Train_MMSE: 0.036677, NMMSE: 0.032321, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:43:50] Epoch 183/250, Loss: 24.093317, Train_MMSE: 0.036671, NMMSE: 0.032319, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:45:35] Epoch 184/250, Loss: 24.437983, Train_MMSE: 0.036675, NMMSE: 0.032318, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:47:17] Epoch 185/250, Loss: 24.279772, Train_MMSE: 0.036679, NMMSE: 0.032325, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:49:02] Epoch 186/250, Loss: 24.275108, Train_MMSE: 0.036676, NMMSE: 0.03232, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:50:46] Epoch 187/250, Loss: 24.287996, Train_MMSE: 0.036673, NMMSE: 0.032322, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:52:30] Epoch 188/250, Loss: 24.376495, Train_MMSE: 0.036675, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:54:14] Epoch 189/250, Loss: 24.298079, Train_MMSE: 0.036674, NMMSE: 0.03232, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:56:00] Epoch 190/250, Loss: 24.344595, Train_MMSE: 0.036678, NMMSE: 0.032319, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:57:43] Epoch 191/250, Loss: 24.138699, Train_MMSE: 0.036671, NMMSE: 0.032316, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 13:59:26] Epoch 192/250, Loss: 24.191370, Train_MMSE: 0.036672, NMMSE: 0.032319, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:01:08] Epoch 193/250, Loss: 24.313242, Train_MMSE: 0.036672, NMMSE: 0.032318, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:02:51] Epoch 194/250, Loss: 24.235931, Train_MMSE: 0.036669, NMMSE: 0.03232, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:04:34] Epoch 195/250, Loss: 24.256441, Train_MMSE: 0.036672, NMMSE: 0.032318, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:06:16] Epoch 196/250, Loss: 24.280725, Train_MMSE: 0.03667, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:08:00] Epoch 197/250, Loss: 24.124170, Train_MMSE: 0.036667, NMMSE: 0.032321, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:09:43] Epoch 198/250, Loss: 24.412750, Train_MMSE: 0.03667, NMMSE: 0.032316, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:11:26] Epoch 199/250, Loss: 24.386353, Train_MMSE: 0.036671, NMMSE: 0.032319, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:13:08] Epoch 200/250, Loss: 24.267830, Train_MMSE: 0.036668, NMMSE: 0.032316, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:14:50] Epoch 201/250, Loss: 24.385849, Train_MMSE: 0.036671, NMMSE: 0.032317, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:16:33] Epoch 202/250, Loss: 24.383017, Train_MMSE: 0.036673, NMMSE: 0.032317, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:18:17] Epoch 203/250, Loss: 24.240595, Train_MMSE: 0.036668, NMMSE: 0.032317, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:19:59] Epoch 204/250, Loss: 24.259165, Train_MMSE: 0.036665, NMMSE: 0.032317, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:21:43] Epoch 205/250, Loss: 24.342875, Train_MMSE: 0.036667, NMMSE: 0.032318, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:23:27] Epoch 206/250, Loss: 24.198938, Train_MMSE: 0.036668, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:25:09] Epoch 207/250, Loss: 24.467548, Train_MMSE: 0.036667, NMMSE: 0.032311, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:26:50] Epoch 208/250, Loss: 24.471525, Train_MMSE: 0.036665, NMMSE: 0.032317, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:28:33] Epoch 209/250, Loss: 24.342863, Train_MMSE: 0.036664, NMMSE: 0.032312, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:30:16] Epoch 210/250, Loss: 24.050875, Train_MMSE: 0.036666, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:32:00] Epoch 211/250, Loss: 24.630375, Train_MMSE: 0.036664, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:33:43] Epoch 212/250, Loss: 24.292717, Train_MMSE: 0.036666, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:35:25] Epoch 213/250, Loss: 24.413633, Train_MMSE: 0.036659, NMMSE: 0.032312, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:37:09] Epoch 214/250, Loss: 24.105101, Train_MMSE: 0.036663, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:38:53] Epoch 215/250, Loss: 24.393894, Train_MMSE: 0.036665, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:40:37] Epoch 216/250, Loss: 24.173243, Train_MMSE: 0.036664, NMMSE: 0.032316, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:42:22] Epoch 217/250, Loss: 24.391855, Train_MMSE: 0.036658, NMMSE: 0.03231, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:44:05] Epoch 218/250, Loss: 24.275820, Train_MMSE: 0.036659, NMMSE: 0.032315, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:45:48] Epoch 219/250, Loss: 24.349239, Train_MMSE: 0.036662, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:47:29] Epoch 220/250, Loss: 24.472845, Train_MMSE: 0.036661, NMMSE: 0.032309, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:49:13] Epoch 221/250, Loss: 24.118732, Train_MMSE: 0.036661, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:50:55] Epoch 222/250, Loss: 24.447405, Train_MMSE: 0.036659, NMMSE: 0.032309, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:52:40] Epoch 223/250, Loss: 24.333693, Train_MMSE: 0.036661, NMMSE: 0.032307, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:54:23] Epoch 224/250, Loss: 24.246990, Train_MMSE: 0.036655, NMMSE: 0.032306, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:56:05] Epoch 225/250, Loss: 24.210014, Train_MMSE: 0.036659, NMMSE: 0.032307, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:57:51] Epoch 226/250, Loss: 24.527975, Train_MMSE: 0.03666, NMMSE: 0.03231, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 14:59:34] Epoch 227/250, Loss: 24.345003, Train_MMSE: 0.03666, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:01:16] Epoch 228/250, Loss: 24.216352, Train_MMSE: 0.036656, NMMSE: 0.032306, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:03:00] Epoch 229/250, Loss: 24.425508, Train_MMSE: 0.036656, NMMSE: 0.032306, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:04:42] Epoch 230/250, Loss: 24.207954, Train_MMSE: 0.036654, NMMSE: 0.032307, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:06:25] Epoch 231/250, Loss: 24.275179, Train_MMSE: 0.036653, NMMSE: 0.032308, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:07:49] Epoch 232/250, Loss: 24.455502, Train_MMSE: 0.036655, NMMSE: 0.032313, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:09:13] Epoch 233/250, Loss: 24.264238, Train_MMSE: 0.036652, NMMSE: 0.032304, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:10:37] Epoch 234/250, Loss: 24.159315, Train_MMSE: 0.036654, NMMSE: 0.032299, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:11:59] Epoch 235/250, Loss: 24.282370, Train_MMSE: 0.036652, NMMSE: 0.032318, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:13:21] Epoch 236/250, Loss: 24.448248, Train_MMSE: 0.036647, NMMSE: 0.032303, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:14:43] Epoch 237/250, Loss: 24.288528, Train_MMSE: 0.036652, NMMSE: 0.032305, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:16:06] Epoch 238/250, Loss: 24.041632, Train_MMSE: 0.036651, NMMSE: 0.032307, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:17:29] Epoch 239/250, Loss: 24.517069, Train_MMSE: 0.036653, NMMSE: 0.032306, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 15:18:54] Epoch 240/250, Loss: 24.222834, Train_MMSE: 0.036652, NMMSE: 0.032303, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:20:16] Epoch 241/250, Loss: 24.048391, Train_MMSE: 0.036648, NMMSE: 0.0323, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:21:38] Epoch 242/250, Loss: 24.184971, Train_MMSE: 0.036645, NMMSE: 0.0323, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:22:59] Epoch 243/250, Loss: 24.472843, Train_MMSE: 0.036646, NMMSE: 0.032298, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:24:21] Epoch 244/250, Loss: 24.345409, Train_MMSE: 0.036647, NMMSE: 0.032301, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:25:43] Epoch 245/250, Loss: 24.395058, Train_MMSE: 0.036649, NMMSE: 0.0323, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:27:05] Epoch 246/250, Loss: 24.279343, Train_MMSE: 0.036644, NMMSE: 0.032301, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:27:56] Epoch 247/250, Loss: 24.214373, Train_MMSE: 0.036643, NMMSE: 0.032299, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:28:25] Epoch 248/250, Loss: 24.231077, Train_MMSE: 0.036644, NMMSE: 0.0323, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:28:46] Epoch 249/250, Loss: 24.304754, Train_MMSE: 0.036643, NMMSE: 0.032299, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
[2025-02-22 15:29:07] Epoch 250/250, Loss: 24.478174, Train_MMSE: 0.036647, NMMSE: 0.032299, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
