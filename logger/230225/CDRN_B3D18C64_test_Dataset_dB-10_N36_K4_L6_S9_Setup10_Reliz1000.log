Train.py PID: 24702

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.022683821909496294
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L6_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L6_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L6_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f50ca4ca2d0>
loss function:: L1Loss()
[2025-02-22 21:45:30] Epoch 1/200, Loss: 27.197863, Train_MMSE: 0.047142, NMMSE: 0.040636, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:46:13] Epoch 2/200, Loss: 27.086727, Train_MMSE: 0.046436, NMMSE: 0.039782, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:46:56] Epoch 3/200, Loss: 26.665628, Train_MMSE: 0.044914, NMMSE: 0.038442, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:47:39] Epoch 4/200, Loss: 26.023994, Train_MMSE: 0.043122, NMMSE: 0.037337, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:48:21] Epoch 5/200, Loss: 25.992113, Train_MMSE: 0.04197, NMMSE: 0.036724, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:49:03] Epoch 6/200, Loss: 25.784113, Train_MMSE: 0.041227, NMMSE: 0.036301, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:49:46] Epoch 7/200, Loss: 25.516811, Train_MMSE: 0.040575, NMMSE: 0.035776, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:50:29] Epoch 8/200, Loss: 25.282673, Train_MMSE: 0.040024, NMMSE: 0.035407, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:51:10] Epoch 9/200, Loss: 25.322571, Train_MMSE: 0.039567, NMMSE: 0.03523, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:51:53] Epoch 10/200, Loss: 25.021511, Train_MMSE: 0.039088, NMMSE: 0.034727, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:52:38] Epoch 11/200, Loss: 24.917015, Train_MMSE: 0.038512, NMMSE: 0.034368, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:53:21] Epoch 12/200, Loss: 24.713467, Train_MMSE: 0.037882, NMMSE: 0.0339, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:54:04] Epoch 13/200, Loss: 24.360357, Train_MMSE: 0.037214, NMMSE: 0.033401, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:54:50] Epoch 14/200, Loss: 24.282104, Train_MMSE: 0.036592, NMMSE: 0.033117, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:55:33] Epoch 15/200, Loss: 24.106667, Train_MMSE: 0.036049, NMMSE: 0.032706, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:56:16] Epoch 16/200, Loss: 23.825481, Train_MMSE: 0.035552, NMMSE: 0.03243, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:57:00] Epoch 17/200, Loss: 23.798731, Train_MMSE: 0.035124, NMMSE: 0.032176, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:57:46] Epoch 18/200, Loss: 23.664154, Train_MMSE: 0.034718, NMMSE: 0.031943, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:58:29] Epoch 19/200, Loss: 23.434397, Train_MMSE: 0.034407, NMMSE: 0.031866, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:59:14] Epoch 20/200, Loss: 23.532175, Train_MMSE: 0.034105, NMMSE: 0.031678, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 21:59:57] Epoch 21/200, Loss: 23.244013, Train_MMSE: 0.03382, NMMSE: 0.031715, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:00:42] Epoch 22/200, Loss: 23.149912, Train_MMSE: 0.033616, NMMSE: 0.031656, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:01:25] Epoch 23/200, Loss: 23.177412, Train_MMSE: 0.033357, NMMSE: 0.031341, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:02:11] Epoch 24/200, Loss: 22.989458, Train_MMSE: 0.033136, NMMSE: 0.031373, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:02:56] Epoch 25/200, Loss: 23.080383, Train_MMSE: 0.032962, NMMSE: 0.03147, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:03:40] Epoch 26/200, Loss: 23.042746, Train_MMSE: 0.0328, NMMSE: 0.031322, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:04:24] Epoch 27/200, Loss: 22.915039, Train_MMSE: 0.032638, NMMSE: 0.031382, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:05:10] Epoch 28/200, Loss: 22.731148, Train_MMSE: 0.032434, NMMSE: 0.031381, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:05:55] Epoch 29/200, Loss: 22.775242, Train_MMSE: 0.032283, NMMSE: 0.031279, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:06:38] Epoch 30/200, Loss: 22.713127, Train_MMSE: 0.032134, NMMSE: 0.031366, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:07:21] Epoch 31/200, Loss: 22.732319, Train_MMSE: 0.032021, NMMSE: 0.031378, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:08:04] Epoch 32/200, Loss: 22.563128, Train_MMSE: 0.031873, NMMSE: 0.031462, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:08:48] Epoch 33/200, Loss: 22.616650, Train_MMSE: 0.031744, NMMSE: 0.031529, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:09:28] Epoch 34/200, Loss: 22.606270, Train_MMSE: 0.031621, NMMSE: 0.031525, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:10:11] Epoch 35/200, Loss: 22.512768, Train_MMSE: 0.031513, NMMSE: 0.031497, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:10:54] Epoch 36/200, Loss: 22.367916, Train_MMSE: 0.031389, NMMSE: 0.031754, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:11:37] Epoch 37/200, Loss: 22.384623, Train_MMSE: 0.031277, NMMSE: 0.031565, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:12:20] Epoch 38/200, Loss: 22.237724, Train_MMSE: 0.031168, NMMSE: 0.031598, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:13:02] Epoch 39/200, Loss: 22.244926, Train_MMSE: 0.031027, NMMSE: 0.031824, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:13:47] Epoch 40/200, Loss: 22.114851, Train_MMSE: 0.030943, NMMSE: 0.031943, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:14:30] Epoch 41/200, Loss: 22.274714, Train_MMSE: 0.030842, NMMSE: 0.031759, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:15:14] Epoch 42/200, Loss: 22.131029, Train_MMSE: 0.030745, NMMSE: 0.031611, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:15:58] Epoch 43/200, Loss: 22.093987, Train_MMSE: 0.030628, NMMSE: 0.032036, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:16:41] Epoch 44/200, Loss: 22.135202, Train_MMSE: 0.030556, NMMSE: 0.032323, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:17:26] Epoch 45/200, Loss: 22.105581, Train_MMSE: 0.030431, NMMSE: 0.03192, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:18:09] Epoch 46/200, Loss: 22.248117, Train_MMSE: 0.030336, NMMSE: 0.031941, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:18:52] Epoch 47/200, Loss: 22.050104, Train_MMSE: 0.030244, NMMSE: 0.032017, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:19:35] Epoch 48/200, Loss: 22.080046, Train_MMSE: 0.030167, NMMSE: 0.031965, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:20:20] Epoch 49/200, Loss: 22.088736, Train_MMSE: 0.030086, NMMSE: 0.032094, LS_NMSE: 0.040811, Lr: 0.01
[2025-02-22 22:21:04] Epoch 50/200, Loss: 21.935894, Train_MMSE: 0.030004, NMMSE: 0.031995, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:21:48] Epoch 51/200, Loss: 21.109385, Train_MMSE: 0.028487, NMMSE: 0.032153, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:22:31] Epoch 52/200, Loss: 21.043291, Train_MMSE: 0.027966, NMMSE: 0.032303, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:23:17] Epoch 53/200, Loss: 20.900522, Train_MMSE: 0.027798, NMMSE: 0.032551, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:24:00] Epoch 54/200, Loss: 20.932177, Train_MMSE: 0.027671, NMMSE: 0.032557, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:24:47] Epoch 55/200, Loss: 20.908173, Train_MMSE: 0.027577, NMMSE: 0.032736, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:25:30] Epoch 56/200, Loss: 20.748163, Train_MMSE: 0.027494, NMMSE: 0.032782, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:26:15] Epoch 57/200, Loss: 20.816446, Train_MMSE: 0.02743, NMMSE: 0.032867, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:26:58] Epoch 58/200, Loss: 20.795427, Train_MMSE: 0.027357, NMMSE: 0.03296, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:27:43] Epoch 59/200, Loss: 20.635214, Train_MMSE: 0.027293, NMMSE: 0.033036, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:28:27] Epoch 60/200, Loss: 20.647556, Train_MMSE: 0.027241, NMMSE: 0.033077, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:29:11] Epoch 61/200, Loss: 20.700691, Train_MMSE: 0.027193, NMMSE: 0.033148, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:29:54] Epoch 62/200, Loss: 20.705002, Train_MMSE: 0.027139, NMMSE: 0.033159, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:30:38] Epoch 63/200, Loss: 20.564501, Train_MMSE: 0.027088, NMMSE: 0.033349, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:31:22] Epoch 64/200, Loss: 20.568649, Train_MMSE: 0.02704, NMMSE: 0.033298, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:32:05] Epoch 65/200, Loss: 20.709789, Train_MMSE: 0.026995, NMMSE: 0.033322, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:32:48] Epoch 66/200, Loss: 20.594486, Train_MMSE: 0.02695, NMMSE: 0.033371, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:33:31] Epoch 67/200, Loss: 20.521309, Train_MMSE: 0.026909, NMMSE: 0.033457, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:34:15] Epoch 68/200, Loss: 20.590538, Train_MMSE: 0.026864, NMMSE: 0.033517, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:34:58] Epoch 69/200, Loss: 20.554930, Train_MMSE: 0.026824, NMMSE: 0.033509, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:35:43] Epoch 70/200, Loss: 20.495419, Train_MMSE: 0.026795, NMMSE: 0.033611, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:36:26] Epoch 71/200, Loss: 20.419373, Train_MMSE: 0.026752, NMMSE: 0.033656, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:37:08] Epoch 72/200, Loss: 20.485273, Train_MMSE: 0.026718, NMMSE: 0.033666, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:37:53] Epoch 73/200, Loss: 20.416948, Train_MMSE: 0.026679, NMMSE: 0.033763, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:38:38] Epoch 74/200, Loss: 20.491404, Train_MMSE: 0.026642, NMMSE: 0.033807, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:39:20] Epoch 75/200, Loss: 20.522943, Train_MMSE: 0.026613, NMMSE: 0.033824, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:40:01] Epoch 76/200, Loss: 20.480410, Train_MMSE: 0.026579, NMMSE: 0.033865, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:40:42] Epoch 77/200, Loss: 20.421885, Train_MMSE: 0.026543, NMMSE: 0.033817, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:41:24] Epoch 78/200, Loss: 20.387135, Train_MMSE: 0.026507, NMMSE: 0.033923, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:42:15] Epoch 79/200, Loss: 20.416517, Train_MMSE: 0.02648, NMMSE: 0.034041, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:43:05] Epoch 80/200, Loss: 20.325932, Train_MMSE: 0.026447, NMMSE: 0.034024, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:43:53] Epoch 81/200, Loss: 20.307602, Train_MMSE: 0.026417, NMMSE: 0.033952, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:44:40] Epoch 82/200, Loss: 20.275957, Train_MMSE: 0.026387, NMMSE: 0.034012, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:45:28] Epoch 83/200, Loss: 20.224327, Train_MMSE: 0.026352, NMMSE: 0.03415, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:46:24] Epoch 84/200, Loss: 20.317759, Train_MMSE: 0.02632, NMMSE: 0.034077, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:47:27] Epoch 85/200, Loss: 20.181606, Train_MMSE: 0.026298, NMMSE: 0.034245, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:48:26] Epoch 86/200, Loss: 20.298454, Train_MMSE: 0.026269, NMMSE: 0.034276, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:49:25] Epoch 87/200, Loss: 20.192196, Train_MMSE: 0.026247, NMMSE: 0.034234, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:50:24] Epoch 88/200, Loss: 20.277222, Train_MMSE: 0.026217, NMMSE: 0.03432, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:51:25] Epoch 89/200, Loss: 20.269455, Train_MMSE: 0.026186, NMMSE: 0.034301, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:52:25] Epoch 90/200, Loss: 20.210201, Train_MMSE: 0.026166, NMMSE: 0.034368, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:53:25] Epoch 91/200, Loss: 20.088972, Train_MMSE: 0.026137, NMMSE: 0.034405, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:54:27] Epoch 92/200, Loss: 20.175508, Train_MMSE: 0.02611, NMMSE: 0.034416, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:55:29] Epoch 93/200, Loss: 20.190790, Train_MMSE: 0.026086, NMMSE: 0.034492, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:56:31] Epoch 94/200, Loss: 20.085371, Train_MMSE: 0.026064, NMMSE: 0.034427, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:57:32] Epoch 95/200, Loss: 20.160456, Train_MMSE: 0.026032, NMMSE: 0.034504, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:58:34] Epoch 96/200, Loss: 20.136257, Train_MMSE: 0.026008, NMMSE: 0.03451, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 22:59:42] Epoch 97/200, Loss: 20.075953, Train_MMSE: 0.025987, NMMSE: 0.034574, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 23:00:41] Epoch 98/200, Loss: 20.068066, Train_MMSE: 0.025952, NMMSE: 0.034667, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 23:01:41] Epoch 99/200, Loss: 20.099649, Train_MMSE: 0.025932, NMMSE: 0.034603, LS_NMSE: 0.040811, Lr: 0.001
[2025-02-22 23:02:41] Epoch 100/200, Loss: 20.173458, Train_MMSE: 0.025913, NMMSE: 0.034666, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:03:43] Epoch 101/200, Loss: 19.779253, Train_MMSE: 0.025505, NMMSE: 0.03478, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:04:43] Epoch 102/200, Loss: 19.732130, Train_MMSE: 0.025423, NMMSE: 0.034795, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:05:43] Epoch 103/200, Loss: 19.793650, Train_MMSE: 0.02541, NMMSE: 0.03483, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:06:46] Epoch 104/200, Loss: 19.798189, Train_MMSE: 0.025392, NMMSE: 0.034848, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:07:45] Epoch 105/200, Loss: 19.833008, Train_MMSE: 0.025388, NMMSE: 0.034855, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:08:43] Epoch 106/200, Loss: 19.854029, Train_MMSE: 0.025379, NMMSE: 0.034904, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:09:42] Epoch 107/200, Loss: 19.758860, Train_MMSE: 0.025379, NMMSE: 0.034894, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:10:43] Epoch 108/200, Loss: 19.775984, Train_MMSE: 0.025372, NMMSE: 0.034915, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:11:43] Epoch 109/200, Loss: 19.741472, Train_MMSE: 0.025365, NMMSE: 0.034937, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:12:41] Epoch 110/200, Loss: 19.688564, Train_MMSE: 0.025358, NMMSE: 0.034957, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:13:41] Epoch 111/200, Loss: 19.655115, Train_MMSE: 0.025355, NMMSE: 0.034936, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:14:38] Epoch 112/200, Loss: 19.674036, Train_MMSE: 0.025347, NMMSE: 0.034963, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:15:40] Epoch 113/200, Loss: 19.754272, Train_MMSE: 0.025339, NMMSE: 0.034979, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:16:37] Epoch 114/200, Loss: 19.710093, Train_MMSE: 0.025338, NMMSE: 0.034964, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:17:33] Epoch 115/200, Loss: 19.703207, Train_MMSE: 0.025338, NMMSE: 0.034995, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:18:29] Epoch 116/200, Loss: 19.740408, Train_MMSE: 0.025333, NMMSE: 0.034987, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:19:30] Epoch 117/200, Loss: 19.751503, Train_MMSE: 0.025328, NMMSE: 0.03499, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:20:30] Epoch 118/200, Loss: 19.725687, Train_MMSE: 0.025332, NMMSE: 0.034994, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:21:29] Epoch 119/200, Loss: 19.814966, Train_MMSE: 0.025317, NMMSE: 0.035007, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:22:28] Epoch 120/200, Loss: 19.803596, Train_MMSE: 0.025318, NMMSE: 0.035012, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:23:26] Epoch 121/200, Loss: 19.777506, Train_MMSE: 0.02531, NMMSE: 0.035025, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:24:24] Epoch 122/200, Loss: 19.710798, Train_MMSE: 0.025307, NMMSE: 0.035022, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:25:20] Epoch 123/200, Loss: 19.716082, Train_MMSE: 0.025302, NMMSE: 0.035038, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:26:17] Epoch 124/200, Loss: 19.600216, Train_MMSE: 0.025301, NMMSE: 0.035059, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:27:13] Epoch 125/200, Loss: 19.761652, Train_MMSE: 0.025293, NMMSE: 0.035019, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:28:11] Epoch 126/200, Loss: 19.729332, Train_MMSE: 0.025294, NMMSE: 0.035078, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:29:08] Epoch 127/200, Loss: 19.742895, Train_MMSE: 0.025291, NMMSE: 0.03505, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:30:06] Epoch 128/200, Loss: 19.680172, Train_MMSE: 0.025289, NMMSE: 0.035066, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:31:07] Epoch 129/200, Loss: 19.758553, Train_MMSE: 0.025281, NMMSE: 0.035066, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:32:09] Epoch 130/200, Loss: 19.725267, Train_MMSE: 0.02528, NMMSE: 0.035066, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:33:06] Epoch 131/200, Loss: 19.614338, Train_MMSE: 0.025275, NMMSE: 0.035069, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:34:05] Epoch 132/200, Loss: 19.624739, Train_MMSE: 0.025274, NMMSE: 0.035106, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:35:06] Epoch 133/200, Loss: 19.784815, Train_MMSE: 0.025265, NMMSE: 0.035102, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:36:04] Epoch 134/200, Loss: 19.713383, Train_MMSE: 0.025265, NMMSE: 0.035115, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:37:05] Epoch 135/200, Loss: 19.650869, Train_MMSE: 0.025262, NMMSE: 0.035085, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:38:05] Epoch 136/200, Loss: 19.675024, Train_MMSE: 0.025259, NMMSE: 0.035127, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:39:09] Epoch 137/200, Loss: 19.753189, Train_MMSE: 0.025251, NMMSE: 0.035139, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:40:10] Epoch 138/200, Loss: 19.699570, Train_MMSE: 0.025253, NMMSE: 0.035127, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:41:11] Epoch 139/200, Loss: 19.730803, Train_MMSE: 0.025247, NMMSE: 0.035143, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:42:11] Epoch 140/200, Loss: 19.652872, Train_MMSE: 0.025247, NMMSE: 0.035128, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:43:11] Epoch 141/200, Loss: 19.687674, Train_MMSE: 0.025241, NMMSE: 0.035151, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:44:12] Epoch 142/200, Loss: 19.747030, Train_MMSE: 0.025238, NMMSE: 0.035129, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:45:12] Epoch 143/200, Loss: 19.631077, Train_MMSE: 0.025238, NMMSE: 0.03514, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:46:09] Epoch 144/200, Loss: 19.695826, Train_MMSE: 0.025241, NMMSE: 0.035183, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:47:13] Epoch 145/200, Loss: 19.581423, Train_MMSE: 0.02523, NMMSE: 0.035143, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:48:13] Epoch 146/200, Loss: 19.831003, Train_MMSE: 0.025231, NMMSE: 0.035173, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:49:11] Epoch 147/200, Loss: 19.684933, Train_MMSE: 0.025221, NMMSE: 0.035185, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:50:14] Epoch 148/200, Loss: 19.702209, Train_MMSE: 0.025227, NMMSE: 0.035163, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:51:18] Epoch 149/200, Loss: 19.690733, Train_MMSE: 0.025218, NMMSE: 0.035181, LS_NMSE: 0.040811, Lr: 0.0001
[2025-02-22 23:52:18] Epoch 150/200, Loss: 19.736567, Train_MMSE: 0.025215, NMMSE: 0.035191, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:53:19] Epoch 151/200, Loss: 19.543705, Train_MMSE: 0.025163, NMMSE: 0.035193, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:54:23] Epoch 152/200, Loss: 19.694256, Train_MMSE: 0.025159, NMMSE: 0.035198, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:55:28] Epoch 153/200, Loss: 19.726679, Train_MMSE: 0.025158, NMMSE: 0.035206, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:56:31] Epoch 154/200, Loss: 19.626913, Train_MMSE: 0.025155, NMMSE: 0.035217, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:57:33] Epoch 155/200, Loss: 19.698093, Train_MMSE: 0.025163, NMMSE: 0.0352, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:58:38] Epoch 156/200, Loss: 19.659571, Train_MMSE: 0.025155, NMMSE: 0.035208, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-22 23:59:38] Epoch 157/200, Loss: 19.637930, Train_MMSE: 0.025159, NMMSE: 0.035218, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:00:41] Epoch 158/200, Loss: 19.691309, Train_MMSE: 0.025155, NMMSE: 0.035197, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:01:40] Epoch 159/200, Loss: 19.551727, Train_MMSE: 0.025151, NMMSE: 0.035208, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:02:40] Epoch 160/200, Loss: 19.629286, Train_MMSE: 0.02515, NMMSE: 0.035224, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:03:39] Epoch 161/200, Loss: 19.585505, Train_MMSE: 0.025156, NMMSE: 0.035211, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:04:40] Epoch 162/200, Loss: 19.633881, Train_MMSE: 0.025157, NMMSE: 0.035222, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:05:40] Epoch 163/200, Loss: 19.556829, Train_MMSE: 0.025158, NMMSE: 0.035216, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:06:38] Epoch 164/200, Loss: 19.635111, Train_MMSE: 0.025155, NMMSE: 0.035214, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:07:39] Epoch 165/200, Loss: 19.806833, Train_MMSE: 0.025149, NMMSE: 0.035211, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:08:40] Epoch 166/200, Loss: 19.592594, Train_MMSE: 0.025157, NMMSE: 0.03522, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:09:43] Epoch 167/200, Loss: 19.581095, Train_MMSE: 0.025151, NMMSE: 0.035224, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:10:46] Epoch 168/200, Loss: 19.563499, Train_MMSE: 0.025154, NMMSE: 0.035225, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:11:45] Epoch 169/200, Loss: 19.543844, Train_MMSE: 0.025149, NMMSE: 0.035206, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:12:44] Epoch 170/200, Loss: 19.486105, Train_MMSE: 0.025147, NMMSE: 0.035223, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:13:45] Epoch 171/200, Loss: 19.597025, Train_MMSE: 0.025153, NMMSE: 0.035224, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:14:46] Epoch 172/200, Loss: 19.665266, Train_MMSE: 0.025153, NMMSE: 0.03522, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:15:49] Epoch 173/200, Loss: 19.617817, Train_MMSE: 0.025151, NMMSE: 0.035219, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:16:51] Epoch 174/200, Loss: 19.490150, Train_MMSE: 0.025154, NMMSE: 0.035228, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:17:52] Epoch 175/200, Loss: 19.553434, Train_MMSE: 0.02515, NMMSE: 0.035219, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:18:55] Epoch 176/200, Loss: 19.702776, Train_MMSE: 0.025154, NMMSE: 0.035227, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:19:56] Epoch 177/200, Loss: 19.678858, Train_MMSE: 0.025146, NMMSE: 0.035229, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:20:59] Epoch 178/200, Loss: 19.599173, Train_MMSE: 0.025147, NMMSE: 0.035219, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:21:59] Epoch 179/200, Loss: 19.598242, Train_MMSE: 0.025153, NMMSE: 0.035232, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:23:00] Epoch 180/200, Loss: 19.686129, Train_MMSE: 0.025147, NMMSE: 0.035219, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:23:51] Epoch 181/200, Loss: 19.551888, Train_MMSE: 0.025149, NMMSE: 0.035232, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:24:42] Epoch 182/200, Loss: 19.691942, Train_MMSE: 0.025151, NMMSE: 0.035231, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:25:31] Epoch 183/200, Loss: 19.542454, Train_MMSE: 0.025146, NMMSE: 0.035228, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:26:21] Epoch 184/200, Loss: 19.657450, Train_MMSE: 0.025147, NMMSE: 0.035228, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:27:11] Epoch 185/200, Loss: 19.631338, Train_MMSE: 0.025145, NMMSE: 0.035227, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:28:02] Epoch 186/200, Loss: 19.563921, Train_MMSE: 0.025149, NMMSE: 0.035228, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:28:52] Epoch 187/200, Loss: 19.653971, Train_MMSE: 0.02515, NMMSE: 0.035229, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:29:42] Epoch 188/200, Loss: 19.644619, Train_MMSE: 0.025145, NMMSE: 0.035233, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:30:29] Epoch 189/200, Loss: 19.618530, Train_MMSE: 0.025148, NMMSE: 0.035233, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:31:19] Epoch 190/200, Loss: 19.664915, Train_MMSE: 0.025145, NMMSE: 0.035229, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:32:11] Epoch 191/200, Loss: 19.661251, Train_MMSE: 0.025142, NMMSE: 0.035241, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:33:04] Epoch 192/200, Loss: 19.583754, Train_MMSE: 0.025142, NMMSE: 0.035236, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:33:55] Epoch 193/200, Loss: 19.474104, Train_MMSE: 0.025141, NMMSE: 0.035247, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:34:44] Epoch 194/200, Loss: 19.630136, Train_MMSE: 0.025146, NMMSE: 0.035236, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:35:37] Epoch 195/200, Loss: 19.564129, Train_MMSE: 0.025141, NMMSE: 0.035228, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:36:27] Epoch 196/200, Loss: 19.580160, Train_MMSE: 0.025142, NMMSE: 0.035239, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:37:12] Epoch 197/200, Loss: 19.519636, Train_MMSE: 0.025149, NMMSE: 0.035237, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:37:47] Epoch 198/200, Loss: 19.667526, Train_MMSE: 0.025137, NMMSE: 0.035241, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:38:19] Epoch 199/200, Loss: 19.599384, Train_MMSE: 0.025144, NMMSE: 0.03524, LS_NMSE: 0.040811, Lr: 1e-05
[2025-02-23 00:38:50] Epoch 200/200, Loss: 19.597261, Train_MMSE: 0.025137, NMMSE: 0.035234, LS_NMSE: 0.040811, Lr: 1.0000000000000002e-06
