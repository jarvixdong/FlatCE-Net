Train.py PID: 24443

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.024458686477191807
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L4_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fa13d5fe0f0>
loss function:: L1Loss()
[2025-02-22 21:43:42] Epoch 1/200, Loss: 27.993067, Train_MMSE: 0.047428, NMMSE: 0.040407, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:44:14] Epoch 2/200, Loss: 27.822046, Train_MMSE: 0.046642, NMMSE: 0.039568, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:44:48] Epoch 3/200, Loss: 27.327179, Train_MMSE: 0.045615, NMMSE: 0.038886, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:45:31] Epoch 4/200, Loss: 26.926403, Train_MMSE: 0.04477, NMMSE: 0.038251, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:46:14] Epoch 5/200, Loss: 27.063019, Train_MMSE: 0.043985, NMMSE: 0.037714, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:46:58] Epoch 6/200, Loss: 26.955339, Train_MMSE: 0.043301, NMMSE: 0.037283, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:47:43] Epoch 7/200, Loss: 26.530375, Train_MMSE: 0.042691, NMMSE: 0.036917, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:48:27] Epoch 8/200, Loss: 26.344540, Train_MMSE: 0.042149, NMMSE: 0.036533, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:49:13] Epoch 9/200, Loss: 26.440386, Train_MMSE: 0.041598, NMMSE: 0.036203, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:49:58] Epoch 10/200, Loss: 26.076857, Train_MMSE: 0.041056, NMMSE: 0.03589, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:50:43] Epoch 11/200, Loss: 25.919920, Train_MMSE: 0.040438, NMMSE: 0.035447, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:51:25] Epoch 12/200, Loss: 25.571604, Train_MMSE: 0.039741, NMMSE: 0.03499, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:52:10] Epoch 13/200, Loss: 25.559021, Train_MMSE: 0.03902, NMMSE: 0.034621, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:52:56] Epoch 14/200, Loss: 25.428236, Train_MMSE: 0.038451, NMMSE: 0.034253, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:53:40] Epoch 15/200, Loss: 25.129910, Train_MMSE: 0.038, NMMSE: 0.033987, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:54:24] Epoch 16/200, Loss: 25.061453, Train_MMSE: 0.037623, NMMSE: 0.03391, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:55:08] Epoch 17/200, Loss: 24.969082, Train_MMSE: 0.037271, NMMSE: 0.033854, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:55:53] Epoch 18/200, Loss: 24.713495, Train_MMSE: 0.036962, NMMSE: 0.033791, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:56:36] Epoch 19/200, Loss: 24.690033, Train_MMSE: 0.036685, NMMSE: 0.0336, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:57:21] Epoch 20/200, Loss: 24.644625, Train_MMSE: 0.036402, NMMSE: 0.033751, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:58:06] Epoch 21/200, Loss: 24.765785, Train_MMSE: 0.036165, NMMSE: 0.033572, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:58:49] Epoch 22/200, Loss: 24.666426, Train_MMSE: 0.035932, NMMSE: 0.033679, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 21:59:33] Epoch 23/200, Loss: 24.390839, Train_MMSE: 0.035724, NMMSE: 0.033563, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:00:15] Epoch 24/200, Loss: 24.438257, Train_MMSE: 0.035513, NMMSE: 0.033688, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:00:59] Epoch 25/200, Loss: 24.264748, Train_MMSE: 0.035309, NMMSE: 0.033409, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:01:43] Epoch 26/200, Loss: 24.135864, Train_MMSE: 0.035122, NMMSE: 0.033396, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:02:25] Epoch 27/200, Loss: 24.167963, Train_MMSE: 0.034924, NMMSE: 0.033544, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:03:09] Epoch 28/200, Loss: 24.199249, Train_MMSE: 0.034762, NMMSE: 0.033509, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:03:52] Epoch 29/200, Loss: 24.031660, Train_MMSE: 0.034578, NMMSE: 0.033284, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:04:37] Epoch 30/200, Loss: 23.961184, Train_MMSE: 0.034404, NMMSE: 0.033455, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:05:21] Epoch 31/200, Loss: 23.963882, Train_MMSE: 0.034245, NMMSE: 0.033697, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:06:07] Epoch 32/200, Loss: 23.961676, Train_MMSE: 0.034091, NMMSE: 0.033398, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:06:50] Epoch 33/200, Loss: 23.927555, Train_MMSE: 0.033931, NMMSE: 0.033524, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:07:34] Epoch 34/200, Loss: 23.820736, Train_MMSE: 0.03379, NMMSE: 0.033367, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:08:17] Epoch 35/200, Loss: 23.669416, Train_MMSE: 0.033659, NMMSE: 0.033399, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:09:00] Epoch 36/200, Loss: 23.710709, Train_MMSE: 0.033527, NMMSE: 0.033497, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:09:42] Epoch 37/200, Loss: 23.662251, Train_MMSE: 0.033408, NMMSE: 0.033746, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:10:28] Epoch 38/200, Loss: 23.706591, Train_MMSE: 0.03326, NMMSE: 0.033474, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:11:12] Epoch 39/200, Loss: 23.731546, Train_MMSE: 0.033142, NMMSE: 0.033533, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:11:58] Epoch 40/200, Loss: 23.461857, Train_MMSE: 0.033003, NMMSE: 0.033608, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:12:43] Epoch 41/200, Loss: 23.587900, Train_MMSE: 0.032896, NMMSE: 0.03382, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:13:27] Epoch 42/200, Loss: 23.627619, Train_MMSE: 0.032798, NMMSE: 0.033751, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:14:13] Epoch 43/200, Loss: 23.475338, Train_MMSE: 0.032691, NMMSE: 0.033943, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:14:55] Epoch 44/200, Loss: 23.477261, Train_MMSE: 0.032559, NMMSE: 0.033971, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:15:39] Epoch 45/200, Loss: 23.396753, Train_MMSE: 0.032447, NMMSE: 0.034015, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:16:24] Epoch 46/200, Loss: 23.239908, Train_MMSE: 0.032356, NMMSE: 0.033657, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:17:09] Epoch 47/200, Loss: 23.297695, Train_MMSE: 0.032256, NMMSE: 0.034072, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:17:53] Epoch 48/200, Loss: 23.219179, Train_MMSE: 0.032145, NMMSE: 0.033736, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:18:36] Epoch 49/200, Loss: 23.123556, Train_MMSE: 0.032054, NMMSE: 0.033845, LS_NMSE: 0.040619, Lr: 0.01
[2025-02-22 22:19:20] Epoch 50/200, Loss: 23.241549, Train_MMSE: 0.031953, NMMSE: 0.033903, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:20:05] Epoch 51/200, Loss: 22.261629, Train_MMSE: 0.030353, NMMSE: 0.03397, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:20:48] Epoch 52/200, Loss: 22.013245, Train_MMSE: 0.029729, NMMSE: 0.034205, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:21:33] Epoch 53/200, Loss: 22.087036, Train_MMSE: 0.029504, NMMSE: 0.034456, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:22:16] Epoch 54/200, Loss: 21.942919, Train_MMSE: 0.029352, NMMSE: 0.034516, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:22:59] Epoch 55/200, Loss: 21.854021, Train_MMSE: 0.029237, NMMSE: 0.034632, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:23:43] Epoch 56/200, Loss: 22.021244, Train_MMSE: 0.029129, NMMSE: 0.034732, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:24:28] Epoch 57/200, Loss: 21.789909, Train_MMSE: 0.029052, NMMSE: 0.034771, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:25:11] Epoch 58/200, Loss: 21.772646, Train_MMSE: 0.028965, NMMSE: 0.034873, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:25:55] Epoch 59/200, Loss: 21.766617, Train_MMSE: 0.028897, NMMSE: 0.034913, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:26:38] Epoch 60/200, Loss: 21.670746, Train_MMSE: 0.028828, NMMSE: 0.035044, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:27:23] Epoch 61/200, Loss: 21.700802, Train_MMSE: 0.028762, NMMSE: 0.035022, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:28:07] Epoch 62/200, Loss: 21.546736, Train_MMSE: 0.028713, NMMSE: 0.03519, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:28:52] Epoch 63/200, Loss: 21.724070, Train_MMSE: 0.028649, NMMSE: 0.035216, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:29:34] Epoch 64/200, Loss: 21.526419, Train_MMSE: 0.028599, NMMSE: 0.035247, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:30:19] Epoch 65/200, Loss: 21.605860, Train_MMSE: 0.028551, NMMSE: 0.035247, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:31:02] Epoch 66/200, Loss: 21.493235, Train_MMSE: 0.028502, NMMSE: 0.035389, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:31:46] Epoch 67/200, Loss: 21.548231, Train_MMSE: 0.028453, NMMSE: 0.035328, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:32:28] Epoch 68/200, Loss: 21.591003, Train_MMSE: 0.028397, NMMSE: 0.035425, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:33:13] Epoch 69/200, Loss: 21.579336, Train_MMSE: 0.028361, NMMSE: 0.035484, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:33:57] Epoch 70/200, Loss: 21.541426, Train_MMSE: 0.028315, NMMSE: 0.035601, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:34:40] Epoch 71/200, Loss: 21.505196, Train_MMSE: 0.028275, NMMSE: 0.035644, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:35:25] Epoch 72/200, Loss: 21.442543, Train_MMSE: 0.028237, NMMSE: 0.035724, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:36:08] Epoch 73/200, Loss: 21.434160, Train_MMSE: 0.0282, NMMSE: 0.035669, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:36:51] Epoch 74/200, Loss: 21.381575, Train_MMSE: 0.02816, NMMSE: 0.035777, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:37:31] Epoch 75/200, Loss: 21.423344, Train_MMSE: 0.028124, NMMSE: 0.03578, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:38:15] Epoch 76/200, Loss: 21.545942, Train_MMSE: 0.028088, NMMSE: 0.035875, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:38:59] Epoch 77/200, Loss: 21.288425, Train_MMSE: 0.028049, NMMSE: 0.035822, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:39:40] Epoch 78/200, Loss: 21.462088, Train_MMSE: 0.02802, NMMSE: 0.035934, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:40:20] Epoch 79/200, Loss: 21.353214, Train_MMSE: 0.027982, NMMSE: 0.035875, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:41:00] Epoch 80/200, Loss: 21.456499, Train_MMSE: 0.027953, NMMSE: 0.036049, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:41:47] Epoch 81/200, Loss: 21.295231, Train_MMSE: 0.027925, NMMSE: 0.036121, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:42:36] Epoch 82/200, Loss: 21.378933, Train_MMSE: 0.027886, NMMSE: 0.035994, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:43:25] Epoch 83/200, Loss: 21.294746, Train_MMSE: 0.027854, NMMSE: 0.036252, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:44:12] Epoch 84/200, Loss: 21.348679, Train_MMSE: 0.027822, NMMSE: 0.036223, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:44:58] Epoch 85/200, Loss: 21.238234, Train_MMSE: 0.027804, NMMSE: 0.036237, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:45:46] Epoch 86/200, Loss: 21.231276, Train_MMSE: 0.027762, NMMSE: 0.036246, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:46:45] Epoch 87/200, Loss: 21.308096, Train_MMSE: 0.027745, NMMSE: 0.036259, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:47:48] Epoch 88/200, Loss: 21.268438, Train_MMSE: 0.027714, NMMSE: 0.036282, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:48:50] Epoch 89/200, Loss: 21.167303, Train_MMSE: 0.027684, NMMSE: 0.036418, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:49:50] Epoch 90/200, Loss: 21.194061, Train_MMSE: 0.027652, NMMSE: 0.036413, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:50:51] Epoch 91/200, Loss: 21.144430, Train_MMSE: 0.027634, NMMSE: 0.036463, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:51:51] Epoch 92/200, Loss: 21.132757, Train_MMSE: 0.027604, NMMSE: 0.036492, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:52:49] Epoch 93/200, Loss: 21.126003, Train_MMSE: 0.027577, NMMSE: 0.036445, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:53:47] Epoch 94/200, Loss: 21.042337, Train_MMSE: 0.027551, NMMSE: 0.03646, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:54:46] Epoch 95/200, Loss: 21.128340, Train_MMSE: 0.027529, NMMSE: 0.036357, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:55:44] Epoch 96/200, Loss: 21.124216, Train_MMSE: 0.027501, NMMSE: 0.03655, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:56:43] Epoch 97/200, Loss: 21.045145, Train_MMSE: 0.027474, NMMSE: 0.036615, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:57:44] Epoch 98/200, Loss: 21.193594, Train_MMSE: 0.027458, NMMSE: 0.036625, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:58:47] Epoch 99/200, Loss: 21.029001, Train_MMSE: 0.027431, NMMSE: 0.036741, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-22 22:59:52] Epoch 100/200, Loss: 21.130142, Train_MMSE: 0.02741, NMMSE: 0.036819, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:00:50] Epoch 101/200, Loss: 20.743876, Train_MMSE: 0.026975, NMMSE: 0.036772, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:01:48] Epoch 102/200, Loss: 20.621372, Train_MMSE: 0.026887, NMMSE: 0.036843, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:02:47] Epoch 103/200, Loss: 20.712063, Train_MMSE: 0.02687, NMMSE: 0.036852, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:03:46] Epoch 104/200, Loss: 20.728668, Train_MMSE: 0.026854, NMMSE: 0.036865, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:04:43] Epoch 105/200, Loss: 20.680346, Train_MMSE: 0.026854, NMMSE: 0.036886, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:05:41] Epoch 106/200, Loss: 20.742132, Train_MMSE: 0.026839, NMMSE: 0.036925, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:06:41] Epoch 107/200, Loss: 20.719736, Train_MMSE: 0.026832, NMMSE: 0.036936, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:07:37] Epoch 108/200, Loss: 20.638193, Train_MMSE: 0.026826, NMMSE: 0.036936, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:08:34] Epoch 109/200, Loss: 20.760426, Train_MMSE: 0.02682, NMMSE: 0.036933, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:09:31] Epoch 110/200, Loss: 20.785025, Train_MMSE: 0.026813, NMMSE: 0.036941, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:10:32] Epoch 111/200, Loss: 20.725126, Train_MMSE: 0.02681, NMMSE: 0.036964, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:11:31] Epoch 112/200, Loss: 20.779137, Train_MMSE: 0.026804, NMMSE: 0.036984, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:12:31] Epoch 113/200, Loss: 20.750053, Train_MMSE: 0.0268, NMMSE: 0.036979, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:13:30] Epoch 114/200, Loss: 20.575409, Train_MMSE: 0.026792, NMMSE: 0.037, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:14:26] Epoch 115/200, Loss: 20.714960, Train_MMSE: 0.026791, NMMSE: 0.036992, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:15:28] Epoch 116/200, Loss: 20.602255, Train_MMSE: 0.026784, NMMSE: 0.037005, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:16:26] Epoch 117/200, Loss: 20.588465, Train_MMSE: 0.026782, NMMSE: 0.037024, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:17:22] Epoch 118/200, Loss: 20.663271, Train_MMSE: 0.026777, NMMSE: 0.037037, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:18:18] Epoch 119/200, Loss: 20.649117, Train_MMSE: 0.026768, NMMSE: 0.037053, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:19:19] Epoch 120/200, Loss: 20.682285, Train_MMSE: 0.026769, NMMSE: 0.037042, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:20:21] Epoch 121/200, Loss: 20.652899, Train_MMSE: 0.026766, NMMSE: 0.037051, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:21:19] Epoch 122/200, Loss: 20.604441, Train_MMSE: 0.026764, NMMSE: 0.03707, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:22:17] Epoch 123/200, Loss: 20.713078, Train_MMSE: 0.026759, NMMSE: 0.037079, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:23:14] Epoch 124/200, Loss: 20.705231, Train_MMSE: 0.026758, NMMSE: 0.037071, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:24:13] Epoch 125/200, Loss: 20.578939, Train_MMSE: 0.026747, NMMSE: 0.037083, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:25:10] Epoch 126/200, Loss: 20.744001, Train_MMSE: 0.026746, NMMSE: 0.037099, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:26:07] Epoch 127/200, Loss: 20.615477, Train_MMSE: 0.026743, NMMSE: 0.037091, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:27:03] Epoch 128/200, Loss: 20.638975, Train_MMSE: 0.026741, NMMSE: 0.037089, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:28:02] Epoch 129/200, Loss: 20.628408, Train_MMSE: 0.026739, NMMSE: 0.037125, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:29:01] Epoch 130/200, Loss: 20.652016, Train_MMSE: 0.026738, NMMSE: 0.037118, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:29:59] Epoch 131/200, Loss: 20.569441, Train_MMSE: 0.026734, NMMSE: 0.037123, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:31:01] Epoch 132/200, Loss: 20.698931, Train_MMSE: 0.026727, NMMSE: 0.037105, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:32:03] Epoch 133/200, Loss: 20.669001, Train_MMSE: 0.026722, NMMSE: 0.037126, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:33:01] Epoch 134/200, Loss: 20.580923, Train_MMSE: 0.026718, NMMSE: 0.037147, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:34:03] Epoch 135/200, Loss: 20.695482, Train_MMSE: 0.026714, NMMSE: 0.03715, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:35:04] Epoch 136/200, Loss: 20.634768, Train_MMSE: 0.026714, NMMSE: 0.037162, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:36:02] Epoch 137/200, Loss: 20.590912, Train_MMSE: 0.02671, NMMSE: 0.037155, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:37:03] Epoch 138/200, Loss: 20.679903, Train_MMSE: 0.02671, NMMSE: 0.037186, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:38:05] Epoch 139/200, Loss: 20.588034, Train_MMSE: 0.026706, NMMSE: 0.037195, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:39:09] Epoch 140/200, Loss: 20.586182, Train_MMSE: 0.0267, NMMSE: 0.037177, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:40:11] Epoch 141/200, Loss: 20.567804, Train_MMSE: 0.026695, NMMSE: 0.037168, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:41:13] Epoch 142/200, Loss: 20.668121, Train_MMSE: 0.026694, NMMSE: 0.037178, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:42:12] Epoch 143/200, Loss: 20.670675, Train_MMSE: 0.02669, NMMSE: 0.037196, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:43:13] Epoch 144/200, Loss: 20.674364, Train_MMSE: 0.026693, NMMSE: 0.037208, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:44:14] Epoch 145/200, Loss: 20.610735, Train_MMSE: 0.026688, NMMSE: 0.037204, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:45:14] Epoch 146/200, Loss: 20.678373, Train_MMSE: 0.026684, NMMSE: 0.037212, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:46:11] Epoch 147/200, Loss: 20.637276, Train_MMSE: 0.026681, NMMSE: 0.037214, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:47:14] Epoch 148/200, Loss: 20.622450, Train_MMSE: 0.026683, NMMSE: 0.037205, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:48:14] Epoch 149/200, Loss: 20.604059, Train_MMSE: 0.026675, NMMSE: 0.037215, LS_NMSE: 0.040619, Lr: 0.0001
[2025-02-22 23:49:11] Epoch 150/200, Loss: 20.484949, Train_MMSE: 0.026677, NMMSE: 0.03723, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:50:14] Epoch 151/200, Loss: 20.560154, Train_MMSE: 0.026622, NMMSE: 0.037247, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:51:16] Epoch 152/200, Loss: 20.636600, Train_MMSE: 0.026616, NMMSE: 0.037245, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:52:17] Epoch 153/200, Loss: 20.514744, Train_MMSE: 0.02661, NMMSE: 0.037239, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:53:18] Epoch 154/200, Loss: 20.534807, Train_MMSE: 0.026611, NMMSE: 0.037244, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:54:23] Epoch 155/200, Loss: 20.595549, Train_MMSE: 0.026612, NMMSE: 0.037237, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:55:28] Epoch 156/200, Loss: 20.613667, Train_MMSE: 0.02661, NMMSE: 0.03726, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:56:29] Epoch 157/200, Loss: 20.600777, Train_MMSE: 0.02661, NMMSE: 0.037251, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:57:31] Epoch 158/200, Loss: 20.558153, Train_MMSE: 0.026605, NMMSE: 0.037248, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:58:33] Epoch 159/200, Loss: 20.668131, Train_MMSE: 0.026611, NMMSE: 0.037247, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-22 23:59:33] Epoch 160/200, Loss: 20.580122, Train_MMSE: 0.026608, NMMSE: 0.037248, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:00:34] Epoch 161/200, Loss: 20.602015, Train_MMSE: 0.026607, NMMSE: 0.037255, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:01:31] Epoch 162/200, Loss: 20.573090, Train_MMSE: 0.026612, NMMSE: 0.037247, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:02:27] Epoch 163/200, Loss: 20.567522, Train_MMSE: 0.026609, NMMSE: 0.037264, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:03:27] Epoch 164/200, Loss: 20.580214, Train_MMSE: 0.026607, NMMSE: 0.037262, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:04:28] Epoch 165/200, Loss: 20.614347, Train_MMSE: 0.026609, NMMSE: 0.037267, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:05:28] Epoch 166/200, Loss: 20.601557, Train_MMSE: 0.026605, NMMSE: 0.037264, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:06:27] Epoch 167/200, Loss: 20.557652, Train_MMSE: 0.026608, NMMSE: 0.037266, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:07:26] Epoch 168/200, Loss: 20.575693, Train_MMSE: 0.026606, NMMSE: 0.037249, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:08:26] Epoch 169/200, Loss: 20.578705, Train_MMSE: 0.026609, NMMSE: 0.037259, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:09:24] Epoch 170/200, Loss: 20.494118, Train_MMSE: 0.026609, NMMSE: 0.037259, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:10:26] Epoch 171/200, Loss: 20.570015, Train_MMSE: 0.026608, NMMSE: 0.037256, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:11:27] Epoch 172/200, Loss: 20.594059, Train_MMSE: 0.026608, NMMSE: 0.037261, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:12:27] Epoch 173/200, Loss: 20.524082, Train_MMSE: 0.026606, NMMSE: 0.037271, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:13:30] Epoch 174/200, Loss: 20.514179, Train_MMSE: 0.026599, NMMSE: 0.03727, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:14:32] Epoch 175/200, Loss: 20.552359, Train_MMSE: 0.026599, NMMSE: 0.037266, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:15:36] Epoch 176/200, Loss: 20.543344, Train_MMSE: 0.026604, NMMSE: 0.03726, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:16:40] Epoch 177/200, Loss: 20.641773, Train_MMSE: 0.026601, NMMSE: 0.037283, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:17:40] Epoch 178/200, Loss: 20.557402, Train_MMSE: 0.026606, NMMSE: 0.03727, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:18:40] Epoch 179/200, Loss: 20.467787, Train_MMSE: 0.026599, NMMSE: 0.037272, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:19:39] Epoch 180/200, Loss: 20.490168, Train_MMSE: 0.026603, NMMSE: 0.037263, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:20:37] Epoch 181/200, Loss: 20.507015, Train_MMSE: 0.026603, NMMSE: 0.037267, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:21:37] Epoch 182/200, Loss: 20.651283, Train_MMSE: 0.0266, NMMSE: 0.037256, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:22:38] Epoch 183/200, Loss: 20.592598, Train_MMSE: 0.026601, NMMSE: 0.037276, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:23:35] Epoch 184/200, Loss: 20.591425, Train_MMSE: 0.0266, NMMSE: 0.037268, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:24:25] Epoch 185/200, Loss: 20.497034, Train_MMSE: 0.026601, NMMSE: 0.037256, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:25:18] Epoch 186/200, Loss: 20.636047, Train_MMSE: 0.026601, NMMSE: 0.037266, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:26:08] Epoch 187/200, Loss: 20.629770, Train_MMSE: 0.026599, NMMSE: 0.037279, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:26:59] Epoch 188/200, Loss: 20.573612, Train_MMSE: 0.026599, NMMSE: 0.037262, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:27:50] Epoch 189/200, Loss: 20.511450, Train_MMSE: 0.026597, NMMSE: 0.037272, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:28:39] Epoch 190/200, Loss: 20.452875, Train_MMSE: 0.026599, NMMSE: 0.03727, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:29:30] Epoch 191/200, Loss: 20.478355, Train_MMSE: 0.026602, NMMSE: 0.037266, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:30:18] Epoch 192/200, Loss: 20.533508, Train_MMSE: 0.026602, NMMSE: 0.037273, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:31:09] Epoch 193/200, Loss: 20.621006, Train_MMSE: 0.026599, NMMSE: 0.037281, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:31:58] Epoch 194/200, Loss: 20.531080, Train_MMSE: 0.026597, NMMSE: 0.037265, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:32:49] Epoch 195/200, Loss: 20.551996, Train_MMSE: 0.026597, NMMSE: 0.037272, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:33:42] Epoch 196/200, Loss: 20.557047, Train_MMSE: 0.026606, NMMSE: 0.037272, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:34:33] Epoch 197/200, Loss: 20.414854, Train_MMSE: 0.02659, NMMSE: 0.037286, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:35:25] Epoch 198/200, Loss: 20.550003, Train_MMSE: 0.026598, NMMSE: 0.037279, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:36:15] Epoch 199/200, Loss: 20.592173, Train_MMSE: 0.026595, NMMSE: 0.037284, LS_NMSE: 0.040619, Lr: 1e-05
[2025-02-23 00:37:03] Epoch 200/200, Loss: 20.480314, Train_MMSE: 0.026592, NMMSE: 0.037278, LS_NMSE: 0.040619, Lr: 1.0000000000000002e-06
