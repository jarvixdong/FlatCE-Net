Train.py PID: 24565

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.023724336884761395
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L5_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L5_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L5_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f65aa653e00>
loss function:: L1Loss()
[2025-02-22 21:44:17] Epoch 1/200, Loss: 25.661371, Train_MMSE: 0.039211, NMMSE: 0.03855, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:44:52] Epoch 2/200, Loss: 25.252272, Train_MMSE: 0.038658, NMMSE: 0.037739, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:45:34] Epoch 3/200, Loss: 25.210394, Train_MMSE: 0.037943, NMMSE: 0.037151, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:46:17] Epoch 4/200, Loss: 25.079334, Train_MMSE: 0.037381, NMMSE: 0.036709, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:47:00] Epoch 5/200, Loss: 25.032619, Train_MMSE: 0.036955, NMMSE: 0.036383, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:47:43] Epoch 6/200, Loss: 24.830582, Train_MMSE: 0.036607, NMMSE: 0.036128, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:48:28] Epoch 7/200, Loss: 24.641317, Train_MMSE: 0.036296, NMMSE: 0.035913, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:49:11] Epoch 8/200, Loss: 24.683943, Train_MMSE: 0.035998, NMMSE: 0.035697, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:49:58] Epoch 9/200, Loss: 24.527979, Train_MMSE: 0.03569, NMMSE: 0.035481, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:50:42] Epoch 10/200, Loss: 24.361338, Train_MMSE: 0.035407, NMMSE: 0.035249, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:51:24] Epoch 11/200, Loss: 24.346514, Train_MMSE: 0.035104, NMMSE: 0.035087, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:52:06] Epoch 12/200, Loss: 24.329668, Train_MMSE: 0.034765, NMMSE: 0.034806, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:52:51] Epoch 13/200, Loss: 24.030876, Train_MMSE: 0.034378, NMMSE: 0.034556, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:53:36] Epoch 14/200, Loss: 23.817104, Train_MMSE: 0.033992, NMMSE: 0.034255, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:54:19] Epoch 15/200, Loss: 23.749538, Train_MMSE: 0.033576, NMMSE: 0.033929, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:55:02] Epoch 16/200, Loss: 23.577551, Train_MMSE: 0.033163, NMMSE: 0.03378, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:55:46] Epoch 17/200, Loss: 23.472046, Train_MMSE: 0.032797, NMMSE: 0.03359, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:56:29] Epoch 18/200, Loss: 23.537123, Train_MMSE: 0.032482, NMMSE: 0.033277, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:57:15] Epoch 19/200, Loss: 23.251215, Train_MMSE: 0.032196, NMMSE: 0.033151, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:57:59] Epoch 20/200, Loss: 23.253658, Train_MMSE: 0.031904, NMMSE: 0.033119, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:58:43] Epoch 21/200, Loss: 23.197077, Train_MMSE: 0.031621, NMMSE: 0.033047, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 21:59:28] Epoch 22/200, Loss: 23.076365, Train_MMSE: 0.031405, NMMSE: 0.032906, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:00:09] Epoch 23/200, Loss: 22.989038, Train_MMSE: 0.031174, NMMSE: 0.032925, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:00:54] Epoch 24/200, Loss: 23.015083, Train_MMSE: 0.030968, NMMSE: 0.032797, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:01:37] Epoch 25/200, Loss: 22.867306, Train_MMSE: 0.030794, NMMSE: 0.032897, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:02:19] Epoch 26/200, Loss: 22.906652, Train_MMSE: 0.030612, NMMSE: 0.032851, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:03:02] Epoch 27/200, Loss: 22.792290, Train_MMSE: 0.030441, NMMSE: 0.03294, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:03:46] Epoch 28/200, Loss: 22.656389, Train_MMSE: 0.030261, NMMSE: 0.033007, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:04:31] Epoch 29/200, Loss: 22.708273, Train_MMSE: 0.030129, NMMSE: 0.032999, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:05:16] Epoch 30/200, Loss: 22.610359, Train_MMSE: 0.029971, NMMSE: 0.033088, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:06:01] Epoch 31/200, Loss: 22.387154, Train_MMSE: 0.029834, NMMSE: 0.032914, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:06:45] Epoch 32/200, Loss: 22.609650, Train_MMSE: 0.029707, NMMSE: 0.032982, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:07:28] Epoch 33/200, Loss: 22.331604, Train_MMSE: 0.029599, NMMSE: 0.033082, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:08:12] Epoch 34/200, Loss: 22.333754, Train_MMSE: 0.029454, NMMSE: 0.033057, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:08:56] Epoch 35/200, Loss: 22.260881, Train_MMSE: 0.029337, NMMSE: 0.033077, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:09:38] Epoch 36/200, Loss: 22.436701, Train_MMSE: 0.02922, NMMSE: 0.033148, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:10:25] Epoch 37/200, Loss: 22.222857, Train_MMSE: 0.029126, NMMSE: 0.033112, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:11:11] Epoch 38/200, Loss: 22.230181, Train_MMSE: 0.029008, NMMSE: 0.033143, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:11:57] Epoch 39/200, Loss: 22.231720, Train_MMSE: 0.028896, NMMSE: 0.033243, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:12:41] Epoch 40/200, Loss: 22.153446, Train_MMSE: 0.028793, NMMSE: 0.03315, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:13:23] Epoch 41/200, Loss: 22.081232, Train_MMSE: 0.028659, NMMSE: 0.033322, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:14:06] Epoch 42/200, Loss: 21.904938, Train_MMSE: 0.028578, NMMSE: 0.033332, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:14:48] Epoch 43/200, Loss: 22.112135, Train_MMSE: 0.028461, NMMSE: 0.033419, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:15:32] Epoch 44/200, Loss: 22.093641, Train_MMSE: 0.028395, NMMSE: 0.03351, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:16:16] Epoch 45/200, Loss: 21.957842, Train_MMSE: 0.0283, NMMSE: 0.033629, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:17:01] Epoch 46/200, Loss: 21.893206, Train_MMSE: 0.028204, NMMSE: 0.033616, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:17:46] Epoch 47/200, Loss: 21.913029, Train_MMSE: 0.028122, NMMSE: 0.033535, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:18:29] Epoch 48/200, Loss: 21.881264, Train_MMSE: 0.028017, NMMSE: 0.033632, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:19:11] Epoch 49/200, Loss: 21.802263, Train_MMSE: 0.027952, NMMSE: 0.033716, LS_NMSE: 0.038781, Lr: 0.01
[2025-02-22 22:19:56] Epoch 50/200, Loss: 21.873671, Train_MMSE: 0.027854, NMMSE: 0.033694, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:20:39] Epoch 51/200, Loss: 20.813334, Train_MMSE: 0.026409, NMMSE: 0.033907, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:21:24] Epoch 52/200, Loss: 20.772943, Train_MMSE: 0.025792, NMMSE: 0.034117, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:22:07] Epoch 53/200, Loss: 20.670185, Train_MMSE: 0.025553, NMMSE: 0.034253, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:22:52] Epoch 54/200, Loss: 20.513119, Train_MMSE: 0.025391, NMMSE: 0.034458, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:23:36] Epoch 55/200, Loss: 20.491638, Train_MMSE: 0.025257, NMMSE: 0.034568, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:24:18] Epoch 56/200, Loss: 20.436825, Train_MMSE: 0.025157, NMMSE: 0.034694, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:25:03] Epoch 57/200, Loss: 20.327885, Train_MMSE: 0.025058, NMMSE: 0.034773, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:25:47] Epoch 58/200, Loss: 20.381220, Train_MMSE: 0.024974, NMMSE: 0.034836, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:26:29] Epoch 59/200, Loss: 20.333931, Train_MMSE: 0.024897, NMMSE: 0.03494, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:27:12] Epoch 60/200, Loss: 20.278784, Train_MMSE: 0.024826, NMMSE: 0.035027, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:27:58] Epoch 61/200, Loss: 20.220398, Train_MMSE: 0.024766, NMMSE: 0.035023, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:28:41] Epoch 62/200, Loss: 20.345451, Train_MMSE: 0.024701, NMMSE: 0.035151, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:29:23] Epoch 63/200, Loss: 20.149448, Train_MMSE: 0.024645, NMMSE: 0.035242, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:30:06] Epoch 64/200, Loss: 20.164974, Train_MMSE: 0.02459, NMMSE: 0.035306, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:30:49] Epoch 65/200, Loss: 20.173851, Train_MMSE: 0.024542, NMMSE: 0.035511, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:31:33] Epoch 66/200, Loss: 20.097696, Train_MMSE: 0.024484, NMMSE: 0.035473, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:32:15] Epoch 67/200, Loss: 20.083469, Train_MMSE: 0.024437, NMMSE: 0.035484, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:32:56] Epoch 68/200, Loss: 20.184687, Train_MMSE: 0.024391, NMMSE: 0.035594, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:33:40] Epoch 69/200, Loss: 20.213009, Train_MMSE: 0.024351, NMMSE: 0.035636, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:34:24] Epoch 70/200, Loss: 19.908911, Train_MMSE: 0.024306, NMMSE: 0.035709, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:35:08] Epoch 71/200, Loss: 19.901402, Train_MMSE: 0.024272, NMMSE: 0.035759, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:35:53] Epoch 72/200, Loss: 19.868521, Train_MMSE: 0.024232, NMMSE: 0.035847, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:36:38] Epoch 73/200, Loss: 20.005972, Train_MMSE: 0.02419, NMMSE: 0.035861, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:37:19] Epoch 74/200, Loss: 19.876066, Train_MMSE: 0.024154, NMMSE: 0.035927, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:38:03] Epoch 75/200, Loss: 19.958496, Train_MMSE: 0.02411, NMMSE: 0.035975, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:38:48] Epoch 76/200, Loss: 19.881254, Train_MMSE: 0.024079, NMMSE: 0.036027, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:39:29] Epoch 77/200, Loss: 19.914869, Train_MMSE: 0.024044, NMMSE: 0.0361, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:40:10] Epoch 78/200, Loss: 19.995935, Train_MMSE: 0.024015, NMMSE: 0.036065, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:40:51] Epoch 79/200, Loss: 19.971170, Train_MMSE: 0.023978, NMMSE: 0.036135, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:41:35] Epoch 80/200, Loss: 19.932417, Train_MMSE: 0.023951, NMMSE: 0.036213, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:42:25] Epoch 81/200, Loss: 19.910007, Train_MMSE: 0.02392, NMMSE: 0.036265, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:43:14] Epoch 82/200, Loss: 19.793587, Train_MMSE: 0.023889, NMMSE: 0.036349, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:44:04] Epoch 83/200, Loss: 19.794739, Train_MMSE: 0.023863, NMMSE: 0.036352, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:44:53] Epoch 84/200, Loss: 19.764936, Train_MMSE: 0.023828, NMMSE: 0.036447, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:45:42] Epoch 85/200, Loss: 19.827240, Train_MMSE: 0.023805, NMMSE: 0.0364, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:46:42] Epoch 86/200, Loss: 19.760805, Train_MMSE: 0.023766, NMMSE: 0.036564, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:47:46] Epoch 87/200, Loss: 19.746284, Train_MMSE: 0.02375, NMMSE: 0.03653, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:48:48] Epoch 88/200, Loss: 19.752237, Train_MMSE: 0.023725, NMMSE: 0.036489, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:49:48] Epoch 89/200, Loss: 19.793728, Train_MMSE: 0.023696, NMMSE: 0.03652, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:50:48] Epoch 90/200, Loss: 19.739370, Train_MMSE: 0.023673, NMMSE: 0.036576, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:51:50] Epoch 91/200, Loss: 19.662851, Train_MMSE: 0.023644, NMMSE: 0.036692, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:52:45] Epoch 92/200, Loss: 19.711359, Train_MMSE: 0.023627, NMMSE: 0.036697, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:53:43] Epoch 93/200, Loss: 19.719767, Train_MMSE: 0.023603, NMMSE: 0.036713, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:54:42] Epoch 94/200, Loss: 19.621981, Train_MMSE: 0.023572, NMMSE: 0.036701, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:55:40] Epoch 95/200, Loss: 19.550936, Train_MMSE: 0.023549, NMMSE: 0.036747, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:56:39] Epoch 96/200, Loss: 19.641872, Train_MMSE: 0.023528, NMMSE: 0.036909, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:57:40] Epoch 97/200, Loss: 19.514650, Train_MMSE: 0.023509, NMMSE: 0.036979, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:58:43] Epoch 98/200, Loss: 19.702766, Train_MMSE: 0.023491, NMMSE: 0.036905, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 22:59:48] Epoch 99/200, Loss: 19.679443, Train_MMSE: 0.023473, NMMSE: 0.036954, LS_NMSE: 0.038781, Lr: 0.001
[2025-02-22 23:00:45] Epoch 100/200, Loss: 19.758749, Train_MMSE: 0.023446, NMMSE: 0.036925, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:01:42] Epoch 101/200, Loss: 19.222719, Train_MMSE: 0.023038, NMMSE: 0.037062, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:02:40] Epoch 102/200, Loss: 19.241028, Train_MMSE: 0.022951, NMMSE: 0.037107, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:03:38] Epoch 103/200, Loss: 19.185270, Train_MMSE: 0.022927, NMMSE: 0.03716, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:04:35] Epoch 104/200, Loss: 19.234461, Train_MMSE: 0.022915, NMMSE: 0.037189, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:05:31] Epoch 105/200, Loss: 19.233433, Train_MMSE: 0.022898, NMMSE: 0.037198, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:06:32] Epoch 106/200, Loss: 19.250181, Train_MMSE: 0.022897, NMMSE: 0.037207, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:07:28] Epoch 107/200, Loss: 19.201181, Train_MMSE: 0.022883, NMMSE: 0.037235, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:08:23] Epoch 108/200, Loss: 19.185343, Train_MMSE: 0.022881, NMMSE: 0.037262, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:09:19] Epoch 109/200, Loss: 19.228300, Train_MMSE: 0.022871, NMMSE: 0.037281, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:10:18] Epoch 110/200, Loss: 19.174673, Train_MMSE: 0.022867, NMMSE: 0.037275, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:11:17] Epoch 111/200, Loss: 19.240976, Train_MMSE: 0.022857, NMMSE: 0.037283, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:12:17] Epoch 112/200, Loss: 19.214998, Train_MMSE: 0.022853, NMMSE: 0.037289, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:13:14] Epoch 113/200, Loss: 19.171186, Train_MMSE: 0.022853, NMMSE: 0.037315, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:14:12] Epoch 114/200, Loss: 19.202305, Train_MMSE: 0.022848, NMMSE: 0.037304, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:15:12] Epoch 115/200, Loss: 19.226675, Train_MMSE: 0.022845, NMMSE: 0.03733, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:16:12] Epoch 116/200, Loss: 19.113962, Train_MMSE: 0.02284, NMMSE: 0.037322, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:17:10] Epoch 117/200, Loss: 19.129774, Train_MMSE: 0.022834, NMMSE: 0.037358, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:18:07] Epoch 118/200, Loss: 19.223820, Train_MMSE: 0.022834, NMMSE: 0.037356, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:19:05] Epoch 119/200, Loss: 19.166983, Train_MMSE: 0.022822, NMMSE: 0.037354, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:20:09] Epoch 120/200, Loss: 19.172262, Train_MMSE: 0.022822, NMMSE: 0.037384, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:21:08] Epoch 121/200, Loss: 19.213404, Train_MMSE: 0.022814, NMMSE: 0.037383, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:22:08] Epoch 122/200, Loss: 19.154703, Train_MMSE: 0.022815, NMMSE: 0.037393, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:23:08] Epoch 123/200, Loss: 19.192844, Train_MMSE: 0.022811, NMMSE: 0.037406, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:24:07] Epoch 124/200, Loss: 19.231569, Train_MMSE: 0.022804, NMMSE: 0.037426, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:25:05] Epoch 125/200, Loss: 19.199816, Train_MMSE: 0.022802, NMMSE: 0.037413, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:26:04] Epoch 126/200, Loss: 19.175236, Train_MMSE: 0.022802, NMMSE: 0.037428, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:27:02] Epoch 127/200, Loss: 19.160669, Train_MMSE: 0.022796, NMMSE: 0.037438, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:28:02] Epoch 128/200, Loss: 19.199480, Train_MMSE: 0.022792, NMMSE: 0.037472, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:29:02] Epoch 129/200, Loss: 19.176916, Train_MMSE: 0.022787, NMMSE: 0.037454, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:30:01] Epoch 130/200, Loss: 19.063816, Train_MMSE: 0.022785, NMMSE: 0.037475, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:31:02] Epoch 131/200, Loss: 19.118364, Train_MMSE: 0.022784, NMMSE: 0.037478, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:32:05] Epoch 132/200, Loss: 19.178068, Train_MMSE: 0.022781, NMMSE: 0.037486, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:33:03] Epoch 133/200, Loss: 19.221407, Train_MMSE: 0.02278, NMMSE: 0.037478, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:34:02] Epoch 134/200, Loss: 19.230734, Train_MMSE: 0.022774, NMMSE: 0.037476, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:35:03] Epoch 135/200, Loss: 19.138313, Train_MMSE: 0.022771, NMMSE: 0.037493, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:36:00] Epoch 136/200, Loss: 19.108166, Train_MMSE: 0.02277, NMMSE: 0.03752, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:37:00] Epoch 137/200, Loss: 19.005184, Train_MMSE: 0.022769, NMMSE: 0.03753, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:38:03] Epoch 138/200, Loss: 19.198099, Train_MMSE: 0.022764, NMMSE: 0.037506, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:39:08] Epoch 139/200, Loss: 19.094143, Train_MMSE: 0.022758, NMMSE: 0.037532, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:40:08] Epoch 140/200, Loss: 19.214891, Train_MMSE: 0.022756, NMMSE: 0.037552, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:41:08] Epoch 141/200, Loss: 19.224699, Train_MMSE: 0.022753, NMMSE: 0.037525, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:42:09] Epoch 142/200, Loss: 19.069082, Train_MMSE: 0.022754, NMMSE: 0.037556, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:43:09] Epoch 143/200, Loss: 19.118876, Train_MMSE: 0.022746, NMMSE: 0.037558, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:44:10] Epoch 144/200, Loss: 19.128748, Train_MMSE: 0.02275, NMMSE: 0.03756, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:45:09] Epoch 145/200, Loss: 19.181871, Train_MMSE: 0.022744, NMMSE: 0.037565, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:46:07] Epoch 146/200, Loss: 19.175642, Train_MMSE: 0.022739, NMMSE: 0.037561, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:47:10] Epoch 147/200, Loss: 19.120026, Train_MMSE: 0.022737, NMMSE: 0.037567, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:48:09] Epoch 148/200, Loss: 19.214912, Train_MMSE: 0.022736, NMMSE: 0.037571, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:49:07] Epoch 149/200, Loss: 19.218193, Train_MMSE: 0.022732, NMMSE: 0.037588, LS_NMSE: 0.038781, Lr: 0.0001
[2025-02-22 23:50:08] Epoch 150/200, Loss: 18.985806, Train_MMSE: 0.022728, NMMSE: 0.037592, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:51:10] Epoch 151/200, Loss: 18.934420, Train_MMSE: 0.022681, NMMSE: 0.037608, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:52:13] Epoch 152/200, Loss: 19.056250, Train_MMSE: 0.022676, NMMSE: 0.037605, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:53:16] Epoch 153/200, Loss: 18.995110, Train_MMSE: 0.022671, NMMSE: 0.037603, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:54:19] Epoch 154/200, Loss: 19.034037, Train_MMSE: 0.022671, NMMSE: 0.037607, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:55:21] Epoch 155/200, Loss: 19.117994, Train_MMSE: 0.022678, NMMSE: 0.037616, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:56:21] Epoch 156/200, Loss: 19.157471, Train_MMSE: 0.022676, NMMSE: 0.037611, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:57:19] Epoch 157/200, Loss: 19.117985, Train_MMSE: 0.02267, NMMSE: 0.037618, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:58:18] Epoch 158/200, Loss: 19.188051, Train_MMSE: 0.022669, NMMSE: 0.037615, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-22 23:59:18] Epoch 159/200, Loss: 19.012976, Train_MMSE: 0.022669, NMMSE: 0.037612, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:00:15] Epoch 160/200, Loss: 19.085846, Train_MMSE: 0.022666, NMMSE: 0.037618, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:01:16] Epoch 161/200, Loss: 19.127232, Train_MMSE: 0.022667, NMMSE: 0.037614, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:02:11] Epoch 162/200, Loss: 19.137175, Train_MMSE: 0.022669, NMMSE: 0.037609, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:03:14] Epoch 163/200, Loss: 19.144436, Train_MMSE: 0.02267, NMMSE: 0.037619, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:04:17] Epoch 164/200, Loss: 19.037758, Train_MMSE: 0.022665, NMMSE: 0.037618, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:05:16] Epoch 165/200, Loss: 19.081743, Train_MMSE: 0.02267, NMMSE: 0.037616, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:06:13] Epoch 166/200, Loss: 19.011545, Train_MMSE: 0.022669, NMMSE: 0.037627, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:07:10] Epoch 167/200, Loss: 18.975861, Train_MMSE: 0.022668, NMMSE: 0.037621, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:08:09] Epoch 168/200, Loss: 19.120279, Train_MMSE: 0.022673, NMMSE: 0.037625, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:09:09] Epoch 169/200, Loss: 19.045340, Train_MMSE: 0.022664, NMMSE: 0.037625, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:10:10] Epoch 170/200, Loss: 18.995447, Train_MMSE: 0.022668, NMMSE: 0.03763, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:11:11] Epoch 171/200, Loss: 19.076426, Train_MMSE: 0.02267, NMMSE: 0.037628, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:12:13] Epoch 172/200, Loss: 19.161329, Train_MMSE: 0.022667, NMMSE: 0.037637, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:13:15] Epoch 173/200, Loss: 19.037460, Train_MMSE: 0.022662, NMMSE: 0.037642, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:14:18] Epoch 174/200, Loss: 19.044350, Train_MMSE: 0.022671, NMMSE: 0.037628, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:15:19] Epoch 175/200, Loss: 19.048544, Train_MMSE: 0.022668, NMMSE: 0.037637, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:16:19] Epoch 176/200, Loss: 19.084215, Train_MMSE: 0.022663, NMMSE: 0.037626, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:17:15] Epoch 177/200, Loss: 19.181849, Train_MMSE: 0.022661, NMMSE: 0.037637, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:18:15] Epoch 178/200, Loss: 19.090284, Train_MMSE: 0.022664, NMMSE: 0.037646, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:19:16] Epoch 179/200, Loss: 19.115543, Train_MMSE: 0.022659, NMMSE: 0.037641, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:20:15] Epoch 180/200, Loss: 19.050585, Train_MMSE: 0.022665, NMMSE: 0.037631, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:21:16] Epoch 181/200, Loss: 18.999628, Train_MMSE: 0.022663, NMMSE: 0.03764, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:22:20] Epoch 182/200, Loss: 18.997402, Train_MMSE: 0.022662, NMMSE: 0.037645, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:23:21] Epoch 183/200, Loss: 19.062263, Train_MMSE: 0.022658, NMMSE: 0.03764, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:24:11] Epoch 184/200, Loss: 19.026974, Train_MMSE: 0.022657, NMMSE: 0.037646, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:25:04] Epoch 185/200, Loss: 19.200697, Train_MMSE: 0.022665, NMMSE: 0.03764, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:25:53] Epoch 186/200, Loss: 19.129679, Train_MMSE: 0.022665, NMMSE: 0.037644, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:26:46] Epoch 187/200, Loss: 19.098436, Train_MMSE: 0.02266, NMMSE: 0.037628, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:27:34] Epoch 188/200, Loss: 18.962433, Train_MMSE: 0.022657, NMMSE: 0.037649, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:28:23] Epoch 189/200, Loss: 19.141485, Train_MMSE: 0.022662, NMMSE: 0.037639, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:29:12] Epoch 190/200, Loss: 19.105467, Train_MMSE: 0.022658, NMMSE: 0.037643, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:30:01] Epoch 191/200, Loss: 19.039820, Train_MMSE: 0.022659, NMMSE: 0.037641, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:30:52] Epoch 192/200, Loss: 19.018484, Train_MMSE: 0.022662, NMMSE: 0.037637, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:31:42] Epoch 193/200, Loss: 19.063745, Train_MMSE: 0.022656, NMMSE: 0.037659, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:32:32] Epoch 194/200, Loss: 19.026411, Train_MMSE: 0.022654, NMMSE: 0.037653, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:33:26] Epoch 195/200, Loss: 19.060816, Train_MMSE: 0.022663, NMMSE: 0.037656, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:34:16] Epoch 196/200, Loss: 18.995083, Train_MMSE: 0.022657, NMMSE: 0.037658, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:35:06] Epoch 197/200, Loss: 18.919655, Train_MMSE: 0.022663, NMMSE: 0.037652, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:35:58] Epoch 198/200, Loss: 19.056574, Train_MMSE: 0.022659, NMMSE: 0.03765, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:36:48] Epoch 199/200, Loss: 18.993568, Train_MMSE: 0.022653, NMMSE: 0.037651, LS_NMSE: 0.038781, Lr: 1e-05
[2025-02-23 00:37:30] Epoch 200/200, Loss: 19.010271, Train_MMSE: 0.022662, NMMSE: 0.037661, LS_NMSE: 0.038781, Lr: 1.0000000000000002e-06
