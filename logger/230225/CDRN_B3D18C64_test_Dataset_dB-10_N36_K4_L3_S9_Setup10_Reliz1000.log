Train.py PID: 22237

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.02615903921953831
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L3_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L3_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L3_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f8df7b75a30>
loss function:: L1Loss()
[2025-02-22 21:39:54] Epoch 1/200, Loss: 33.956409, Train_MMSE: 0.070339, NMMSE: 0.056493, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:40:07] Epoch 2/200, Loss: 33.328117, Train_MMSE: 0.067983, NMMSE: 0.053857, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:40:20] Epoch 3/200, Loss: 32.673550, Train_MMSE: 0.064784, NMMSE: 0.05186, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:40:33] Epoch 4/200, Loss: 32.058460, Train_MMSE: 0.062179, NMMSE: 0.050299, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:40:45] Epoch 5/200, Loss: 31.597887, Train_MMSE: 0.060029, NMMSE: 0.048862, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:40:58] Epoch 6/200, Loss: 30.982477, Train_MMSE: 0.058217, NMMSE: 0.048012, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:41:11] Epoch 7/200, Loss: 30.670650, Train_MMSE: 0.0568, NMMSE: 0.047081, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:41:24] Epoch 8/200, Loss: 30.204262, Train_MMSE: 0.05549, NMMSE: 0.046214, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:41:36] Epoch 9/200, Loss: 30.153687, Train_MMSE: 0.054162, NMMSE: 0.045067, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:41:49] Epoch 10/200, Loss: 29.682760, Train_MMSE: 0.05298, NMMSE: 0.044226, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:42:02] Epoch 11/200, Loss: 29.249018, Train_MMSE: 0.051947, NMMSE: 0.043541, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:42:15] Epoch 12/200, Loss: 29.179010, Train_MMSE: 0.050971, NMMSE: 0.043034, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:42:28] Epoch 13/200, Loss: 28.725122, Train_MMSE: 0.05018, NMMSE: 0.042454, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:42:40] Epoch 14/200, Loss: 28.544306, Train_MMSE: 0.049492, NMMSE: 0.042176, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:42:54] Epoch 15/200, Loss: 28.542402, Train_MMSE: 0.048962, NMMSE: 0.042028, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:43:10] Epoch 16/200, Loss: 28.510477, Train_MMSE: 0.048479, NMMSE: 0.04178, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:43:34] Epoch 17/200, Loss: 28.237831, Train_MMSE: 0.048103, NMMSE: 0.041596, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:44:04] Epoch 18/200, Loss: 28.244282, Train_MMSE: 0.047762, NMMSE: 0.041572, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:44:37] Epoch 19/200, Loss: 28.028650, Train_MMSE: 0.047413, NMMSE: 0.041304, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:45:16] Epoch 20/200, Loss: 27.692129, Train_MMSE: 0.047121, NMMSE: 0.041115, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:45:57] Epoch 21/200, Loss: 27.720510, Train_MMSE: 0.046792, NMMSE: 0.040932, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:46:39] Epoch 22/200, Loss: 27.909010, Train_MMSE: 0.046504, NMMSE: 0.040962, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:47:22] Epoch 23/200, Loss: 27.640688, Train_MMSE: 0.04617, NMMSE: 0.040685, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:48:02] Epoch 24/200, Loss: 27.542294, Train_MMSE: 0.045872, NMMSE: 0.040785, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:48:44] Epoch 25/200, Loss: 27.467529, Train_MMSE: 0.045621, NMMSE: 0.04046, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:49:27] Epoch 26/200, Loss: 27.328337, Train_MMSE: 0.04532, NMMSE: 0.040592, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:50:12] Epoch 27/200, Loss: 27.399912, Train_MMSE: 0.045104, NMMSE: 0.040955, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:50:54] Epoch 28/200, Loss: 27.452698, Train_MMSE: 0.044882, NMMSE: 0.040486, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:51:36] Epoch 29/200, Loss: 27.199196, Train_MMSE: 0.044638, NMMSE: 0.040425, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:52:19] Epoch 30/200, Loss: 27.125071, Train_MMSE: 0.044458, NMMSE: 0.040289, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:53:06] Epoch 31/200, Loss: 27.054314, Train_MMSE: 0.04429, NMMSE: 0.040534, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:53:48] Epoch 32/200, Loss: 26.979795, Train_MMSE: 0.044135, NMMSE: 0.040502, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:54:32] Epoch 33/200, Loss: 26.926531, Train_MMSE: 0.043978, NMMSE: 0.040382, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:55:16] Epoch 34/200, Loss: 27.023027, Train_MMSE: 0.043826, NMMSE: 0.040562, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:55:59] Epoch 35/200, Loss: 26.878532, Train_MMSE: 0.043622, NMMSE: 0.040593, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:56:44] Epoch 36/200, Loss: 26.668058, Train_MMSE: 0.043487, NMMSE: 0.04053, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:57:27] Epoch 37/200, Loss: 26.818373, Train_MMSE: 0.043304, NMMSE: 0.040935, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:58:11] Epoch 38/200, Loss: 26.728882, Train_MMSE: 0.043163, NMMSE: 0.040509, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:58:55] Epoch 39/200, Loss: 26.775486, Train_MMSE: 0.043022, NMMSE: 0.040664, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 21:59:39] Epoch 40/200, Loss: 26.681805, Train_MMSE: 0.042852, NMMSE: 0.040657, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:00:23] Epoch 41/200, Loss: 26.703493, Train_MMSE: 0.0427, NMMSE: 0.040988, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:01:09] Epoch 42/200, Loss: 26.534807, Train_MMSE: 0.042578, NMMSE: 0.040633, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:01:52] Epoch 43/200, Loss: 26.489237, Train_MMSE: 0.042389, NMMSE: 0.040421, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:02:37] Epoch 44/200, Loss: 26.367912, Train_MMSE: 0.042293, NMMSE: 0.040674, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:03:19] Epoch 45/200, Loss: 26.341925, Train_MMSE: 0.042153, NMMSE: 0.040505, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:04:04] Epoch 46/200, Loss: 26.350451, Train_MMSE: 0.042009, NMMSE: 0.040435, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:04:49] Epoch 47/200, Loss: 26.445671, Train_MMSE: 0.04188, NMMSE: 0.040802, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:05:34] Epoch 48/200, Loss: 26.394367, Train_MMSE: 0.041787, NMMSE: 0.040722, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:06:18] Epoch 49/200, Loss: 26.361376, Train_MMSE: 0.041663, NMMSE: 0.040645, LS_NMSE: 0.057274, Lr: 0.01
[2025-02-22 22:07:02] Epoch 50/200, Loss: 26.187258, Train_MMSE: 0.041526, NMMSE: 0.040927, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:07:45] Epoch 51/200, Loss: 25.238588, Train_MMSE: 0.039562, NMMSE: 0.040459, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:08:27] Epoch 52/200, Loss: 25.062057, Train_MMSE: 0.038915, NMMSE: 0.040664, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:09:10] Epoch 53/200, Loss: 24.968410, Train_MMSE: 0.038692, NMMSE: 0.040762, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:09:54] Epoch 54/200, Loss: 25.100359, Train_MMSE: 0.038548, NMMSE: 0.041029, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:10:37] Epoch 55/200, Loss: 24.872665, Train_MMSE: 0.038429, NMMSE: 0.041056, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:11:22] Epoch 56/200, Loss: 24.923178, Train_MMSE: 0.038327, NMMSE: 0.041202, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:12:06] Epoch 57/200, Loss: 24.824272, Train_MMSE: 0.038228, NMMSE: 0.041234, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:12:49] Epoch 58/200, Loss: 24.846310, Train_MMSE: 0.038141, NMMSE: 0.041291, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:13:32] Epoch 59/200, Loss: 24.778860, Train_MMSE: 0.038076, NMMSE: 0.04142, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:14:14] Epoch 60/200, Loss: 24.734257, Train_MMSE: 0.037991, NMMSE: 0.041455, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:14:59] Epoch 61/200, Loss: 24.893833, Train_MMSE: 0.037923, NMMSE: 0.041488, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:15:44] Epoch 62/200, Loss: 24.752192, Train_MMSE: 0.037859, NMMSE: 0.041547, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:16:30] Epoch 63/200, Loss: 24.796051, Train_MMSE: 0.037797, NMMSE: 0.041658, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:17:14] Epoch 64/200, Loss: 24.670208, Train_MMSE: 0.037737, NMMSE: 0.041787, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:17:59] Epoch 65/200, Loss: 24.686165, Train_MMSE: 0.037676, NMMSE: 0.041796, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:18:43] Epoch 66/200, Loss: 24.599726, Train_MMSE: 0.037623, NMMSE: 0.04181, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:19:27] Epoch 67/200, Loss: 24.721941, Train_MMSE: 0.037565, NMMSE: 0.041872, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:20:11] Epoch 68/200, Loss: 24.617348, Train_MMSE: 0.037511, NMMSE: 0.041999, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:20:55] Epoch 69/200, Loss: 24.691864, Train_MMSE: 0.037449, NMMSE: 0.041981, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:21:40] Epoch 70/200, Loss: 24.472147, Train_MMSE: 0.037399, NMMSE: 0.042079, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:22:26] Epoch 71/200, Loss: 24.596903, Train_MMSE: 0.037348, NMMSE: 0.042043, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:23:09] Epoch 72/200, Loss: 24.549465, Train_MMSE: 0.037293, NMMSE: 0.042183, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:23:52] Epoch 73/200, Loss: 24.443537, Train_MMSE: 0.037247, NMMSE: 0.042127, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:24:37] Epoch 74/200, Loss: 24.502712, Train_MMSE: 0.03719, NMMSE: 0.042228, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:25:20] Epoch 75/200, Loss: 24.573084, Train_MMSE: 0.037157, NMMSE: 0.04225, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:26:04] Epoch 76/200, Loss: 24.227070, Train_MMSE: 0.037107, NMMSE: 0.042288, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:26:46] Epoch 77/200, Loss: 24.352610, Train_MMSE: 0.037064, NMMSE: 0.042341, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:27:30] Epoch 78/200, Loss: 24.405910, Train_MMSE: 0.037015, NMMSE: 0.042453, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:28:14] Epoch 79/200, Loss: 24.333815, Train_MMSE: 0.03697, NMMSE: 0.042433, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:28:58] Epoch 80/200, Loss: 24.471476, Train_MMSE: 0.036941, NMMSE: 0.042705, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:29:42] Epoch 81/200, Loss: 24.416307, Train_MMSE: 0.036889, NMMSE: 0.042566, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:30:25] Epoch 82/200, Loss: 24.611830, Train_MMSE: 0.036857, NMMSE: 0.04275, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:31:08] Epoch 83/200, Loss: 24.224154, Train_MMSE: 0.036807, NMMSE: 0.042659, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:31:53] Epoch 84/200, Loss: 24.390146, Train_MMSE: 0.036759, NMMSE: 0.04261, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:32:36] Epoch 85/200, Loss: 24.334785, Train_MMSE: 0.036728, NMMSE: 0.042692, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:33:21] Epoch 86/200, Loss: 24.243322, Train_MMSE: 0.036686, NMMSE: 0.042787, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:34:04] Epoch 87/200, Loss: 24.265385, Train_MMSE: 0.03665, NMMSE: 0.042927, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:34:48] Epoch 88/200, Loss: 24.256144, Train_MMSE: 0.036618, NMMSE: 0.042879, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:35:35] Epoch 89/200, Loss: 24.448267, Train_MMSE: 0.036578, NMMSE: 0.042941, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:36:19] Epoch 90/200, Loss: 24.320929, Train_MMSE: 0.036539, NMMSE: 0.042893, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:37:03] Epoch 91/200, Loss: 24.280569, Train_MMSE: 0.0365, NMMSE: 0.04307, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:37:44] Epoch 92/200, Loss: 24.247694, Train_MMSE: 0.036456, NMMSE: 0.04297, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:38:26] Epoch 93/200, Loss: 24.234348, Train_MMSE: 0.03643, NMMSE: 0.043127, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:39:10] Epoch 94/200, Loss: 24.203291, Train_MMSE: 0.036399, NMMSE: 0.043103, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:39:51] Epoch 95/200, Loss: 24.158573, Train_MMSE: 0.036353, NMMSE: 0.043228, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:40:31] Epoch 96/200, Loss: 24.305908, Train_MMSE: 0.03631, NMMSE: 0.043117, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:41:10] Epoch 97/200, Loss: 24.149851, Train_MMSE: 0.036286, NMMSE: 0.043194, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:41:59] Epoch 98/200, Loss: 24.164503, Train_MMSE: 0.03625, NMMSE: 0.04328, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:42:50] Epoch 99/200, Loss: 24.020166, Train_MMSE: 0.036226, NMMSE: 0.043422, LS_NMSE: 0.057274, Lr: 0.001
[2025-02-22 22:43:39] Epoch 100/200, Loss: 24.110123, Train_MMSE: 0.036193, NMMSE: 0.043301, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:44:27] Epoch 101/200, Loss: 23.788233, Train_MMSE: 0.035633, NMMSE: 0.043381, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:45:14] Epoch 102/200, Loss: 23.814432, Train_MMSE: 0.035512, NMMSE: 0.043457, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:46:05] Epoch 103/200, Loss: 23.764950, Train_MMSE: 0.0355, NMMSE: 0.043462, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:47:04] Epoch 104/200, Loss: 23.675360, Train_MMSE: 0.035478, NMMSE: 0.043491, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:48:07] Epoch 105/200, Loss: 23.743525, Train_MMSE: 0.035466, NMMSE: 0.043506, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:49:08] Epoch 106/200, Loss: 23.625826, Train_MMSE: 0.035457, NMMSE: 0.043548, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:50:08] Epoch 107/200, Loss: 23.736752, Train_MMSE: 0.035455, NMMSE: 0.04358, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:51:11] Epoch 108/200, Loss: 23.727053, Train_MMSE: 0.035443, NMMSE: 0.04358, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:52:12] Epoch 109/200, Loss: 23.824501, Train_MMSE: 0.035427, NMMSE: 0.04362, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:53:12] Epoch 110/200, Loss: 23.693480, Train_MMSE: 0.035424, NMMSE: 0.0436, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:54:11] Epoch 111/200, Loss: 23.765574, Train_MMSE: 0.035418, NMMSE: 0.04364, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:55:08] Epoch 112/200, Loss: 23.696846, Train_MMSE: 0.035407, NMMSE: 0.043632, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:56:07] Epoch 113/200, Loss: 23.632835, Train_MMSE: 0.035412, NMMSE: 0.043604, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:57:06] Epoch 114/200, Loss: 23.692434, Train_MMSE: 0.035403, NMMSE: 0.043649, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:58:06] Epoch 115/200, Loss: 23.695173, Train_MMSE: 0.035402, NMMSE: 0.043671, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 22:59:08] Epoch 116/200, Loss: 23.730982, Train_MMSE: 0.035394, NMMSE: 0.043674, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:00:11] Epoch 117/200, Loss: 23.840328, Train_MMSE: 0.035383, NMMSE: 0.043682, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:01:09] Epoch 118/200, Loss: 23.715885, Train_MMSE: 0.035377, NMMSE: 0.043704, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:02:06] Epoch 119/200, Loss: 23.766331, Train_MMSE: 0.035367, NMMSE: 0.043703, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:03:05] Epoch 120/200, Loss: 23.701843, Train_MMSE: 0.035363, NMMSE: 0.04371, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:04:04] Epoch 121/200, Loss: 23.721935, Train_MMSE: 0.035367, NMMSE: 0.043721, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:05:01] Epoch 122/200, Loss: 23.721710, Train_MMSE: 0.035358, NMMSE: 0.043731, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:05:59] Epoch 123/200, Loss: 23.723253, Train_MMSE: 0.035347, NMMSE: 0.043743, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:06:59] Epoch 124/200, Loss: 23.813690, Train_MMSE: 0.03534, NMMSE: 0.043777, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:07:55] Epoch 125/200, Loss: 23.874908, Train_MMSE: 0.035341, NMMSE: 0.043755, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:08:50] Epoch 126/200, Loss: 23.690336, Train_MMSE: 0.035333, NMMSE: 0.043742, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:09:46] Epoch 127/200, Loss: 23.760736, Train_MMSE: 0.03533, NMMSE: 0.043803, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:10:46] Epoch 128/200, Loss: 23.702868, Train_MMSE: 0.03532, NMMSE: 0.043803, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:11:44] Epoch 129/200, Loss: 23.575115, Train_MMSE: 0.035324, NMMSE: 0.043817, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:12:44] Epoch 130/200, Loss: 23.734140, Train_MMSE: 0.035314, NMMSE: 0.043793, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:13:44] Epoch 131/200, Loss: 23.675278, Train_MMSE: 0.03531, NMMSE: 0.043786, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:14:42] Epoch 132/200, Loss: 23.597286, Train_MMSE: 0.035304, NMMSE: 0.043831, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:15:46] Epoch 133/200, Loss: 23.587568, Train_MMSE: 0.035303, NMMSE: 0.043793, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:16:45] Epoch 134/200, Loss: 23.572546, Train_MMSE: 0.035295, NMMSE: 0.043827, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:17:42] Epoch 135/200, Loss: 23.654078, Train_MMSE: 0.035282, NMMSE: 0.043847, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:18:39] Epoch 136/200, Loss: 23.674437, Train_MMSE: 0.035278, NMMSE: 0.043849, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:19:40] Epoch 137/200, Loss: 23.678068, Train_MMSE: 0.035278, NMMSE: 0.04389, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:20:42] Epoch 138/200, Loss: 23.624285, Train_MMSE: 0.035273, NMMSE: 0.043896, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:21:41] Epoch 139/200, Loss: 23.670250, Train_MMSE: 0.035268, NMMSE: 0.043864, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:22:40] Epoch 140/200, Loss: 23.533899, Train_MMSE: 0.03527, NMMSE: 0.043897, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:23:38] Epoch 141/200, Loss: 23.759405, Train_MMSE: 0.035262, NMMSE: 0.043903, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:24:37] Epoch 142/200, Loss: 23.653988, Train_MMSE: 0.035256, NMMSE: 0.043878, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:25:33] Epoch 143/200, Loss: 23.643665, Train_MMSE: 0.035254, NMMSE: 0.043911, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:26:31] Epoch 144/200, Loss: 23.530514, Train_MMSE: 0.035248, NMMSE: 0.043922, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:27:27] Epoch 145/200, Loss: 23.511255, Train_MMSE: 0.035243, NMMSE: 0.043906, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:28:25] Epoch 146/200, Loss: 23.632036, Train_MMSE: 0.035239, NMMSE: 0.043924, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:29:25] Epoch 147/200, Loss: 23.535532, Train_MMSE: 0.035232, NMMSE: 0.043917, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:30:23] Epoch 148/200, Loss: 23.579601, Train_MMSE: 0.035237, NMMSE: 0.043931, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:31:26] Epoch 149/200, Loss: 23.629221, Train_MMSE: 0.03522, NMMSE: 0.043933, LS_NMSE: 0.057274, Lr: 0.0001
[2025-02-22 23:32:27] Epoch 150/200, Loss: 23.591623, Train_MMSE: 0.035228, NMMSE: 0.043941, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:33:25] Epoch 151/200, Loss: 23.530680, Train_MMSE: 0.035151, NMMSE: 0.043958, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:34:27] Epoch 152/200, Loss: 23.655561, Train_MMSE: 0.035142, NMMSE: 0.043974, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:35:28] Epoch 153/200, Loss: 23.645813, Train_MMSE: 0.035136, NMMSE: 0.043958, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:36:27] Epoch 154/200, Loss: 23.517685, Train_MMSE: 0.035141, NMMSE: 0.043956, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:37:26] Epoch 155/200, Loss: 23.516005, Train_MMSE: 0.035147, NMMSE: 0.04397, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:38:29] Epoch 156/200, Loss: 23.435432, Train_MMSE: 0.035141, NMMSE: 0.043971, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:39:32] Epoch 157/200, Loss: 23.562662, Train_MMSE: 0.035145, NMMSE: 0.043966, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:40:33] Epoch 158/200, Loss: 23.514605, Train_MMSE: 0.035136, NMMSE: 0.043981, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:41:32] Epoch 159/200, Loss: 23.533735, Train_MMSE: 0.035144, NMMSE: 0.043972, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:42:31] Epoch 160/200, Loss: 23.603622, Train_MMSE: 0.035135, NMMSE: 0.043979, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:43:31] Epoch 161/200, Loss: 23.634600, Train_MMSE: 0.035146, NMMSE: 0.043971, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:44:33] Epoch 162/200, Loss: 23.575663, Train_MMSE: 0.035141, NMMSE: 0.043985, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:45:32] Epoch 163/200, Loss: 23.585514, Train_MMSE: 0.035133, NMMSE: 0.04398, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:46:31] Epoch 164/200, Loss: 23.541229, Train_MMSE: 0.035139, NMMSE: 0.043976, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:47:33] Epoch 165/200, Loss: 23.657782, Train_MMSE: 0.035137, NMMSE: 0.043985, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:48:34] Epoch 166/200, Loss: 23.594929, Train_MMSE: 0.035135, NMMSE: 0.043976, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:49:33] Epoch 167/200, Loss: 23.581463, Train_MMSE: 0.035137, NMMSE: 0.043974, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:50:34] Epoch 168/200, Loss: 23.670328, Train_MMSE: 0.035139, NMMSE: 0.043984, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:51:38] Epoch 169/200, Loss: 23.598749, Train_MMSE: 0.035138, NMMSE: 0.04398, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:52:38] Epoch 170/200, Loss: 23.646969, Train_MMSE: 0.03513, NMMSE: 0.043982, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:53:39] Epoch 171/200, Loss: 23.551765, Train_MMSE: 0.035131, NMMSE: 0.043992, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:54:41] Epoch 172/200, Loss: 23.470200, Train_MMSE: 0.035134, NMMSE: 0.04399, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:55:43] Epoch 173/200, Loss: 23.507685, Train_MMSE: 0.035139, NMMSE: 0.043984, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:56:45] Epoch 174/200, Loss: 23.532423, Train_MMSE: 0.035136, NMMSE: 0.043984, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:57:47] Epoch 175/200, Loss: 23.706293, Train_MMSE: 0.035125, NMMSE: 0.043987, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:58:49] Epoch 176/200, Loss: 23.636328, Train_MMSE: 0.035128, NMMSE: 0.043975, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-22 23:59:50] Epoch 177/200, Loss: 23.649456, Train_MMSE: 0.035137, NMMSE: 0.043975, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:00:50] Epoch 178/200, Loss: 23.598488, Train_MMSE: 0.035128, NMMSE: 0.043994, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:01:49] Epoch 179/200, Loss: 23.601479, Train_MMSE: 0.035138, NMMSE: 0.044001, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:02:48] Epoch 180/200, Loss: 23.512587, Train_MMSE: 0.035129, NMMSE: 0.043989, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:03:47] Epoch 181/200, Loss: 23.560286, Train_MMSE: 0.035127, NMMSE: 0.043979, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:04:47] Epoch 182/200, Loss: 23.638842, Train_MMSE: 0.035133, NMMSE: 0.043983, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:05:47] Epoch 183/200, Loss: 23.552620, Train_MMSE: 0.035127, NMMSE: 0.043998, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:06:45] Epoch 184/200, Loss: 23.627293, Train_MMSE: 0.035131, NMMSE: 0.043988, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:07:44] Epoch 185/200, Loss: 23.529091, Train_MMSE: 0.035128, NMMSE: 0.043983, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:08:45] Epoch 186/200, Loss: 23.578060, Train_MMSE: 0.035123, NMMSE: 0.044001, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:09:48] Epoch 187/200, Loss: 23.700218, Train_MMSE: 0.035127, NMMSE: 0.043975, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:10:55] Epoch 188/200, Loss: 23.428085, Train_MMSE: 0.035122, NMMSE: 0.044001, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:11:56] Epoch 189/200, Loss: 23.641668, Train_MMSE: 0.035126, NMMSE: 0.043995, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:12:56] Epoch 190/200, Loss: 23.679623, Train_MMSE: 0.035121, NMMSE: 0.043991, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:13:56] Epoch 191/200, Loss: 23.597670, Train_MMSE: 0.035119, NMMSE: 0.043991, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:14:57] Epoch 192/200, Loss: 23.486544, Train_MMSE: 0.035125, NMMSE: 0.043998, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:16:00] Epoch 193/200, Loss: 23.447180, Train_MMSE: 0.035119, NMMSE: 0.043998, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:17:02] Epoch 194/200, Loss: 23.553627, Train_MMSE: 0.035125, NMMSE: 0.043993, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:18:02] Epoch 195/200, Loss: 23.514774, Train_MMSE: 0.035121, NMMSE: 0.044022, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:19:05] Epoch 196/200, Loss: 23.536852, Train_MMSE: 0.035123, NMMSE: 0.044003, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:20:08] Epoch 197/200, Loss: 23.540228, Train_MMSE: 0.035124, NMMSE: 0.044013, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:21:10] Epoch 198/200, Loss: 23.614542, Train_MMSE: 0.035118, NMMSE: 0.044015, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:22:12] Epoch 199/200, Loss: 23.510855, Train_MMSE: 0.035117, NMMSE: 0.04401, LS_NMSE: 0.057274, Lr: 1e-05
[2025-02-23 00:23:13] Epoch 200/200, Loss: 23.556726, Train_MMSE: 0.035115, NMMSE: 0.044018, LS_NMSE: 0.057274, Lr: 1.0000000000000002e-06
