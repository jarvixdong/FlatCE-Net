Train.py PID: 36957

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.009842292867443248
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S11_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S11_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/flatCE_L3C32_test_Dataset_dB-10_N36_K4_L4_S11_Setup10_Reliz1000.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f4f13a961e0>
loss function:: L1Loss()
[2025-02-22 22:20:25] Epoch 1/200, Loss: 67.317871, Train_MMSE: 0.809354, NMMSE: 0.739143, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:21:08] Epoch 2/200, Loss: 65.989586, Train_MMSE: 0.506495, NMMSE: 0.508472, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:21:56] Epoch 3/200, Loss: 65.229820, Train_MMSE: 0.505314, NMMSE: 0.507112, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:22:51] Epoch 4/200, Loss: 65.380646, Train_MMSE: 0.50463, NMMSE: 0.50693, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:23:46] Epoch 5/200, Loss: 65.949097, Train_MMSE: 0.504697, NMMSE: 0.506838, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:24:42] Epoch 6/200, Loss: 66.071579, Train_MMSE: 0.504588, NMMSE: 0.50685, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:25:38] Epoch 7/200, Loss: 65.637146, Train_MMSE: 0.504555, NMMSE: 0.506774, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:26:33] Epoch 8/200, Loss: 65.597878, Train_MMSE: 0.504647, NMMSE: 0.506706, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:27:29] Epoch 9/200, Loss: 65.964149, Train_MMSE: 0.504557, NMMSE: 0.50678, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:28:25] Epoch 10/200, Loss: 66.418579, Train_MMSE: 0.504536, NMMSE: 0.506745, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:29:19] Epoch 11/200, Loss: 66.444443, Train_MMSE: 0.504533, NMMSE: 0.507003, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:30:16] Epoch 12/200, Loss: 66.715347, Train_MMSE: 0.504595, NMMSE: 0.506976, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:31:12] Epoch 13/200, Loss: 65.949501, Train_MMSE: 0.50454, NMMSE: 0.506746, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:32:08] Epoch 14/200, Loss: 66.254913, Train_MMSE: 0.504496, NMMSE: 0.506754, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:33:04] Epoch 15/200, Loss: 66.657196, Train_MMSE: 0.504544, NMMSE: 0.507036, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:34:00] Epoch 16/200, Loss: 66.225883, Train_MMSE: 0.504573, NMMSE: 0.50732, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:34:55] Epoch 17/200, Loss: 65.022331, Train_MMSE: 0.504551, NMMSE: 0.506876, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:35:51] Epoch 18/200, Loss: 65.620049, Train_MMSE: 0.50456, NMMSE: 0.506696, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:36:47] Epoch 19/200, Loss: 66.617462, Train_MMSE: 0.504555, NMMSE: 0.506962, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:37:42] Epoch 20/200, Loss: 65.986748, Train_MMSE: 0.504494, NMMSE: 0.5068, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:38:38] Epoch 21/200, Loss: 66.118294, Train_MMSE: 0.504537, NMMSE: 0.507003, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:39:36] Epoch 22/200, Loss: 65.579170, Train_MMSE: 0.504465, NMMSE: 0.506689, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:40:32] Epoch 23/200, Loss: 65.998711, Train_MMSE: 0.504526, NMMSE: 0.506638, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:41:27] Epoch 24/200, Loss: 66.454086, Train_MMSE: 0.504478, NMMSE: 0.506699, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:42:23] Epoch 25/200, Loss: 66.352394, Train_MMSE: 0.504473, NMMSE: 0.507622, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:43:18] Epoch 26/200, Loss: 66.661163, Train_MMSE: 0.503793, NMMSE: 0.502956, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:44:13] Epoch 27/200, Loss: 13.122239, Train_MMSE: 0.230996, NMMSE: 0.050426, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:45:08] Epoch 28/200, Loss: 12.691022, Train_MMSE: 0.010562, NMMSE: 0.012651, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:46:02] Epoch 29/200, Loss: 12.931952, Train_MMSE: 0.010046, NMMSE: 0.012121, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:46:55] Epoch 30/200, Loss: 12.651938, Train_MMSE: 0.009951, NMMSE: 0.011984, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:47:50] Epoch 31/200, Loss: 12.697101, Train_MMSE: 0.009875, NMMSE: 0.012144, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:48:48] Epoch 32/200, Loss: 12.819650, Train_MMSE: 0.009692, NMMSE: 0.011449, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:49:45] Epoch 33/200, Loss: 12.357239, Train_MMSE: 0.009726, NMMSE: 0.011215, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:50:43] Epoch 34/200, Loss: 12.472658, Train_MMSE: 0.009653, NMMSE: 0.011462, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:51:40] Epoch 35/200, Loss: 12.899144, Train_MMSE: 0.009722, NMMSE: 0.011463, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:52:38] Epoch 36/200, Loss: 12.491635, Train_MMSE: 0.009679, NMMSE: 0.011591, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:53:35] Epoch 37/200, Loss: 12.567250, Train_MMSE: 0.009716, NMMSE: 0.011552, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:54:33] Epoch 38/200, Loss: 12.307012, Train_MMSE: 0.009748, NMMSE: 0.011745, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:55:31] Epoch 39/200, Loss: 12.464094, Train_MMSE: 0.009739, NMMSE: 0.011096, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:56:29] Epoch 40/200, Loss: 12.445499, Train_MMSE: 0.009935, NMMSE: 0.011481, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:57:26] Epoch 41/200, Loss: 12.342777, Train_MMSE: 0.00966, NMMSE: 0.011291, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:58:24] Epoch 42/200, Loss: 12.560155, Train_MMSE: 0.009508, NMMSE: 0.011034, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 22:59:23] Epoch 43/200, Loss: 12.261205, Train_MMSE: 0.009584, NMMSE: 0.011698, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:00:21] Epoch 44/200, Loss: 12.386486, Train_MMSE: 0.009579, NMMSE: 0.011383, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:01:19] Epoch 45/200, Loss: 12.999959, Train_MMSE: 0.009566, NMMSE: 0.011268, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:02:18] Epoch 46/200, Loss: 12.407761, Train_MMSE: 0.00974, NMMSE: 0.01119, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:03:16] Epoch 47/200, Loss: 12.432727, Train_MMSE: 0.009569, NMMSE: 0.011291, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:04:15] Epoch 48/200, Loss: 12.291669, Train_MMSE: 0.009511, NMMSE: 0.011234, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:05:12] Epoch 49/200, Loss: 12.427358, Train_MMSE: 0.009479, NMMSE: 0.011069, LS_NMSE: 0.011111, Lr: 0.01
[2025-02-22 23:06:10] Epoch 50/200, Loss: 12.752092, Train_MMSE: 0.009445, NMMSE: 0.011191, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:07:09] Epoch 51/200, Loss: 12.075477, Train_MMSE: 0.009122, NMMSE: 0.010869, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:08:08] Epoch 52/200, Loss: 12.110820, Train_MMSE: 0.009078, NMMSE: 0.010876, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:09:02] Epoch 53/200, Loss: 12.120034, Train_MMSE: 0.009077, NMMSE: 0.010892, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:09:55] Epoch 54/200, Loss: 12.207413, Train_MMSE: 0.009078, NMMSE: 0.010919, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:10:47] Epoch 55/200, Loss: 12.116049, Train_MMSE: 0.009051, NMMSE: 0.010928, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:11:41] Epoch 56/200, Loss: 12.242183, Train_MMSE: 0.009022, NMMSE: 0.010961, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:12:33] Epoch 57/200, Loss: 12.026285, Train_MMSE: 0.009036, NMMSE: 0.010935, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:13:25] Epoch 58/200, Loss: 12.088843, Train_MMSE: 0.008987, NMMSE: 0.010995, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:14:17] Epoch 59/200, Loss: 12.009260, Train_MMSE: 0.008971, NMMSE: 0.010988, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:15:08] Epoch 60/200, Loss: 12.046066, Train_MMSE: 0.008971, NMMSE: 0.011018, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:16:01] Epoch 61/200, Loss: 12.151533, Train_MMSE: 0.008947, NMMSE: 0.010999, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:16:53] Epoch 62/200, Loss: 12.319911, Train_MMSE: 0.008968, NMMSE: 0.011108, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:17:44] Epoch 63/200, Loss: 12.020946, Train_MMSE: 0.008948, NMMSE: 0.011065, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:18:34] Epoch 64/200, Loss: 12.223321, Train_MMSE: 0.008939, NMMSE: 0.01105, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:19:20] Epoch 65/200, Loss: 12.317079, Train_MMSE: 0.008917, NMMSE: 0.011088, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:20:06] Epoch 66/200, Loss: 11.968152, Train_MMSE: 0.008899, NMMSE: 0.011106, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:20:53] Epoch 67/200, Loss: 11.986033, Train_MMSE: 0.008912, NMMSE: 0.011119, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:21:40] Epoch 68/200, Loss: 12.021355, Train_MMSE: 0.008888, NMMSE: 0.011093, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:22:24] Epoch 69/200, Loss: 11.976650, Train_MMSE: 0.008879, NMMSE: 0.011119, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:23:10] Epoch 70/200, Loss: 11.991137, Train_MMSE: 0.008856, NMMSE: 0.011135, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:23:57] Epoch 71/200, Loss: 11.951543, Train_MMSE: 0.008815, NMMSE: 0.011135, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:24:42] Epoch 72/200, Loss: 11.988778, Train_MMSE: 0.008836, NMMSE: 0.011186, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:25:29] Epoch 73/200, Loss: 11.880968, Train_MMSE: 0.008807, NMMSE: 0.01121, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:26:15] Epoch 74/200, Loss: 11.902488, Train_MMSE: 0.008819, NMMSE: 0.011204, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:27:00] Epoch 75/200, Loss: 11.940640, Train_MMSE: 0.008792, NMMSE: 0.011212, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:27:46] Epoch 76/200, Loss: 11.885406, Train_MMSE: 0.008828, NMMSE: 0.011203, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:28:32] Epoch 77/200, Loss: 11.960497, Train_MMSE: 0.00879, NMMSE: 0.011238, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:29:17] Epoch 78/200, Loss: 11.943777, Train_MMSE: 0.008743, NMMSE: 0.011232, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:30:03] Epoch 79/200, Loss: 11.869498, Train_MMSE: 0.008765, NMMSE: 0.011252, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:30:48] Epoch 80/200, Loss: 11.992103, Train_MMSE: 0.008747, NMMSE: 0.011262, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:31:34] Epoch 81/200, Loss: 11.773297, Train_MMSE: 0.008748, NMMSE: 0.01128, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:32:19] Epoch 82/200, Loss: 11.875300, Train_MMSE: 0.008745, NMMSE: 0.011298, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:33:05] Epoch 83/200, Loss: 11.809954, Train_MMSE: 0.008768, NMMSE: 0.011309, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:33:51] Epoch 84/200, Loss: 11.869777, Train_MMSE: 0.008706, NMMSE: 0.01131, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:34:37] Epoch 85/200, Loss: 11.853070, Train_MMSE: 0.008686, NMMSE: 0.011418, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:35:23] Epoch 86/200, Loss: 11.823667, Train_MMSE: 0.008711, NMMSE: 0.011337, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:36:09] Epoch 87/200, Loss: 11.783736, Train_MMSE: 0.008678, NMMSE: 0.011359, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:36:55] Epoch 88/200, Loss: 12.093111, Train_MMSE: 0.00869, NMMSE: 0.011351, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:37:41] Epoch 89/200, Loss: 12.007267, Train_MMSE: 0.008674, NMMSE: 0.011381, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:38:27] Epoch 90/200, Loss: 11.811062, Train_MMSE: 0.00867, NMMSE: 0.011353, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:39:14] Epoch 91/200, Loss: 11.965524, Train_MMSE: 0.008627, NMMSE: 0.011381, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:40:00] Epoch 92/200, Loss: 11.884835, Train_MMSE: 0.008647, NMMSE: 0.01137, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:40:46] Epoch 93/200, Loss: 11.780228, Train_MMSE: 0.008642, NMMSE: 0.011436, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:41:31] Epoch 94/200, Loss: 11.788054, Train_MMSE: 0.008607, NMMSE: 0.011411, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:42:17] Epoch 95/200, Loss: 11.904912, Train_MMSE: 0.008592, NMMSE: 0.011429, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:43:03] Epoch 96/200, Loss: 11.796119, Train_MMSE: 0.008578, NMMSE: 0.011426, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:43:48] Epoch 97/200, Loss: 12.022707, Train_MMSE: 0.008628, NMMSE: 0.011492, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:44:34] Epoch 98/200, Loss: 11.726562, Train_MMSE: 0.00856, NMMSE: 0.011462, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:45:20] Epoch 99/200, Loss: 11.838842, Train_MMSE: 0.008583, NMMSE: 0.011479, LS_NMSE: 0.011111, Lr: 0.001
[2025-02-22 23:46:06] Epoch 100/200, Loss: 11.744095, Train_MMSE: 0.008567, NMMSE: 0.011459, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:46:52] Epoch 101/200, Loss: 11.667007, Train_MMSE: 0.008415, NMMSE: 0.011554, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:47:38] Epoch 102/200, Loss: 11.589650, Train_MMSE: 0.008389, NMMSE: 0.011609, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:48:23] Epoch 103/200, Loss: 11.541510, Train_MMSE: 0.00837, NMMSE: 0.011631, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:49:09] Epoch 104/200, Loss: 11.841078, Train_MMSE: 0.008386, NMMSE: 0.011647, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:49:54] Epoch 105/200, Loss: 11.655005, Train_MMSE: 0.008378, NMMSE: 0.01166, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:50:39] Epoch 106/200, Loss: 11.633804, Train_MMSE: 0.008358, NMMSE: 0.011677, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:51:23] Epoch 107/200, Loss: 11.546060, Train_MMSE: 0.008335, NMMSE: 0.011683, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:52:09] Epoch 108/200, Loss: 11.544767, Train_MMSE: 0.008336, NMMSE: 0.011697, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:52:54] Epoch 109/200, Loss: 11.537317, Train_MMSE: 0.008337, NMMSE: 0.011709, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:53:40] Epoch 110/200, Loss: 11.552604, Train_MMSE: 0.008324, NMMSE: 0.011702, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:54:26] Epoch 111/200, Loss: 11.719982, Train_MMSE: 0.008334, NMMSE: 0.011714, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:55:12] Epoch 112/200, Loss: 11.526471, Train_MMSE: 0.008331, NMMSE: 0.011709, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:55:57] Epoch 113/200, Loss: 11.502620, Train_MMSE: 0.008339, NMMSE: 0.011729, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:56:43] Epoch 114/200, Loss: 11.537406, Train_MMSE: 0.008323, NMMSE: 0.011745, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:57:28] Epoch 115/200, Loss: 11.666794, Train_MMSE: 0.008342, NMMSE: 0.011741, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:58:14] Epoch 116/200, Loss: 11.510840, Train_MMSE: 0.0083, NMMSE: 0.011742, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:58:59] Epoch 117/200, Loss: 11.517946, Train_MMSE: 0.008328, NMMSE: 0.011759, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-22 23:59:45] Epoch 118/200, Loss: 11.498483, Train_MMSE: 0.008325, NMMSE: 0.011753, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:00:30] Epoch 119/200, Loss: 11.529401, Train_MMSE: 0.008315, NMMSE: 0.011761, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:01:15] Epoch 120/200, Loss: 11.535947, Train_MMSE: 0.0083, NMMSE: 0.011773, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:02:01] Epoch 121/200, Loss: 11.701440, Train_MMSE: 0.00831, NMMSE: 0.011782, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:02:46] Epoch 122/200, Loss: 11.674031, Train_MMSE: 0.008324, NMMSE: 0.011774, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:03:32] Epoch 123/200, Loss: 11.477858, Train_MMSE: 0.008303, NMMSE: 0.011777, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:04:17] Epoch 124/200, Loss: 11.521824, Train_MMSE: 0.008311, NMMSE: 0.011778, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:05:02] Epoch 125/200, Loss: 11.662381, Train_MMSE: 0.008292, NMMSE: 0.011783, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:05:48] Epoch 126/200, Loss: 11.594610, Train_MMSE: 0.008312, NMMSE: 0.011788, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:06:33] Epoch 127/200, Loss: 11.533142, Train_MMSE: 0.008308, NMMSE: 0.011803, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:07:18] Epoch 128/200, Loss: 11.560256, Train_MMSE: 0.008318, NMMSE: 0.01181, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:08:00] Epoch 129/200, Loss: 11.498619, Train_MMSE: 0.008306, NMMSE: 0.011813, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:08:37] Epoch 130/200, Loss: 11.634147, Train_MMSE: 0.008293, NMMSE: 0.01182, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:09:14] Epoch 131/200, Loss: 11.471785, Train_MMSE: 0.008293, NMMSE: 0.011821, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:09:51] Epoch 132/200, Loss: 11.466697, Train_MMSE: 0.008284, NMMSE: 0.011819, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:10:28] Epoch 133/200, Loss: 11.498261, Train_MMSE: 0.008293, NMMSE: 0.011818, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:11:06] Epoch 134/200, Loss: 11.495945, Train_MMSE: 0.008282, NMMSE: 0.011824, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:11:44] Epoch 135/200, Loss: 11.661505, Train_MMSE: 0.008273, NMMSE: 0.011837, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:12:21] Epoch 136/200, Loss: 11.580779, Train_MMSE: 0.008276, NMMSE: 0.011836, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:12:59] Epoch 137/200, Loss: 11.662782, Train_MMSE: 0.008277, NMMSE: 0.011844, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:13:36] Epoch 138/200, Loss: 11.497464, Train_MMSE: 0.008278, NMMSE: 0.011834, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:14:13] Epoch 139/200, Loss: 11.525868, Train_MMSE: 0.008299, NMMSE: 0.011846, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:14:50] Epoch 140/200, Loss: 11.645558, Train_MMSE: 0.008262, NMMSE: 0.011838, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:15:28] Epoch 141/200, Loss: 11.542867, Train_MMSE: 0.008287, NMMSE: 0.011849, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:16:06] Epoch 142/200, Loss: 11.679310, Train_MMSE: 0.008264, NMMSE: 0.011845, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:16:44] Epoch 143/200, Loss: 11.412805, Train_MMSE: 0.008263, NMMSE: 0.011855, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:17:22] Epoch 144/200, Loss: 11.423096, Train_MMSE: 0.008275, NMMSE: 0.011863, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:17:59] Epoch 145/200, Loss: 11.495380, Train_MMSE: 0.008257, NMMSE: 0.011862, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:18:37] Epoch 146/200, Loss: 11.581853, Train_MMSE: 0.00827, NMMSE: 0.011879, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:19:15] Epoch 147/200, Loss: 11.516336, Train_MMSE: 0.008266, NMMSE: 0.011881, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:19:53] Epoch 148/200, Loss: 11.578956, Train_MMSE: 0.008257, NMMSE: 0.011885, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:20:30] Epoch 149/200, Loss: 11.471075, Train_MMSE: 0.008265, NMMSE: 0.011882, LS_NMSE: 0.011111, Lr: 0.0001
[2025-02-23 00:21:08] Epoch 150/200, Loss: 11.542899, Train_MMSE: 0.008253, NMMSE: 0.011885, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:21:46] Epoch 151/200, Loss: 11.470311, Train_MMSE: 0.008218, NMMSE: 0.011887, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:22:24] Epoch 152/200, Loss: 11.457792, Train_MMSE: 0.00823, NMMSE: 0.011894, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:23:02] Epoch 153/200, Loss: 11.795922, Train_MMSE: 0.008256, NMMSE: 0.011911, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:23:40] Epoch 154/200, Loss: 11.540688, Train_MMSE: 0.008214, NMMSE: 0.011905, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:24:17] Epoch 155/200, Loss: 11.529855, Train_MMSE: 0.008226, NMMSE: 0.011907, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:24:56] Epoch 156/200, Loss: 11.429831, Train_MMSE: 0.008213, NMMSE: 0.011901, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:25:34] Epoch 157/200, Loss: 11.441243, Train_MMSE: 0.008231, NMMSE: 0.011903, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:26:11] Epoch 158/200, Loss: 11.474484, Train_MMSE: 0.008217, NMMSE: 0.011902, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:26:49] Epoch 159/200, Loss: 11.451275, Train_MMSE: 0.008256, NMMSE: 0.01192, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:27:26] Epoch 160/200, Loss: 11.545696, Train_MMSE: 0.008218, NMMSE: 0.011908, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:28:03] Epoch 161/200, Loss: 11.505556, Train_MMSE: 0.008213, NMMSE: 0.011916, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:28:40] Epoch 162/200, Loss: 11.440175, Train_MMSE: 0.008224, NMMSE: 0.011919, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:29:16] Epoch 163/200, Loss: 11.518231, Train_MMSE: 0.008222, NMMSE: 0.01191, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:29:53] Epoch 164/200, Loss: 11.447496, Train_MMSE: 0.008241, NMMSE: 0.011907, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:30:31] Epoch 165/200, Loss: 11.610724, Train_MMSE: 0.008226, NMMSE: 0.011911, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:31:08] Epoch 166/200, Loss: 11.577245, Train_MMSE: 0.008214, NMMSE: 0.011919, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:31:45] Epoch 167/200, Loss: 11.475513, Train_MMSE: 0.008213, NMMSE: 0.011916, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:32:22] Epoch 168/200, Loss: 11.610770, Train_MMSE: 0.008236, NMMSE: 0.011929, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:32:59] Epoch 169/200, Loss: 11.471502, Train_MMSE: 0.00822, NMMSE: 0.011918, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:33:37] Epoch 170/200, Loss: 11.444476, Train_MMSE: 0.008215, NMMSE: 0.011918, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:34:14] Epoch 171/200, Loss: 11.376199, Train_MMSE: 0.008218, NMMSE: 0.011922, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:34:52] Epoch 172/200, Loss: 11.496946, Train_MMSE: 0.008228, NMMSE: 0.011918, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:35:29] Epoch 173/200, Loss: 11.402494, Train_MMSE: 0.008222, NMMSE: 0.011918, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:36:06] Epoch 174/200, Loss: 11.561523, Train_MMSE: 0.008229, NMMSE: 0.011919, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:36:43] Epoch 175/200, Loss: 11.624326, Train_MMSE: 0.008211, NMMSE: 0.011921, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:37:20] Epoch 176/200, Loss: 11.551830, Train_MMSE: 0.008228, NMMSE: 0.011921, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:37:58] Epoch 177/200, Loss: 11.661553, Train_MMSE: 0.008225, NMMSE: 0.011933, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:38:35] Epoch 178/200, Loss: 11.436147, Train_MMSE: 0.008226, NMMSE: 0.011924, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:39:13] Epoch 179/200, Loss: 11.444537, Train_MMSE: 0.008226, NMMSE: 0.011924, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:39:51] Epoch 180/200, Loss: 11.710488, Train_MMSE: 0.008225, NMMSE: 0.011934, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:40:28] Epoch 181/200, Loss: 11.468939, Train_MMSE: 0.008225, NMMSE: 0.011923, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:41:06] Epoch 182/200, Loss: 11.529186, Train_MMSE: 0.008236, NMMSE: 0.011919, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:41:43] Epoch 183/200, Loss: 11.492434, Train_MMSE: 0.008245, NMMSE: 0.011926, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:42:21] Epoch 184/200, Loss: 11.420615, Train_MMSE: 0.008225, NMMSE: 0.01192, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:42:58] Epoch 185/200, Loss: 11.449121, Train_MMSE: 0.008221, NMMSE: 0.01192, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:43:36] Epoch 186/200, Loss: 11.398758, Train_MMSE: 0.00822, NMMSE: 0.011919, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:44:12] Epoch 187/200, Loss: 11.572625, Train_MMSE: 0.008231, NMMSE: 0.011931, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:44:43] Epoch 188/200, Loss: 11.501384, Train_MMSE: 0.00822, NMMSE: 0.011928, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:45:14] Epoch 189/200, Loss: 11.503346, Train_MMSE: 0.008214, NMMSE: 0.011929, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:45:46] Epoch 190/200, Loss: 11.429406, Train_MMSE: 0.008203, NMMSE: 0.011928, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:46:16] Epoch 191/200, Loss: 11.446757, Train_MMSE: 0.008226, NMMSE: 0.011937, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:46:47] Epoch 192/200, Loss: 11.594083, Train_MMSE: 0.008225, NMMSE: 0.011929, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:47:13] Epoch 193/200, Loss: 11.547378, Train_MMSE: 0.008192, NMMSE: 0.011926, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:47:32] Epoch 194/200, Loss: 11.557706, Train_MMSE: 0.008201, NMMSE: 0.011933, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:47:46] Epoch 195/200, Loss: 11.431623, Train_MMSE: 0.008219, NMMSE: 0.011936, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:48:01] Epoch 196/200, Loss: 11.586555, Train_MMSE: 0.008239, NMMSE: 0.011936, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:48:15] Epoch 197/200, Loss: 11.374853, Train_MMSE: 0.008234, NMMSE: 0.011931, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:48:30] Epoch 198/200, Loss: 11.503896, Train_MMSE: 0.008205, NMMSE: 0.011929, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:48:45] Epoch 199/200, Loss: 11.418294, Train_MMSE: 0.008204, NMMSE: 0.011934, LS_NMSE: 0.011111, Lr: 1e-05
[2025-02-23 00:48:59] Epoch 200/200, Loss: 11.426634, Train_MMSE: 0.008215, NMMSE: 0.01193, LS_NMSE: 0.011111, Lr: 1.0000000000000002e-06
