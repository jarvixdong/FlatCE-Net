Train.py PID: 19847

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.055706420795849976
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L2_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L2_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B5D18C32_test_Dataset_dB-10_N36_K4_L2_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 5,
                      'depth': 18,
                      'filters': 32,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-4): 5 x Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 2.86 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f9a17505760>
loss function:: L1Loss()
[2025-02-22 21:27:36] Epoch 1/200, Loss: 61.015301, Train_MMSE: 0.262804, NMMSE: 0.196941, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:28:32] Epoch 2/200, Loss: 52.679844, Train_MMSE: 0.193989, NMMSE: 0.155238, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:29:27] Epoch 3/200, Loss: 49.657333, Train_MMSE: 0.163328, NMMSE: 0.13847, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:30:23] Epoch 4/200, Loss: 46.904816, Train_MMSE: 0.145678, NMMSE: 0.126008, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:31:18] Epoch 5/200, Loss: 45.285530, Train_MMSE: 0.133086, NMMSE: 0.116569, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:32:14] Epoch 6/200, Loss: 43.084049, Train_MMSE: 0.122757, NMMSE: 0.109007, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:33:11] Epoch 7/200, Loss: 42.482082, Train_MMSE: 0.114288, NMMSE: 0.101859, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:34:06] Epoch 8/200, Loss: 40.968094, Train_MMSE: 0.109181, NMMSE: 0.099692, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:35:02] Epoch 9/200, Loss: 41.091743, Train_MMSE: 0.106105, NMMSE: 0.097208, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:35:58] Epoch 10/200, Loss: 40.289734, Train_MMSE: 0.103988, NMMSE: 0.096203, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:36:54] Epoch 11/200, Loss: 40.434330, Train_MMSE: 0.102428, NMMSE: 0.094902, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:37:49] Epoch 12/200, Loss: 39.813564, Train_MMSE: 0.101211, NMMSE: 0.093685, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:38:45] Epoch 13/200, Loss: 39.898033, Train_MMSE: 0.100107, NMMSE: 0.093493, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:39:40] Epoch 14/200, Loss: 39.496861, Train_MMSE: 0.099058, NMMSE: 0.091973, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:40:36] Epoch 15/200, Loss: 39.570160, Train_MMSE: 0.098459, NMMSE: 0.091624, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:41:32] Epoch 16/200, Loss: 38.995960, Train_MMSE: 0.09778, NMMSE: 0.090598, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:42:28] Epoch 17/200, Loss: 39.445576, Train_MMSE: 0.097233, NMMSE: 0.09001, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:43:24] Epoch 18/200, Loss: 38.949844, Train_MMSE: 0.096556, NMMSE: 0.090274, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:44:20] Epoch 19/200, Loss: 38.927650, Train_MMSE: 0.096107, NMMSE: 0.089575, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:45:16] Epoch 20/200, Loss: 39.004704, Train_MMSE: 0.095634, NMMSE: 0.089328, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:46:12] Epoch 21/200, Loss: 38.978203, Train_MMSE: 0.095264, NMMSE: 0.088712, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:47:08] Epoch 22/200, Loss: 38.920185, Train_MMSE: 0.094811, NMMSE: 0.088625, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:48:01] Epoch 23/200, Loss: 38.403812, Train_MMSE: 0.094527, NMMSE: 0.089081, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:48:53] Epoch 24/200, Loss: 38.477192, Train_MMSE: 0.094228, NMMSE: 0.088155, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:49:45] Epoch 25/200, Loss: 38.524399, Train_MMSE: 0.093832, NMMSE: 0.088065, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:50:38] Epoch 26/200, Loss: 38.730667, Train_MMSE: 0.093649, NMMSE: 0.087815, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:51:32] Epoch 27/200, Loss: 38.281277, Train_MMSE: 0.093362, NMMSE: 0.087198, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:52:49] Epoch 28/200, Loss: 38.366684, Train_MMSE: 0.093219, NMMSE: 0.087887, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:54:06] Epoch 29/200, Loss: 38.328392, Train_MMSE: 0.092876, NMMSE: 0.087629, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:55:44] Epoch 30/200, Loss: 38.292431, Train_MMSE: 0.092604, NMMSE: 0.087775, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:57:27] Epoch 31/200, Loss: 38.305580, Train_MMSE: 0.092351, NMMSE: 0.087448, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:59:09] Epoch 32/200, Loss: 38.064117, Train_MMSE: 0.092116, NMMSE: 0.087309, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:00:51] Epoch 33/200, Loss: 37.948746, Train_MMSE: 0.092004, NMMSE: 0.086517, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:02:33] Epoch 34/200, Loss: 38.096413, Train_MMSE: 0.091756, NMMSE: 0.086417, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:04:17] Epoch 35/200, Loss: 37.809746, Train_MMSE: 0.091403, NMMSE: 0.087016, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:05:59] Epoch 36/200, Loss: 37.828201, Train_MMSE: 0.091347, NMMSE: 0.08586, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:07:44] Epoch 37/200, Loss: 37.988968, Train_MMSE: 0.091124, NMMSE: 0.087055, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:09:27] Epoch 38/200, Loss: 37.630291, Train_MMSE: 0.090963, NMMSE: 0.086972, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:11:12] Epoch 39/200, Loss: 37.777752, Train_MMSE: 0.090791, NMMSE: 0.085744, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:12:57] Epoch 40/200, Loss: 38.157154, Train_MMSE: 0.090666, NMMSE: 0.086719, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:14:40] Epoch 41/200, Loss: 37.952991, Train_MMSE: 0.090434, NMMSE: 0.085694, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:16:23] Epoch 42/200, Loss: 37.873436, Train_MMSE: 0.090294, NMMSE: 0.085771, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:18:07] Epoch 43/200, Loss: 38.210815, Train_MMSE: 0.090075, NMMSE: 0.085653, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:19:49] Epoch 44/200, Loss: 37.460377, Train_MMSE: 0.089968, NMMSE: 0.085277, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:21:31] Epoch 45/200, Loss: 37.672459, Train_MMSE: 0.089785, NMMSE: 0.085881, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:23:13] Epoch 46/200, Loss: 37.716446, Train_MMSE: 0.089659, NMMSE: 0.085315, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:24:57] Epoch 47/200, Loss: 37.475864, Train_MMSE: 0.089557, NMMSE: 0.085212, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:26:40] Epoch 48/200, Loss: 37.640854, Train_MMSE: 0.089445, NMMSE: 0.085169, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:28:24] Epoch 49/200, Loss: 37.450981, Train_MMSE: 0.089295, NMMSE: 0.085104, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 22:30:07] Epoch 50/200, Loss: 37.611465, Train_MMSE: 0.0892, NMMSE: 0.085215, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:31:50] Epoch 51/200, Loss: 36.167458, Train_MMSE: 0.08556, NMMSE: 0.081526, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:33:33] Epoch 52/200, Loss: 36.564384, Train_MMSE: 0.08499, NMMSE: 0.081451, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:35:17] Epoch 53/200, Loss: 36.028427, Train_MMSE: 0.084807, NMMSE: 0.081443, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:36:59] Epoch 54/200, Loss: 36.579544, Train_MMSE: 0.084689, NMMSE: 0.081412, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:38:42] Epoch 55/200, Loss: 36.341145, Train_MMSE: 0.08459, NMMSE: 0.081364, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:40:25] Epoch 56/200, Loss: 36.245686, Train_MMSE: 0.084511, NMMSE: 0.081404, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:42:08] Epoch 57/200, Loss: 36.295940, Train_MMSE: 0.084448, NMMSE: 0.081406, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:43:50] Epoch 58/200, Loss: 36.435917, Train_MMSE: 0.084376, NMMSE: 0.081465, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:45:33] Epoch 59/200, Loss: 36.512302, Train_MMSE: 0.084315, NMMSE: 0.081421, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:47:16] Epoch 60/200, Loss: 36.600544, Train_MMSE: 0.084264, NMMSE: 0.08146, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:48:59] Epoch 61/200, Loss: 36.869488, Train_MMSE: 0.08419, NMMSE: 0.081497, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:50:43] Epoch 62/200, Loss: 36.390171, Train_MMSE: 0.084156, NMMSE: 0.081485, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:52:27] Epoch 63/200, Loss: 35.968914, Train_MMSE: 0.084118, NMMSE: 0.081584, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:54:10] Epoch 64/200, Loss: 36.016872, Train_MMSE: 0.084073, NMMSE: 0.08144, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:55:53] Epoch 65/200, Loss: 35.927303, Train_MMSE: 0.08403, NMMSE: 0.081462, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:57:35] Epoch 66/200, Loss: 36.561958, Train_MMSE: 0.084003, NMMSE: 0.081416, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 22:59:15] Epoch 67/200, Loss: 36.178551, Train_MMSE: 0.083937, NMMSE: 0.081362, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:00:58] Epoch 68/200, Loss: 36.141041, Train_MMSE: 0.08389, NMMSE: 0.081548, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:02:41] Epoch 69/200, Loss: 35.871201, Train_MMSE: 0.083872, NMMSE: 0.08139, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:04:24] Epoch 70/200, Loss: 36.495213, Train_MMSE: 0.083816, NMMSE: 0.081459, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:06:09] Epoch 71/200, Loss: 35.935799, Train_MMSE: 0.083784, NMMSE: 0.081535, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:07:51] Epoch 72/200, Loss: 36.172436, Train_MMSE: 0.083756, NMMSE: 0.081548, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:09:34] Epoch 73/200, Loss: 36.089794, Train_MMSE: 0.083718, NMMSE: 0.081497, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:11:17] Epoch 74/200, Loss: 36.336922, Train_MMSE: 0.08368, NMMSE: 0.081607, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:13:00] Epoch 75/200, Loss: 36.390110, Train_MMSE: 0.083618, NMMSE: 0.081459, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:14:42] Epoch 76/200, Loss: 36.371681, Train_MMSE: 0.083608, NMMSE: 0.08153, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:16:23] Epoch 77/200, Loss: 36.209362, Train_MMSE: 0.083561, NMMSE: 0.081558, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:18:05] Epoch 78/200, Loss: 35.830067, Train_MMSE: 0.08354, NMMSE: 0.081586, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:19:48] Epoch 79/200, Loss: 36.248882, Train_MMSE: 0.083511, NMMSE: 0.081527, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:21:31] Epoch 80/200, Loss: 36.298676, Train_MMSE: 0.083495, NMMSE: 0.081481, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:23:15] Epoch 81/200, Loss: 36.307266, Train_MMSE: 0.083442, NMMSE: 0.081647, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:24:58] Epoch 82/200, Loss: 36.066338, Train_MMSE: 0.083425, NMMSE: 0.081578, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:26:41] Epoch 83/200, Loss: 35.864101, Train_MMSE: 0.083374, NMMSE: 0.081615, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:28:23] Epoch 84/200, Loss: 36.180885, Train_MMSE: 0.083353, NMMSE: 0.081558, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:30:05] Epoch 85/200, Loss: 36.105183, Train_MMSE: 0.083312, NMMSE: 0.0815, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:31:49] Epoch 86/200, Loss: 36.050163, Train_MMSE: 0.083297, NMMSE: 0.081718, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:33:32] Epoch 87/200, Loss: 36.106647, Train_MMSE: 0.083271, NMMSE: 0.081672, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:35:15] Epoch 88/200, Loss: 36.198509, Train_MMSE: 0.083256, NMMSE: 0.08161, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:36:58] Epoch 89/200, Loss: 35.963715, Train_MMSE: 0.083214, NMMSE: 0.081634, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:38:42] Epoch 90/200, Loss: 35.933750, Train_MMSE: 0.083175, NMMSE: 0.081547, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:40:19] Epoch 91/200, Loss: 36.124084, Train_MMSE: 0.083145, NMMSE: 0.081699, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:41:54] Epoch 92/200, Loss: 35.987785, Train_MMSE: 0.08314, NMMSE: 0.08167, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:43:29] Epoch 93/200, Loss: 36.022346, Train_MMSE: 0.083103, NMMSE: 0.081611, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:44:27] Epoch 94/200, Loss: 35.864147, Train_MMSE: 0.083082, NMMSE: 0.081709, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:45:19] Epoch 95/200, Loss: 35.813774, Train_MMSE: 0.083042, NMMSE: 0.081817, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:46:11] Epoch 96/200, Loss: 35.805992, Train_MMSE: 0.083009, NMMSE: 0.081692, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:47:04] Epoch 97/200, Loss: 36.231380, Train_MMSE: 0.082999, NMMSE: 0.081624, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:47:56] Epoch 98/200, Loss: 36.339344, Train_MMSE: 0.082951, NMMSE: 0.081673, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:48:49] Epoch 99/200, Loss: 36.307411, Train_MMSE: 0.082933, NMMSE: 0.081647, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 23:49:41] Epoch 100/200, Loss: 35.846966, Train_MMSE: 0.082922, NMMSE: 0.081742, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:50:33] Epoch 101/200, Loss: 35.405823, Train_MMSE: 0.082199, NMMSE: 0.08131, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:51:26] Epoch 102/200, Loss: 35.686985, Train_MMSE: 0.082092, NMMSE: 0.0813, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:52:18] Epoch 103/200, Loss: 35.510788, Train_MMSE: 0.082068, NMMSE: 0.081322, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:53:10] Epoch 104/200, Loss: 35.793171, Train_MMSE: 0.082058, NMMSE: 0.081322, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:54:02] Epoch 105/200, Loss: 35.739910, Train_MMSE: 0.082053, NMMSE: 0.081332, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:54:54] Epoch 106/200, Loss: 35.267658, Train_MMSE: 0.082044, NMMSE: 0.081338, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:55:47] Epoch 107/200, Loss: 35.742970, Train_MMSE: 0.08204, NMMSE: 0.081338, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:56:39] Epoch 108/200, Loss: 35.331898, Train_MMSE: 0.082037, NMMSE: 0.081345, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:57:31] Epoch 109/200, Loss: 36.184429, Train_MMSE: 0.082025, NMMSE: 0.081365, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:58:24] Epoch 110/200, Loss: 35.645248, Train_MMSE: 0.08202, NMMSE: 0.081373, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 23:59:16] Epoch 111/200, Loss: 35.951828, Train_MMSE: 0.082016, NMMSE: 0.081353, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:00:08] Epoch 112/200, Loss: 35.828709, Train_MMSE: 0.082007, NMMSE: 0.081373, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:01:01] Epoch 113/200, Loss: 36.117458, Train_MMSE: 0.082002, NMMSE: 0.081367, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:01:53] Epoch 114/200, Loss: 35.431396, Train_MMSE: 0.082003, NMMSE: 0.081389, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:02:45] Epoch 115/200, Loss: 36.033222, Train_MMSE: 0.081992, NMMSE: 0.081373, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:03:37] Epoch 116/200, Loss: 35.738823, Train_MMSE: 0.081992, NMMSE: 0.0814, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:04:30] Epoch 117/200, Loss: 35.457569, Train_MMSE: 0.081982, NMMSE: 0.081396, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:05:22] Epoch 118/200, Loss: 36.262524, Train_MMSE: 0.081992, NMMSE: 0.081383, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:06:15] Epoch 119/200, Loss: 35.954304, Train_MMSE: 0.081969, NMMSE: 0.081383, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:07:07] Epoch 120/200, Loss: 35.713654, Train_MMSE: 0.081968, NMMSE: 0.081369, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:08:00] Epoch 121/200, Loss: 35.189449, Train_MMSE: 0.081959, NMMSE: 0.081398, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:08:52] Epoch 122/200, Loss: 35.774509, Train_MMSE: 0.081956, NMMSE: 0.081423, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:09:44] Epoch 123/200, Loss: 35.757378, Train_MMSE: 0.081957, NMMSE: 0.081395, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:10:37] Epoch 124/200, Loss: 35.807541, Train_MMSE: 0.081951, NMMSE: 0.081412, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:11:29] Epoch 125/200, Loss: 35.649525, Train_MMSE: 0.081939, NMMSE: 0.081428, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:12:21] Epoch 126/200, Loss: 35.747375, Train_MMSE: 0.081937, NMMSE: 0.081419, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:13:13] Epoch 127/200, Loss: 35.665371, Train_MMSE: 0.081946, NMMSE: 0.081408, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:14:06] Epoch 128/200, Loss: 35.914234, Train_MMSE: 0.08194, NMMSE: 0.081415, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:14:58] Epoch 129/200, Loss: 35.778019, Train_MMSE: 0.081934, NMMSE: 0.081409, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:15:50] Epoch 130/200, Loss: 35.646706, Train_MMSE: 0.081934, NMMSE: 0.081409, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:16:43] Epoch 131/200, Loss: 35.639118, Train_MMSE: 0.081917, NMMSE: 0.081413, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:17:35] Epoch 132/200, Loss: 35.324421, Train_MMSE: 0.081916, NMMSE: 0.081411, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:18:27] Epoch 133/200, Loss: 35.801384, Train_MMSE: 0.081918, NMMSE: 0.081414, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:19:20] Epoch 134/200, Loss: 35.708313, Train_MMSE: 0.08191, NMMSE: 0.081439, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:20:12] Epoch 135/200, Loss: 35.834248, Train_MMSE: 0.081904, NMMSE: 0.081437, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:21:04] Epoch 136/200, Loss: 35.552143, Train_MMSE: 0.081906, NMMSE: 0.081452, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:21:56] Epoch 137/200, Loss: 35.597103, Train_MMSE: 0.081894, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:22:49] Epoch 138/200, Loss: 35.929920, Train_MMSE: 0.081891, NMMSE: 0.081417, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:23:41] Epoch 139/200, Loss: 35.637699, Train_MMSE: 0.081888, NMMSE: 0.081432, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:24:33] Epoch 140/200, Loss: 35.503578, Train_MMSE: 0.081877, NMMSE: 0.081447, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:25:26] Epoch 141/200, Loss: 35.688301, Train_MMSE: 0.081879, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:26:18] Epoch 142/200, Loss: 35.668308, Train_MMSE: 0.081877, NMMSE: 0.081458, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:27:10] Epoch 143/200, Loss: 35.739975, Train_MMSE: 0.081875, NMMSE: 0.081439, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:28:02] Epoch 144/200, Loss: 35.603001, Train_MMSE: 0.081863, NMMSE: 0.081446, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:28:55] Epoch 145/200, Loss: 35.796375, Train_MMSE: 0.081861, NMMSE: 0.081463, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:29:47] Epoch 146/200, Loss: 35.305637, Train_MMSE: 0.081857, NMMSE: 0.081461, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:30:39] Epoch 147/200, Loss: 36.118248, Train_MMSE: 0.081858, NMMSE: 0.08145, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:31:32] Epoch 148/200, Loss: 35.724331, Train_MMSE: 0.081856, NMMSE: 0.081462, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:32:24] Epoch 149/200, Loss: 35.909290, Train_MMSE: 0.081847, NMMSE: 0.081433, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-23 00:33:16] Epoch 150/200, Loss: 35.723160, Train_MMSE: 0.081841, NMMSE: 0.081449, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:34:09] Epoch 151/200, Loss: 35.827061, Train_MMSE: 0.081752, NMMSE: 0.081434, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:35:01] Epoch 152/200, Loss: 35.824841, Train_MMSE: 0.081739, NMMSE: 0.081424, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:35:53] Epoch 153/200, Loss: 35.667629, Train_MMSE: 0.081741, NMMSE: 0.081444, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:36:46] Epoch 154/200, Loss: 35.438049, Train_MMSE: 0.081739, NMMSE: 0.081427, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:37:38] Epoch 155/200, Loss: 35.930180, Train_MMSE: 0.081748, NMMSE: 0.081423, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:38:31] Epoch 156/200, Loss: 35.694965, Train_MMSE: 0.081739, NMMSE: 0.081435, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:39:23] Epoch 157/200, Loss: 35.488976, Train_MMSE: 0.081746, NMMSE: 0.081452, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:40:15] Epoch 158/200, Loss: 35.729465, Train_MMSE: 0.08174, NMMSE: 0.081431, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:41:08] Epoch 159/200, Loss: 35.594925, Train_MMSE: 0.081733, NMMSE: 0.081442, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:42:00] Epoch 160/200, Loss: 35.493706, Train_MMSE: 0.081733, NMMSE: 0.081439, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:42:52] Epoch 161/200, Loss: 35.585217, Train_MMSE: 0.081736, NMMSE: 0.081438, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:43:45] Epoch 162/200, Loss: 35.722038, Train_MMSE: 0.081738, NMMSE: 0.081441, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:44:37] Epoch 163/200, Loss: 35.645752, Train_MMSE: 0.08174, NMMSE: 0.081428, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:45:29] Epoch 164/200, Loss: 35.500721, Train_MMSE: 0.081738, NMMSE: 0.081441, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:46:22] Epoch 165/200, Loss: 35.621700, Train_MMSE: 0.081734, NMMSE: 0.081444, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:47:14] Epoch 166/200, Loss: 35.547596, Train_MMSE: 0.081741, NMMSE: 0.081454, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:48:06] Epoch 167/200, Loss: 35.396881, Train_MMSE: 0.081734, NMMSE: 0.081453, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:48:59] Epoch 168/200, Loss: 35.534866, Train_MMSE: 0.08174, NMMSE: 0.081451, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:49:51] Epoch 169/200, Loss: 35.542603, Train_MMSE: 0.081738, NMMSE: 0.081449, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:50:43] Epoch 170/200, Loss: 35.586555, Train_MMSE: 0.081729, NMMSE: 0.081428, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:51:35] Epoch 171/200, Loss: 35.221470, Train_MMSE: 0.081741, NMMSE: 0.081443, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:52:27] Epoch 172/200, Loss: 35.463402, Train_MMSE: 0.081735, NMMSE: 0.081451, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:53:20] Epoch 173/200, Loss: 35.311462, Train_MMSE: 0.081734, NMMSE: 0.08147, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:54:12] Epoch 174/200, Loss: 35.290787, Train_MMSE: 0.081737, NMMSE: 0.081442, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:55:04] Epoch 175/200, Loss: 35.463448, Train_MMSE: 0.081732, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:55:57] Epoch 176/200, Loss: 35.671154, Train_MMSE: 0.081733, NMMSE: 0.081428, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:56:49] Epoch 177/200, Loss: 35.687206, Train_MMSE: 0.081727, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:57:41] Epoch 178/200, Loss: 35.499294, Train_MMSE: 0.08173, NMMSE: 0.081447, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:58:33] Epoch 179/200, Loss: 35.594818, Train_MMSE: 0.081737, NMMSE: 0.08145, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:59:26] Epoch 180/200, Loss: 35.681263, Train_MMSE: 0.081726, NMMSE: 0.081447, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:00:18] Epoch 181/200, Loss: 35.574497, Train_MMSE: 0.081732, NMMSE: 0.081444, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:01:10] Epoch 182/200, Loss: 35.394955, Train_MMSE: 0.08174, NMMSE: 0.081444, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:02:03] Epoch 183/200, Loss: 35.340843, Train_MMSE: 0.081732, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:02:55] Epoch 184/200, Loss: 35.734837, Train_MMSE: 0.081726, NMMSE: 0.081447, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:03:47] Epoch 185/200, Loss: 35.071541, Train_MMSE: 0.081728, NMMSE: 0.081481, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:04:39] Epoch 186/200, Loss: 35.670128, Train_MMSE: 0.081736, NMMSE: 0.081448, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:05:32] Epoch 187/200, Loss: 35.513023, Train_MMSE: 0.08173, NMMSE: 0.081445, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:06:24] Epoch 188/200, Loss: 35.936764, Train_MMSE: 0.081737, NMMSE: 0.081454, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:07:17] Epoch 189/200, Loss: 35.459053, Train_MMSE: 0.081726, NMMSE: 0.081468, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:08:09] Epoch 190/200, Loss: 35.666954, Train_MMSE: 0.081729, NMMSE: 0.08144, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:09:01] Epoch 191/200, Loss: 35.638924, Train_MMSE: 0.081727, NMMSE: 0.081463, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:09:54] Epoch 192/200, Loss: 35.480385, Train_MMSE: 0.081716, NMMSE: 0.081435, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:10:46] Epoch 193/200, Loss: 35.449753, Train_MMSE: 0.081727, NMMSE: 0.081458, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:11:38] Epoch 194/200, Loss: 35.708393, Train_MMSE: 0.08172, NMMSE: 0.081456, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:12:30] Epoch 195/200, Loss: 35.851826, Train_MMSE: 0.081725, NMMSE: 0.08145, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:13:23] Epoch 196/200, Loss: 35.674168, Train_MMSE: 0.081729, NMMSE: 0.081436, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:14:15] Epoch 197/200, Loss: 35.386646, Train_MMSE: 0.081726, NMMSE: 0.081449, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:15:08] Epoch 198/200, Loss: 35.840801, Train_MMSE: 0.081724, NMMSE: 0.08145, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:16:00] Epoch 199/200, Loss: 35.447903, Train_MMSE: 0.081716, NMMSE: 0.081452, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 01:16:52] Epoch 200/200, Loss: 35.461075, Train_MMSE: 0.081723, NMMSE: 0.081468, LS_NMSE: 0.242602, Lr: 1.0000000000000002e-06
