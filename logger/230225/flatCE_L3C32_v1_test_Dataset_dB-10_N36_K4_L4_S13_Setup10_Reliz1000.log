Train.py PID: 11407

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.0033634739987592
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_train_Dataset_dB-10_N36_K4_L4_S13_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_test_Dataset_dB-10_N36_K4_L4_S13_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/flatCE_L3C32_v1_test_Dataset_dB-10_N36_K4_L4_S13_Setup10_Reliz1000.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f8bdcf38ef0>
loss function:: L1Loss()
[2025-02-22 22:48:21] Epoch 1/200, Loss: 22.343958, Train_MMSE: 0.752219, NMMSE: 2.235989, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:49:19] Epoch 2/200, Loss: 10.507785, Train_MMSE: 0.013593, NMMSE: 0.011724, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:50:18] Epoch 3/200, Loss: 10.129796, Train_MMSE: 0.007762, NMMSE: 0.008123, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:51:18] Epoch 4/200, Loss: 9.392092, Train_MMSE: 0.006009, NMMSE: 0.004807, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:52:18] Epoch 5/200, Loss: 9.283853, Train_MMSE: 0.005178, NMMSE: 0.004677, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:53:17] Epoch 6/200, Loss: 8.994198, Train_MMSE: 0.004942, NMMSE: 0.004545, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:54:17] Epoch 7/200, Loss: 9.105981, Train_MMSE: 0.004827, NMMSE: 0.004021, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:55:17] Epoch 8/200, Loss: 9.672060, Train_MMSE: 0.004847, NMMSE: 0.004326, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:56:17] Epoch 9/200, Loss: 8.965314, Train_MMSE: 0.004801, NMMSE: 0.003711, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:57:17] Epoch 10/200, Loss: 9.135454, Train_MMSE: 0.004846, NMMSE: 0.004514, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:58:17] Epoch 11/200, Loss: 8.948464, Train_MMSE: 0.004736, NMMSE: 0.003833, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 22:59:17] Epoch 12/200, Loss: 8.796960, Train_MMSE: 0.004704, NMMSE: 0.003737, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:00:17] Epoch 13/200, Loss: 8.886224, Train_MMSE: 0.005025, NMMSE: 0.004039, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:01:18] Epoch 14/200, Loss: 9.104424, Train_MMSE: 0.004773, NMMSE: 0.004085, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:02:18] Epoch 15/200, Loss: 9.031812, Train_MMSE: 0.004742, NMMSE: 0.004161, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:03:18] Epoch 16/200, Loss: 8.850986, Train_MMSE: 0.004818, NMMSE: 0.004012, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:04:18] Epoch 17/200, Loss: 8.806571, Train_MMSE: 0.004761, NMMSE: 0.004363, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:05:18] Epoch 18/200, Loss: 9.100615, Train_MMSE: 0.004776, NMMSE: 0.004025, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:06:17] Epoch 19/200, Loss: 9.266142, Train_MMSE: 0.00468, NMMSE: 0.003833, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:07:17] Epoch 20/200, Loss: 8.844462, Train_MMSE: 0.004716, NMMSE: 0.00425, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:08:17] Epoch 21/200, Loss: 9.006988, Train_MMSE: 0.004784, NMMSE: 0.003954, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:09:11] Epoch 22/200, Loss: 8.882107, Train_MMSE: 0.004701, NMMSE: 0.003955, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:10:05] Epoch 23/200, Loss: 8.941651, Train_MMSE: 0.00477, NMMSE: 0.003764, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:11:00] Epoch 24/200, Loss: 9.125216, Train_MMSE: 0.004718, NMMSE: 0.003637, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:11:54] Epoch 25/200, Loss: 10.441313, Train_MMSE: 0.004862, NMMSE: 0.004594, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:12:48] Epoch 26/200, Loss: 9.632390, Train_MMSE: 0.004828, NMMSE: 0.004265, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:13:41] Epoch 27/200, Loss: 8.839283, Train_MMSE: 0.004752, NMMSE: 0.004008, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:14:35] Epoch 28/200, Loss: 8.850627, Train_MMSE: 0.004597, NMMSE: 0.003656, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:15:29] Epoch 29/200, Loss: 8.889515, Train_MMSE: 0.004688, NMMSE: 0.004839, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:16:22] Epoch 30/200, Loss: 8.889853, Train_MMSE: 0.004758, NMMSE: 0.004231, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:17:15] Epoch 31/200, Loss: 8.938048, Train_MMSE: 0.004745, NMMSE: 0.003696, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:18:08] Epoch 32/200, Loss: 8.852589, Train_MMSE: 0.004761, NMMSE: 0.003828, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:18:57] Epoch 33/200, Loss: 9.655840, Train_MMSE: 0.004739, NMMSE: 0.004035, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:19:45] Epoch 34/200, Loss: 9.030867, Train_MMSE: 0.004833, NMMSE: 0.004095, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:20:33] Epoch 35/200, Loss: 9.295592, Train_MMSE: 0.004733, NMMSE: 0.00368, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:21:20] Epoch 36/200, Loss: 8.919381, Train_MMSE: 0.004746, NMMSE: 0.003776, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:22:08] Epoch 37/200, Loss: 8.971303, Train_MMSE: 0.004698, NMMSE: 0.005519, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:22:56] Epoch 38/200, Loss: 8.799571, Train_MMSE: 0.004938, NMMSE: 0.003698, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:23:44] Epoch 39/200, Loss: 8.983772, Train_MMSE: 0.004697, NMMSE: 0.003768, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:24:31] Epoch 40/200, Loss: 8.773451, Train_MMSE: 0.004857, NMMSE: 0.004446, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:25:20] Epoch 41/200, Loss: 9.076094, Train_MMSE: 0.004875, NMMSE: 0.003723, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:26:07] Epoch 42/200, Loss: 8.825334, Train_MMSE: 0.004681, NMMSE: 0.004368, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:26:55] Epoch 43/200, Loss: 8.887435, Train_MMSE: 0.004862, NMMSE: 0.004051, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:27:43] Epoch 44/200, Loss: 8.786419, Train_MMSE: 0.004661, NMMSE: 0.004122, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:28:31] Epoch 45/200, Loss: 8.805555, Train_MMSE: 0.004776, NMMSE: 0.003853, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:29:19] Epoch 46/200, Loss: 8.943047, Train_MMSE: 0.004693, NMMSE: 0.004245, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:30:07] Epoch 47/200, Loss: 9.094646, Train_MMSE: 0.00476, NMMSE: 0.004156, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:30:54] Epoch 48/200, Loss: 8.810643, Train_MMSE: 0.004742, NMMSE: 0.003828, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:31:42] Epoch 49/200, Loss: 8.889652, Train_MMSE: 0.004684, NMMSE: 0.003636, LS_NMSE: 0.003524, Lr: 0.01
[2025-02-22 23:32:30] Epoch 50/200, Loss: 8.803724, Train_MMSE: 0.00472, NMMSE: 0.003714, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:33:18] Epoch 51/200, Loss: 8.640929, Train_MMSE: 0.004415, NMMSE: 0.003548, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:34:06] Epoch 52/200, Loss: 8.670907, Train_MMSE: 0.004397, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:34:54] Epoch 53/200, Loss: 8.716290, Train_MMSE: 0.004425, NMMSE: 0.003595, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:35:42] Epoch 54/200, Loss: 8.673551, Train_MMSE: 0.004417, NMMSE: 0.003549, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:36:30] Epoch 55/200, Loss: 8.609892, Train_MMSE: 0.0044, NMMSE: 0.003539, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:37:18] Epoch 56/200, Loss: 8.638245, Train_MMSE: 0.004403, NMMSE: 0.003546, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:38:06] Epoch 57/200, Loss: 8.677237, Train_MMSE: 0.004398, NMMSE: 0.003541, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:38:53] Epoch 58/200, Loss: 8.689753, Train_MMSE: 0.004422, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:39:42] Epoch 59/200, Loss: 8.678887, Train_MMSE: 0.004428, NMMSE: 0.003656, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:40:29] Epoch 60/200, Loss: 8.737386, Train_MMSE: 0.004433, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:41:16] Epoch 61/200, Loss: 8.667066, Train_MMSE: 0.004424, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:42:03] Epoch 62/200, Loss: 8.674131, Train_MMSE: 0.004405, NMMSE: 0.003546, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:42:50] Epoch 63/200, Loss: 8.731307, Train_MMSE: 0.004415, NMMSE: 0.003578, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:43:37] Epoch 64/200, Loss: 8.664323, Train_MMSE: 0.004409, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:44:24] Epoch 65/200, Loss: 8.609369, Train_MMSE: 0.00441, NMMSE: 0.003542, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:45:10] Epoch 66/200, Loss: 8.873024, Train_MMSE: 0.004414, NMMSE: 0.003568, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:45:57] Epoch 67/200, Loss: 8.682939, Train_MMSE: 0.004413, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:46:44] Epoch 68/200, Loss: 9.043008, Train_MMSE: 0.004425, NMMSE: 0.003576, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:47:30] Epoch 69/200, Loss: 8.642059, Train_MMSE: 0.004396, NMMSE: 0.00355, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:48:18] Epoch 70/200, Loss: 8.693876, Train_MMSE: 0.004429, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:49:05] Epoch 71/200, Loss: 8.757300, Train_MMSE: 0.004399, NMMSE: 0.003567, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:49:51] Epoch 72/200, Loss: 8.632119, Train_MMSE: 0.004402, NMMSE: 0.003545, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:50:38] Epoch 73/200, Loss: 8.635295, Train_MMSE: 0.004409, NMMSE: 0.003556, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:51:25] Epoch 74/200, Loss: 8.656004, Train_MMSE: 0.004397, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:52:13] Epoch 75/200, Loss: 8.647830, Train_MMSE: 0.004407, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:53:00] Epoch 76/200, Loss: 9.130338, Train_MMSE: 0.004407, NMMSE: 0.00362, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:53:47] Epoch 77/200, Loss: 8.686174, Train_MMSE: 0.004396, NMMSE: 0.003588, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:54:34] Epoch 78/200, Loss: 8.770859, Train_MMSE: 0.004394, NMMSE: 0.003576, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:55:21] Epoch 79/200, Loss: 8.713943, Train_MMSE: 0.004402, NMMSE: 0.003584, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:56:09] Epoch 80/200, Loss: 8.665764, Train_MMSE: 0.004409, NMMSE: 0.003573, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:56:56] Epoch 81/200, Loss: 8.674613, Train_MMSE: 0.004427, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:57:43] Epoch 82/200, Loss: 8.635673, Train_MMSE: 0.004418, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:58:30] Epoch 83/200, Loss: 8.618883, Train_MMSE: 0.0044, NMMSE: 0.003612, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-22 23:59:17] Epoch 84/200, Loss: 8.748651, Train_MMSE: 0.0044, NMMSE: 0.003593, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:00:05] Epoch 85/200, Loss: 8.637037, Train_MMSE: 0.004405, NMMSE: 0.003607, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:00:52] Epoch 86/200, Loss: 8.631920, Train_MMSE: 0.004409, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:01:40] Epoch 87/200, Loss: 8.688065, Train_MMSE: 0.004405, NMMSE: 0.003554, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:02:27] Epoch 88/200, Loss: 8.634768, Train_MMSE: 0.004402, NMMSE: 0.003586, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:03:14] Epoch 89/200, Loss: 8.690194, Train_MMSE: 0.0044, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:04:01] Epoch 90/200, Loss: 9.293638, Train_MMSE: 0.004398, NMMSE: 0.003578, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:04:49] Epoch 91/200, Loss: 9.228425, Train_MMSE: 0.004395, NMMSE: 0.003582, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:05:36] Epoch 92/200, Loss: 8.694855, Train_MMSE: 0.004418, NMMSE: 0.003624, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:06:23] Epoch 93/200, Loss: 8.610511, Train_MMSE: 0.004402, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:07:11] Epoch 94/200, Loss: 8.854803, Train_MMSE: 0.004398, NMMSE: 0.003584, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:07:54] Epoch 95/200, Loss: 8.633207, Train_MMSE: 0.00439, NMMSE: 0.003576, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:08:34] Epoch 96/200, Loss: 8.632160, Train_MMSE: 0.004392, NMMSE: 0.003574, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:09:13] Epoch 97/200, Loss: 8.852188, Train_MMSE: 0.004391, NMMSE: 0.003589, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:09:52] Epoch 98/200, Loss: 8.806035, Train_MMSE: 0.004408, NMMSE: 0.003593, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:10:30] Epoch 99/200, Loss: 8.598399, Train_MMSE: 0.004394, NMMSE: 0.003586, LS_NMSE: 0.003524, Lr: 0.001
[2025-02-23 00:11:09] Epoch 100/200, Loss: 8.565735, Train_MMSE: 0.004395, NMMSE: 0.003604, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:11:49] Epoch 101/200, Loss: 8.616684, Train_MMSE: 0.00438, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:12:28] Epoch 102/200, Loss: 8.630469, Train_MMSE: 0.004381, NMMSE: 0.003558, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:13:07] Epoch 103/200, Loss: 8.615849, Train_MMSE: 0.004375, NMMSE: 0.003558, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:13:45] Epoch 104/200, Loss: 8.727707, Train_MMSE: 0.004373, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:14:24] Epoch 105/200, Loss: 8.611295, Train_MMSE: 0.004379, NMMSE: 0.003559, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:15:03] Epoch 106/200, Loss: 8.635758, Train_MMSE: 0.004369, NMMSE: 0.003559, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:15:41] Epoch 107/200, Loss: 8.634685, Train_MMSE: 0.004363, NMMSE: 0.003558, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:16:20] Epoch 108/200, Loss: 8.751266, Train_MMSE: 0.004393, NMMSE: 0.003559, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:16:58] Epoch 109/200, Loss: 8.581163, Train_MMSE: 0.004376, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:17:36] Epoch 110/200, Loss: 8.690907, Train_MMSE: 0.004375, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:18:16] Epoch 111/200, Loss: 8.596251, Train_MMSE: 0.004368, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:18:55] Epoch 112/200, Loss: 8.588630, Train_MMSE: 0.00437, NMMSE: 0.003559, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:19:34] Epoch 113/200, Loss: 8.595876, Train_MMSE: 0.004371, NMMSE: 0.003559, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:20:12] Epoch 114/200, Loss: 8.633082, Train_MMSE: 0.004368, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:20:50] Epoch 115/200, Loss: 8.594752, Train_MMSE: 0.004366, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:21:29] Epoch 116/200, Loss: 8.567302, Train_MMSE: 0.004379, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:22:08] Epoch 117/200, Loss: 8.627919, Train_MMSE: 0.004369, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:22:46] Epoch 118/200, Loss: 8.604606, Train_MMSE: 0.004368, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:23:25] Epoch 119/200, Loss: 8.800254, Train_MMSE: 0.004377, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:24:03] Epoch 120/200, Loss: 8.656876, Train_MMSE: 0.004369, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:24:42] Epoch 121/200, Loss: 8.764426, Train_MMSE: 0.004363, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:25:21] Epoch 122/200, Loss: 8.643385, Train_MMSE: 0.004378, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:25:59] Epoch 123/200, Loss: 8.575879, Train_MMSE: 0.004378, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:26:38] Epoch 124/200, Loss: 8.675611, Train_MMSE: 0.004364, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:27:17] Epoch 125/200, Loss: 8.652577, Train_MMSE: 0.004358, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:27:56] Epoch 126/200, Loss: 8.676831, Train_MMSE: 0.004364, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:28:35] Epoch 127/200, Loss: 8.607205, Train_MMSE: 0.004368, NMMSE: 0.00356, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:29:13] Epoch 128/200, Loss: 8.721315, Train_MMSE: 0.004388, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:29:51] Epoch 129/200, Loss: 8.649355, Train_MMSE: 0.004362, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:30:30] Epoch 130/200, Loss: 8.570731, Train_MMSE: 0.004356, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:31:09] Epoch 131/200, Loss: 8.585648, Train_MMSE: 0.004374, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:31:47] Epoch 132/200, Loss: 8.582845, Train_MMSE: 0.004368, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:32:26] Epoch 133/200, Loss: 8.641273, Train_MMSE: 0.004377, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:33:04] Epoch 134/200, Loss: 8.840814, Train_MMSE: 0.004379, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:33:43] Epoch 135/200, Loss: 8.572872, Train_MMSE: 0.004354, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:34:22] Epoch 136/200, Loss: 8.606052, Train_MMSE: 0.00437, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:35:00] Epoch 137/200, Loss: 8.650389, Train_MMSE: 0.004371, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:35:39] Epoch 138/200, Loss: 8.573612, Train_MMSE: 0.004372, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:36:17] Epoch 139/200, Loss: 8.664664, Train_MMSE: 0.004369, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:36:55] Epoch 140/200, Loss: 8.623175, Train_MMSE: 0.004366, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:37:33] Epoch 141/200, Loss: 8.640180, Train_MMSE: 0.004368, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:38:12] Epoch 142/200, Loss: 8.623761, Train_MMSE: 0.004391, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:38:50] Epoch 143/200, Loss: 8.622791, Train_MMSE: 0.00437, NMMSE: 0.003561, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:39:29] Epoch 144/200, Loss: 8.619761, Train_MMSE: 0.004369, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:40:07] Epoch 145/200, Loss: 8.710426, Train_MMSE: 0.004379, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:40:46] Epoch 146/200, Loss: 9.148097, Train_MMSE: 0.004378, NMMSE: 0.003581, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:41:24] Epoch 147/200, Loss: 8.604529, Train_MMSE: 0.004385, NMMSE: 0.003562, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:42:02] Epoch 148/200, Loss: 8.609097, Train_MMSE: 0.004369, NMMSE: 0.003567, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:42:40] Epoch 149/200, Loss: 8.600170, Train_MMSE: 0.00437, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 0.0001
[2025-02-23 00:43:18] Epoch 150/200, Loss: 8.589101, Train_MMSE: 0.004369, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:43:56] Epoch 151/200, Loss: 8.595270, Train_MMSE: 0.004363, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:44:29] Epoch 152/200, Loss: 8.569020, Train_MMSE: 0.004367, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:45:02] Epoch 153/200, Loss: 8.807175, Train_MMSE: 0.004367, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:45:35] Epoch 154/200, Loss: 8.596634, Train_MMSE: 0.004373, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:46:07] Epoch 155/200, Loss: 8.653017, Train_MMSE: 0.00437, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:46:38] Epoch 156/200, Loss: 8.623647, Train_MMSE: 0.004361, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:47:07] Epoch 157/200, Loss: 8.605342, Train_MMSE: 0.004359, NMMSE: 0.003575, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:47:29] Epoch 158/200, Loss: 8.609808, Train_MMSE: 0.004359, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:47:44] Epoch 159/200, Loss: 8.619245, Train_MMSE: 0.004366, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:48:00] Epoch 160/200, Loss: 8.633779, Train_MMSE: 0.004358, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:48:15] Epoch 161/200, Loss: 8.664174, Train_MMSE: 0.004353, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:48:30] Epoch 162/200, Loss: 8.618979, Train_MMSE: 0.004364, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:48:45] Epoch 163/200, Loss: 8.581538, Train_MMSE: 0.004367, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:00] Epoch 164/200, Loss: 8.578496, Train_MMSE: 0.004348, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:08] Epoch 165/200, Loss: 8.650118, Train_MMSE: 0.004361, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:16] Epoch 166/200, Loss: 8.618871, Train_MMSE: 0.004362, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:24] Epoch 167/200, Loss: 8.580671, Train_MMSE: 0.004362, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:32] Epoch 168/200, Loss: 8.620555, Train_MMSE: 0.00436, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:40] Epoch 169/200, Loss: 8.627210, Train_MMSE: 0.00436, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:47] Epoch 170/200, Loss: 8.736212, Train_MMSE: 0.004379, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:49:55] Epoch 171/200, Loss: 8.648875, Train_MMSE: 0.004365, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:03] Epoch 172/200, Loss: 8.729399, Train_MMSE: 0.004352, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:11] Epoch 173/200, Loss: 8.557290, Train_MMSE: 0.004368, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:19] Epoch 174/200, Loss: 8.732778, Train_MMSE: 0.004364, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:27] Epoch 175/200, Loss: 8.622045, Train_MMSE: 0.004378, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:34] Epoch 176/200, Loss: 8.737431, Train_MMSE: 0.004377, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:42] Epoch 177/200, Loss: 8.653374, Train_MMSE: 0.004357, NMMSE: 0.003569, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:50] Epoch 178/200, Loss: 8.702174, Train_MMSE: 0.004363, NMMSE: 0.003568, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:50:58] Epoch 179/200, Loss: 8.636853, Train_MMSE: 0.004379, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:06] Epoch 180/200, Loss: 8.598330, Train_MMSE: 0.004357, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:14] Epoch 181/200, Loss: 8.587209, Train_MMSE: 0.004356, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:21] Epoch 182/200, Loss: 8.688420, Train_MMSE: 0.004361, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:29] Epoch 183/200, Loss: 8.626292, Train_MMSE: 0.004367, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:37] Epoch 184/200, Loss: 8.611116, Train_MMSE: 0.004359, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:45] Epoch 185/200, Loss: 8.608823, Train_MMSE: 0.004359, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:51:53] Epoch 186/200, Loss: 8.574482, Train_MMSE: 0.00436, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:00] Epoch 187/200, Loss: 8.605516, Train_MMSE: 0.004369, NMMSE: 0.003567, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:08] Epoch 188/200, Loss: 8.623283, Train_MMSE: 0.004363, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:16] Epoch 189/200, Loss: 8.587624, Train_MMSE: 0.004373, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:24] Epoch 190/200, Loss: 8.500294, Train_MMSE: 0.00436, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:31] Epoch 191/200, Loss: 8.626711, Train_MMSE: 0.004353, NMMSE: 0.003567, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:39] Epoch 192/200, Loss: 8.603596, Train_MMSE: 0.004359, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:47] Epoch 193/200, Loss: 8.618158, Train_MMSE: 0.00436, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:52:55] Epoch 194/200, Loss: 8.711342, Train_MMSE: 0.004369, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:03] Epoch 195/200, Loss: 8.640251, Train_MMSE: 0.004352, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:11] Epoch 196/200, Loss: 8.634401, Train_MMSE: 0.004371, NMMSE: 0.003565, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:18] Epoch 197/200, Loss: 8.642312, Train_MMSE: 0.004363, NMMSE: 0.003563, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:26] Epoch 198/200, Loss: 8.589315, Train_MMSE: 0.00437, NMMSE: 0.003566, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:34] Epoch 199/200, Loss: 8.656316, Train_MMSE: 0.004364, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1e-05
[2025-02-23 00:53:42] Epoch 200/200, Loss: 8.645786, Train_MMSE: 0.004362, NMMSE: 0.003564, LS_NMSE: 0.003524, Lr: 1.0000000000000002e-06
