Train.py PID: 4076

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.055706420795849976
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L2_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L2_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C32_test_Dataset_dB-10_N36_K4_L2_S9_Setup10_Reliz1000.log',
 'logger': {'path': None},
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 32,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 1.71 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fa1cb0fe780>
loss function:: L1Loss()
[2025-02-22 20:28:41] Epoch 1/200, Loss: 62.664558, Train_MMSE: 0.268647, NMMSE: 0.210833, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:29:14] Epoch 2/200, Loss: 53.536308, Train_MMSE: 0.208818, NMMSE: 0.159987, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:29:48] Epoch 3/200, Loss: 50.405746, Train_MMSE: 0.167807, NMMSE: 0.142227, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:30:21] Epoch 4/200, Loss: 47.112476, Train_MMSE: 0.147639, NMMSE: 0.126545, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:30:54] Epoch 5/200, Loss: 45.361046, Train_MMSE: 0.132936, NMMSE: 0.116675, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:31:28] Epoch 6/200, Loss: 43.455708, Train_MMSE: 0.124006, NMMSE: 0.111265, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:32:01] Epoch 7/200, Loss: 43.327106, Train_MMSE: 0.118092, NMMSE: 0.10626, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:32:34] Epoch 8/200, Loss: 42.141884, Train_MMSE: 0.113596, NMMSE: 0.102412, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:33:08] Epoch 9/200, Loss: 41.746689, Train_MMSE: 0.110015, NMMSE: 0.101463, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:33:41] Epoch 10/200, Loss: 41.038719, Train_MMSE: 0.107762, NMMSE: 0.099357, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:34:14] Epoch 11/200, Loss: 40.866089, Train_MMSE: 0.105756, NMMSE: 0.097365, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:34:48] Epoch 12/200, Loss: 40.793022, Train_MMSE: 0.10428, NMMSE: 0.09562, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:35:21] Epoch 13/200, Loss: 40.199524, Train_MMSE: 0.103126, NMMSE: 0.095514, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:35:55] Epoch 14/200, Loss: 39.901367, Train_MMSE: 0.102202, NMMSE: 0.094353, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:36:28] Epoch 15/200, Loss: 40.048153, Train_MMSE: 0.101262, NMMSE: 0.094394, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:37:01] Epoch 16/200, Loss: 40.074162, Train_MMSE: 0.100474, NMMSE: 0.093366, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:37:34] Epoch 17/200, Loss: 39.827015, Train_MMSE: 0.099844, NMMSE: 0.092425, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:38:07] Epoch 18/200, Loss: 39.800312, Train_MMSE: 0.099219, NMMSE: 0.092286, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:38:41] Epoch 19/200, Loss: 39.541702, Train_MMSE: 0.098764, NMMSE: 0.091987, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:39:14] Epoch 20/200, Loss: 39.884510, Train_MMSE: 0.098183, NMMSE: 0.091539, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:39:47] Epoch 21/200, Loss: 39.270481, Train_MMSE: 0.097754, NMMSE: 0.091617, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:40:20] Epoch 22/200, Loss: 39.302975, Train_MMSE: 0.097337, NMMSE: 0.090543, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:40:54] Epoch 23/200, Loss: 39.153214, Train_MMSE: 0.097081, NMMSE: 0.090312, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:41:27] Epoch 24/200, Loss: 38.885387, Train_MMSE: 0.096607, NMMSE: 0.09019, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:42:01] Epoch 25/200, Loss: 39.358307, Train_MMSE: 0.096311, NMMSE: 0.090124, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:42:34] Epoch 26/200, Loss: 38.622837, Train_MMSE: 0.096025, NMMSE: 0.089459, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:43:07] Epoch 27/200, Loss: 39.658649, Train_MMSE: 0.095718, NMMSE: 0.088537, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:43:40] Epoch 28/200, Loss: 39.002979, Train_MMSE: 0.095454, NMMSE: 0.089636, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:44:14] Epoch 29/200, Loss: 38.672459, Train_MMSE: 0.095214, NMMSE: 0.089226, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:44:49] Epoch 30/200, Loss: 39.157436, Train_MMSE: 0.094986, NMMSE: 0.089496, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:45:22] Epoch 31/200, Loss: 38.906452, Train_MMSE: 0.094679, NMMSE: 0.088696, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:45:57] Epoch 32/200, Loss: 39.319782, Train_MMSE: 0.094462, NMMSE: 0.088185, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:46:41] Epoch 33/200, Loss: 38.553661, Train_MMSE: 0.094246, NMMSE: 0.088118, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:47:29] Epoch 34/200, Loss: 38.270718, Train_MMSE: 0.094183, NMMSE: 0.08815, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:48:19] Epoch 35/200, Loss: 38.631626, Train_MMSE: 0.093949, NMMSE: 0.088012, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:49:07] Epoch 36/200, Loss: 38.411579, Train_MMSE: 0.093676, NMMSE: 0.087152, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:49:55] Epoch 37/200, Loss: 38.493000, Train_MMSE: 0.093562, NMMSE: 0.087781, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:50:44] Epoch 38/200, Loss: 39.054184, Train_MMSE: 0.093395, NMMSE: 0.086699, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:51:33] Epoch 39/200, Loss: 38.481339, Train_MMSE: 0.093203, NMMSE: 0.087811, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:52:21] Epoch 40/200, Loss: 38.077454, Train_MMSE: 0.093076, NMMSE: 0.086782, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:53:10] Epoch 41/200, Loss: 38.471767, Train_MMSE: 0.092849, NMMSE: 0.087742, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:53:58] Epoch 42/200, Loss: 38.063595, Train_MMSE: 0.092784, NMMSE: 0.086578, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:54:47] Epoch 43/200, Loss: 38.101273, Train_MMSE: 0.092559, NMMSE: 0.087915, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:55:34] Epoch 44/200, Loss: 38.505302, Train_MMSE: 0.092457, NMMSE: 0.087101, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:56:29] Epoch 45/200, Loss: 38.108936, Train_MMSE: 0.092269, NMMSE: 0.086986, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:57:24] Epoch 46/200, Loss: 38.412987, Train_MMSE: 0.092158, NMMSE: 0.086865, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:58:19] Epoch 47/200, Loss: 38.355644, Train_MMSE: 0.09196, NMMSE: 0.08626, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 20:59:15] Epoch 48/200, Loss: 37.892815, Train_MMSE: 0.091862, NMMSE: 0.086164, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:00:17] Epoch 49/200, Loss: 38.322529, Train_MMSE: 0.091722, NMMSE: 0.086465, LS_NMSE: 0.242602, Lr: 0.01
[2025-02-22 21:01:21] Epoch 50/200, Loss: 37.894051, Train_MMSE: 0.091683, NMMSE: 0.086824, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:02:24] Epoch 51/200, Loss: 37.086384, Train_MMSE: 0.087785, NMMSE: 0.081829, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:03:27] Epoch 52/200, Loss: 36.858616, Train_MMSE: 0.0872, NMMSE: 0.081733, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:04:30] Epoch 53/200, Loss: 37.190716, Train_MMSE: 0.08704, NMMSE: 0.081523, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:05:33] Epoch 54/200, Loss: 36.901344, Train_MMSE: 0.086912, NMMSE: 0.081625, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:06:35] Epoch 55/200, Loss: 36.702442, Train_MMSE: 0.086828, NMMSE: 0.081546, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:07:37] Epoch 56/200, Loss: 36.709469, Train_MMSE: 0.086729, NMMSE: 0.08152, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:08:39] Epoch 57/200, Loss: 36.638500, Train_MMSE: 0.086663, NMMSE: 0.081364, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:09:41] Epoch 58/200, Loss: 36.714577, Train_MMSE: 0.086581, NMMSE: 0.081384, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:10:44] Epoch 59/200, Loss: 37.054276, Train_MMSE: 0.086502, NMMSE: 0.081471, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:11:46] Epoch 60/200, Loss: 36.592236, Train_MMSE: 0.08644, NMMSE: 0.081315, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:12:48] Epoch 61/200, Loss: 37.232773, Train_MMSE: 0.086381, NMMSE: 0.081353, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:13:50] Epoch 62/200, Loss: 37.144093, Train_MMSE: 0.086325, NMMSE: 0.08122, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:14:49] Epoch 63/200, Loss: 36.835995, Train_MMSE: 0.08627, NMMSE: 0.081304, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:15:45] Epoch 64/200, Loss: 36.542500, Train_MMSE: 0.086234, NMMSE: 0.081245, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:16:40] Epoch 65/200, Loss: 36.748665, Train_MMSE: 0.08618, NMMSE: 0.081162, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:17:35] Epoch 66/200, Loss: 36.622356, Train_MMSE: 0.086121, NMMSE: 0.081189, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:18:32] Epoch 67/200, Loss: 36.785290, Train_MMSE: 0.086064, NMMSE: 0.081194, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:19:27] Epoch 68/200, Loss: 36.699574, Train_MMSE: 0.086029, NMMSE: 0.081115, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:20:24] Epoch 69/200, Loss: 36.855808, Train_MMSE: 0.085972, NMMSE: 0.081195, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:21:22] Epoch 70/200, Loss: 36.798634, Train_MMSE: 0.085931, NMMSE: 0.081124, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:22:20] Epoch 71/200, Loss: 37.109116, Train_MMSE: 0.085889, NMMSE: 0.081081, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:23:19] Epoch 72/200, Loss: 36.580681, Train_MMSE: 0.085866, NMMSE: 0.081012, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:24:17] Epoch 73/200, Loss: 36.612904, Train_MMSE: 0.085806, NMMSE: 0.081083, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:25:14] Epoch 74/200, Loss: 36.732960, Train_MMSE: 0.085787, NMMSE: 0.081065, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:26:11] Epoch 75/200, Loss: 36.349377, Train_MMSE: 0.085707, NMMSE: 0.081151, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:27:10] Epoch 76/200, Loss: 36.599781, Train_MMSE: 0.08568, NMMSE: 0.08106, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:28:07] Epoch 77/200, Loss: 36.512512, Train_MMSE: 0.085669, NMMSE: 0.080972, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:29:06] Epoch 78/200, Loss: 36.714981, Train_MMSE: 0.085625, NMMSE: 0.0809, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:30:03] Epoch 79/200, Loss: 36.667126, Train_MMSE: 0.085594, NMMSE: 0.080919, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:31:01] Epoch 80/200, Loss: 36.952271, Train_MMSE: 0.085548, NMMSE: 0.08102, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:31:59] Epoch 81/200, Loss: 36.601063, Train_MMSE: 0.085503, NMMSE: 0.080879, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:32:57] Epoch 82/200, Loss: 36.775677, Train_MMSE: 0.085469, NMMSE: 0.081069, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:33:55] Epoch 83/200, Loss: 36.717152, Train_MMSE: 0.085444, NMMSE: 0.080962, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:34:53] Epoch 84/200, Loss: 36.730686, Train_MMSE: 0.08542, NMMSE: 0.080825, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:35:51] Epoch 85/200, Loss: 36.220650, Train_MMSE: 0.085385, NMMSE: 0.08099, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:36:49] Epoch 86/200, Loss: 36.332294, Train_MMSE: 0.085332, NMMSE: 0.080845, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:37:47] Epoch 87/200, Loss: 36.745972, Train_MMSE: 0.085309, NMMSE: 0.080951, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:38:44] Epoch 88/200, Loss: 36.541599, Train_MMSE: 0.085281, NMMSE: 0.080932, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:39:42] Epoch 89/200, Loss: 36.730293, Train_MMSE: 0.085238, NMMSE: 0.080891, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:40:40] Epoch 90/200, Loss: 36.500057, Train_MMSE: 0.085217, NMMSE: 0.080871, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:41:37] Epoch 91/200, Loss: 36.858784, Train_MMSE: 0.085173, NMMSE: 0.08076, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:42:35] Epoch 92/200, Loss: 37.043427, Train_MMSE: 0.085164, NMMSE: 0.080908, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:43:33] Epoch 93/200, Loss: 36.431606, Train_MMSE: 0.085118, NMMSE: 0.080853, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:44:30] Epoch 94/200, Loss: 36.922230, Train_MMSE: 0.085091, NMMSE: 0.081173, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:45:28] Epoch 95/200, Loss: 37.033916, Train_MMSE: 0.085065, NMMSE: 0.080868, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:46:26] Epoch 96/200, Loss: 36.653286, Train_MMSE: 0.085027, NMMSE: 0.080899, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:47:25] Epoch 97/200, Loss: 36.463867, Train_MMSE: 0.08501, NMMSE: 0.080811, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:48:22] Epoch 98/200, Loss: 36.250813, Train_MMSE: 0.084999, NMMSE: 0.080702, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:49:21] Epoch 99/200, Loss: 36.576824, Train_MMSE: 0.084952, NMMSE: 0.080635, LS_NMSE: 0.242602, Lr: 0.001
[2025-02-22 21:50:19] Epoch 100/200, Loss: 36.757401, Train_MMSE: 0.084935, NMMSE: 0.080714, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:51:17] Epoch 101/200, Loss: 36.412003, Train_MMSE: 0.084123, NMMSE: 0.080069, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:52:15] Epoch 102/200, Loss: 36.149033, Train_MMSE: 0.084021, NMMSE: 0.08005, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:53:13] Epoch 103/200, Loss: 36.363472, Train_MMSE: 0.083994, NMMSE: 0.080051, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:54:11] Epoch 104/200, Loss: 36.634602, Train_MMSE: 0.083989, NMMSE: 0.080029, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:55:09] Epoch 105/200, Loss: 36.551670, Train_MMSE: 0.083975, NMMSE: 0.080024, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:56:06] Epoch 106/200, Loss: 36.103306, Train_MMSE: 0.083968, NMMSE: 0.080031, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:57:05] Epoch 107/200, Loss: 36.017056, Train_MMSE: 0.083954, NMMSE: 0.08003, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:58:04] Epoch 108/200, Loss: 36.209248, Train_MMSE: 0.083951, NMMSE: 0.08004, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 21:59:01] Epoch 109/200, Loss: 36.020248, Train_MMSE: 0.083941, NMMSE: 0.080016, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:00:00] Epoch 110/200, Loss: 36.353069, Train_MMSE: 0.08393, NMMSE: 0.080021, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:00:57] Epoch 111/200, Loss: 36.327587, Train_MMSE: 0.083931, NMMSE: 0.080022, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:01:56] Epoch 112/200, Loss: 36.221844, Train_MMSE: 0.083922, NMMSE: 0.080014, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:02:53] Epoch 113/200, Loss: 36.389717, Train_MMSE: 0.083916, NMMSE: 0.080006, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:03:51] Epoch 114/200, Loss: 36.494358, Train_MMSE: 0.083905, NMMSE: 0.080028, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:04:48] Epoch 115/200, Loss: 36.192841, Train_MMSE: 0.083899, NMMSE: 0.080039, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:05:46] Epoch 116/200, Loss: 36.043804, Train_MMSE: 0.083901, NMMSE: 0.080034, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:06:43] Epoch 117/200, Loss: 36.653049, Train_MMSE: 0.08389, NMMSE: 0.080051, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:07:41] Epoch 118/200, Loss: 36.306843, Train_MMSE: 0.083882, NMMSE: 0.080025, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:08:39] Epoch 119/200, Loss: 36.224178, Train_MMSE: 0.083869, NMMSE: 0.080036, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:09:36] Epoch 120/200, Loss: 36.356270, Train_MMSE: 0.083872, NMMSE: 0.080007, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:10:34] Epoch 121/200, Loss: 36.195606, Train_MMSE: 0.083869, NMMSE: 0.080045, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:11:32] Epoch 122/200, Loss: 36.203571, Train_MMSE: 0.083862, NMMSE: 0.080004, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:12:30] Epoch 123/200, Loss: 36.220592, Train_MMSE: 0.083853, NMMSE: 0.08002, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:13:28] Epoch 124/200, Loss: 36.481632, Train_MMSE: 0.08385, NMMSE: 0.080014, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:14:25] Epoch 125/200, Loss: 35.916340, Train_MMSE: 0.083838, NMMSE: 0.080036, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:15:23] Epoch 126/200, Loss: 36.593628, Train_MMSE: 0.083846, NMMSE: 0.080007, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:16:20] Epoch 127/200, Loss: 36.158611, Train_MMSE: 0.083838, NMMSE: 0.080007, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:17:09] Epoch 128/200, Loss: 36.279770, Train_MMSE: 0.08383, NMMSE: 0.080025, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:17:55] Epoch 129/200, Loss: 36.178516, Train_MMSE: 0.083821, NMMSE: 0.080012, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:18:43] Epoch 130/200, Loss: 36.425190, Train_MMSE: 0.083819, NMMSE: 0.080014, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:19:34] Epoch 131/200, Loss: 36.131340, Train_MMSE: 0.083813, NMMSE: 0.080011, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:20:38] Epoch 132/200, Loss: 36.770557, Train_MMSE: 0.083807, NMMSE: 0.08003, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:21:48] Epoch 133/200, Loss: 36.195591, Train_MMSE: 0.083804, NMMSE: 0.080011, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:23:18] Epoch 134/200, Loss: 36.144138, Train_MMSE: 0.083798, NMMSE: 0.080026, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:24:51] Epoch 135/200, Loss: 36.001968, Train_MMSE: 0.083791, NMMSE: 0.079987, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:26:21] Epoch 136/200, Loss: 36.456322, Train_MMSE: 0.083783, NMMSE: 0.080007, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:27:53] Epoch 137/200, Loss: 36.326496, Train_MMSE: 0.083785, NMMSE: 0.079991, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:29:25] Epoch 138/200, Loss: 36.571114, Train_MMSE: 0.083771, NMMSE: 0.079981, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:30:59] Epoch 139/200, Loss: 36.429672, Train_MMSE: 0.083774, NMMSE: 0.079992, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:32:30] Epoch 140/200, Loss: 36.122108, Train_MMSE: 0.08376, NMMSE: 0.080005, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:34:01] Epoch 141/200, Loss: 36.276356, Train_MMSE: 0.08376, NMMSE: 0.079982, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:35:35] Epoch 142/200, Loss: 36.750332, Train_MMSE: 0.083749, NMMSE: 0.079983, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:37:06] Epoch 143/200, Loss: 36.314308, Train_MMSE: 0.083746, NMMSE: 0.079978, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:38:38] Epoch 144/200, Loss: 36.532459, Train_MMSE: 0.08375, NMMSE: 0.079975, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:40:11] Epoch 145/200, Loss: 36.015003, Train_MMSE: 0.083732, NMMSE: 0.080013, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:41:44] Epoch 146/200, Loss: 36.575996, Train_MMSE: 0.083729, NMMSE: 0.079988, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:43:15] Epoch 147/200, Loss: 36.441238, Train_MMSE: 0.083733, NMMSE: 0.079957, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:44:45] Epoch 148/200, Loss: 35.995796, Train_MMSE: 0.083721, NMMSE: 0.08, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:46:12] Epoch 149/200, Loss: 36.152473, Train_MMSE: 0.083714, NMMSE: 0.079969, LS_NMSE: 0.242602, Lr: 0.0001
[2025-02-22 22:47:42] Epoch 150/200, Loss: 36.406464, Train_MMSE: 0.08371, NMMSE: 0.079985, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:49:16] Epoch 151/200, Loss: 35.993229, Train_MMSE: 0.083624, NMMSE: 0.079924, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:50:48] Epoch 152/200, Loss: 36.420353, Train_MMSE: 0.083605, NMMSE: 0.079917, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:52:23] Epoch 153/200, Loss: 36.152905, Train_MMSE: 0.083603, NMMSE: 0.079921, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:53:57] Epoch 154/200, Loss: 36.120407, Train_MMSE: 0.08361, NMMSE: 0.079914, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:55:29] Epoch 155/200, Loss: 36.384583, Train_MMSE: 0.083611, NMMSE: 0.079915, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:57:01] Epoch 156/200, Loss: 36.384567, Train_MMSE: 0.0836, NMMSE: 0.079931, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 22:58:35] Epoch 157/200, Loss: 36.315540, Train_MMSE: 0.083599, NMMSE: 0.07991, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:00:07] Epoch 158/200, Loss: 36.604156, Train_MMSE: 0.083599, NMMSE: 0.079905, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:01:39] Epoch 159/200, Loss: 36.237652, Train_MMSE: 0.083602, NMMSE: 0.079923, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:03:13] Epoch 160/200, Loss: 36.402653, Train_MMSE: 0.083602, NMMSE: 0.079926, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:04:47] Epoch 161/200, Loss: 36.290783, Train_MMSE: 0.083605, NMMSE: 0.079942, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:06:21] Epoch 162/200, Loss: 36.097618, Train_MMSE: 0.083595, NMMSE: 0.079915, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:07:55] Epoch 163/200, Loss: 36.361637, Train_MMSE: 0.083596, NMMSE: 0.079916, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:09:26] Epoch 164/200, Loss: 36.180672, Train_MMSE: 0.083595, NMMSE: 0.07992, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:10:55] Epoch 165/200, Loss: 36.226524, Train_MMSE: 0.083598, NMMSE: 0.079926, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:12:23] Epoch 166/200, Loss: 36.503654, Train_MMSE: 0.083594, NMMSE: 0.079924, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:13:51] Epoch 167/200, Loss: 36.219040, Train_MMSE: 0.083597, NMMSE: 0.079911, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:15:19] Epoch 168/200, Loss: 35.846542, Train_MMSE: 0.08359, NMMSE: 0.079917, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:16:48] Epoch 169/200, Loss: 36.136513, Train_MMSE: 0.083593, NMMSE: 0.079921, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:18:18] Epoch 170/200, Loss: 35.661690, Train_MMSE: 0.08359, NMMSE: 0.079927, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:19:55] Epoch 171/200, Loss: 36.071888, Train_MMSE: 0.083598, NMMSE: 0.079929, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:21:36] Epoch 172/200, Loss: 36.095894, Train_MMSE: 0.083594, NMMSE: 0.079922, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:23:15] Epoch 173/200, Loss: 35.878220, Train_MMSE: 0.083588, NMMSE: 0.079923, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:24:55] Epoch 174/200, Loss: 36.326824, Train_MMSE: 0.083592, NMMSE: 0.079921, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:26:32] Epoch 175/200, Loss: 36.213284, Train_MMSE: 0.083589, NMMSE: 0.079916, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:28:11] Epoch 176/200, Loss: 36.264885, Train_MMSE: 0.083597, NMMSE: 0.079926, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:29:48] Epoch 177/200, Loss: 36.467216, Train_MMSE: 0.083597, NMMSE: 0.079921, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:31:26] Epoch 178/200, Loss: 35.914185, Train_MMSE: 0.083597, NMMSE: 0.079912, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:33:02] Epoch 179/200, Loss: 36.038662, Train_MMSE: 0.083585, NMMSE: 0.079921, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:34:38] Epoch 180/200, Loss: 36.349606, Train_MMSE: 0.083585, NMMSE: 0.07992, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:36:15] Epoch 181/200, Loss: 36.231174, Train_MMSE: 0.083585, NMMSE: 0.07992, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:37:51] Epoch 182/200, Loss: 36.010937, Train_MMSE: 0.083591, NMMSE: 0.07992, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:39:27] Epoch 183/200, Loss: 36.492668, Train_MMSE: 0.083593, NMMSE: 0.079918, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:41:05] Epoch 184/200, Loss: 36.108055, Train_MMSE: 0.083591, NMMSE: 0.079922, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:42:42] Epoch 185/200, Loss: 36.280064, Train_MMSE: 0.083586, NMMSE: 0.079913, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:44:21] Epoch 186/200, Loss: 36.139088, Train_MMSE: 0.083591, NMMSE: 0.079922, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:46:02] Epoch 187/200, Loss: 36.131226, Train_MMSE: 0.083585, NMMSE: 0.079917, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:47:43] Epoch 188/200, Loss: 36.511951, Train_MMSE: 0.083594, NMMSE: 0.079914, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:49:24] Epoch 189/200, Loss: 36.151039, Train_MMSE: 0.083582, NMMSE: 0.079933, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:51:07] Epoch 190/200, Loss: 36.667835, Train_MMSE: 0.083584, NMMSE: 0.079928, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:52:46] Epoch 191/200, Loss: 36.177357, Train_MMSE: 0.083582, NMMSE: 0.07991, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:54:25] Epoch 192/200, Loss: 36.050617, Train_MMSE: 0.083583, NMMSE: 0.079915, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:56:04] Epoch 193/200, Loss: 36.093784, Train_MMSE: 0.083574, NMMSE: 0.079912, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:57:42] Epoch 194/200, Loss: 36.403370, Train_MMSE: 0.08358, NMMSE: 0.07991, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-22 23:59:20] Epoch 195/200, Loss: 36.354794, Train_MMSE: 0.083584, NMMSE: 0.07992, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:00:58] Epoch 196/200, Loss: 36.242924, Train_MMSE: 0.083583, NMMSE: 0.079912, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:02:38] Epoch 197/200, Loss: 35.859093, Train_MMSE: 0.083579, NMMSE: 0.079939, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:04:17] Epoch 198/200, Loss: 36.202412, Train_MMSE: 0.083585, NMMSE: 0.079906, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:05:55] Epoch 199/200, Loss: 36.492996, Train_MMSE: 0.083578, NMMSE: 0.079901, LS_NMSE: 0.242602, Lr: 1e-05
[2025-02-23 00:07:36] Epoch 200/200, Loss: 36.091545, Train_MMSE: 0.083576, NMMSE: 0.079913, LS_NMSE: 0.242602, Lr: 1.0000000000000002e-06
