Train.py PID: 37756

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.013322863042657928
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S10_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S10_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L4_S11_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fdbbc116f30>
loss function:: L1Loss()
[2025-02-22 22:22:15] Epoch 1/200, Loss: 16.723343, Train_MMSE: 0.016423, NMMSE: 0.016695, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:23:07] Epoch 2/200, Loss: 16.811262, Train_MMSE: 0.01638, NMMSE: 0.016623, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:24:01] Epoch 3/200, Loss: 16.830137, Train_MMSE: 0.016291, NMMSE: 0.016531, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:24:55] Epoch 4/200, Loss: 16.787321, Train_MMSE: 0.016218, NMMSE: 0.016479, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:25:49] Epoch 5/200, Loss: 16.658270, Train_MMSE: 0.016161, NMMSE: 0.016447, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:26:42] Epoch 6/200, Loss: 16.573116, Train_MMSE: 0.016116, NMMSE: 0.016427, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:27:37] Epoch 7/200, Loss: 16.647200, Train_MMSE: 0.01608, NMMSE: 0.016411, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:28:31] Epoch 8/200, Loss: 16.629831, Train_MMSE: 0.016041, NMMSE: 0.016402, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:29:27] Epoch 9/200, Loss: 16.636963, Train_MMSE: 0.016009, NMMSE: 0.016395, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:30:20] Epoch 10/200, Loss: 16.593563, Train_MMSE: 0.015974, NMMSE: 0.016391, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:31:14] Epoch 11/200, Loss: 16.650570, Train_MMSE: 0.015952, NMMSE: 0.016391, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:32:07] Epoch 12/200, Loss: 16.433664, Train_MMSE: 0.01592, NMMSE: 0.016393, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:33:00] Epoch 13/200, Loss: 16.567148, Train_MMSE: 0.015899, NMMSE: 0.016392, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:33:53] Epoch 14/200, Loss: 16.533604, Train_MMSE: 0.015875, NMMSE: 0.016393, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:34:45] Epoch 15/200, Loss: 16.590834, Train_MMSE: 0.015851, NMMSE: 0.016399, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:35:39] Epoch 16/200, Loss: 16.616564, Train_MMSE: 0.015828, NMMSE: 0.016405, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:36:31] Epoch 17/200, Loss: 16.482630, Train_MMSE: 0.015804, NMMSE: 0.016413, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:37:23] Epoch 18/200, Loss: 16.527208, Train_MMSE: 0.015784, NMMSE: 0.016442, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:38:15] Epoch 19/200, Loss: 16.428095, Train_MMSE: 0.015764, NMMSE: 0.016427, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:39:11] Epoch 20/200, Loss: 16.383438, Train_MMSE: 0.015747, NMMSE: 0.016445, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:40:03] Epoch 21/200, Loss: 16.371305, Train_MMSE: 0.015727, NMMSE: 0.016427, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:40:56] Epoch 22/200, Loss: 16.458744, Train_MMSE: 0.015705, NMMSE: 0.016453, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:41:48] Epoch 23/200, Loss: 16.394958, Train_MMSE: 0.015685, NMMSE: 0.016459, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:42:40] Epoch 24/200, Loss: 16.321926, Train_MMSE: 0.015667, NMMSE: 0.016464, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:43:33] Epoch 25/200, Loss: 16.405453, Train_MMSE: 0.015649, NMMSE: 0.016466, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:44:24] Epoch 26/200, Loss: 16.357817, Train_MMSE: 0.015625, NMMSE: 0.016496, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:45:15] Epoch 27/200, Loss: 16.382778, Train_MMSE: 0.015604, NMMSE: 0.016479, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:46:06] Epoch 28/200, Loss: 16.434685, Train_MMSE: 0.015588, NMMSE: 0.016516, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:46:56] Epoch 29/200, Loss: 16.344828, Train_MMSE: 0.015568, NMMSE: 0.016502, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:47:48] Epoch 30/200, Loss: 16.369202, Train_MMSE: 0.01555, NMMSE: 0.016496, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:48:43] Epoch 31/200, Loss: 16.373264, Train_MMSE: 0.015528, NMMSE: 0.016532, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:49:37] Epoch 32/200, Loss: 16.265842, Train_MMSE: 0.01551, NMMSE: 0.016549, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:50:31] Epoch 33/200, Loss: 16.287292, Train_MMSE: 0.015486, NMMSE: 0.016522, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:51:25] Epoch 34/200, Loss: 16.275160, Train_MMSE: 0.015466, NMMSE: 0.016508, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:52:19] Epoch 35/200, Loss: 16.255171, Train_MMSE: 0.015447, NMMSE: 0.016553, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:53:13] Epoch 36/200, Loss: 16.288530, Train_MMSE: 0.015428, NMMSE: 0.016541, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:54:07] Epoch 37/200, Loss: 16.272362, Train_MMSE: 0.015403, NMMSE: 0.016552, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:55:02] Epoch 38/200, Loss: 16.272635, Train_MMSE: 0.015383, NMMSE: 0.016597, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:55:56] Epoch 39/200, Loss: 16.330374, Train_MMSE: 0.01536, NMMSE: 0.016577, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:56:51] Epoch 40/200, Loss: 16.185032, Train_MMSE: 0.015341, NMMSE: 0.016569, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:57:45] Epoch 41/200, Loss: 16.207144, Train_MMSE: 0.015322, NMMSE: 0.016622, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:58:39] Epoch 42/200, Loss: 16.258959, Train_MMSE: 0.015302, NMMSE: 0.016587, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:59:34] Epoch 43/200, Loss: 16.173292, Train_MMSE: 0.015281, NMMSE: 0.016621, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:00:28] Epoch 44/200, Loss: 16.171289, Train_MMSE: 0.015257, NMMSE: 0.016572, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:01:23] Epoch 45/200, Loss: 16.236181, Train_MMSE: 0.015231, NMMSE: 0.016586, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:02:18] Epoch 46/200, Loss: 16.209614, Train_MMSE: 0.01521, NMMSE: 0.016628, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:03:12] Epoch 47/200, Loss: 16.083683, Train_MMSE: 0.015191, NMMSE: 0.016676, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:04:06] Epoch 48/200, Loss: 16.092133, Train_MMSE: 0.015168, NMMSE: 0.016623, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:05:00] Epoch 49/200, Loss: 16.119869, Train_MMSE: 0.01515, NMMSE: 0.016638, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:05:54] Epoch 50/200, Loss: 16.131212, Train_MMSE: 0.015124, NMMSE: 0.016677, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:06:49] Epoch 51/200, Loss: 15.780086, Train_MMSE: 0.014782, NMMSE: 0.016765, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:07:44] Epoch 52/200, Loss: 15.724607, Train_MMSE: 0.01459, NMMSE: 0.01684, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:08:37] Epoch 53/200, Loss: 15.660071, Train_MMSE: 0.014498, NMMSE: 0.016904, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:09:25] Epoch 54/200, Loss: 15.651258, Train_MMSE: 0.014426, NMMSE: 0.016964, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:10:15] Epoch 55/200, Loss: 15.531684, Train_MMSE: 0.014376, NMMSE: 0.017022, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:11:04] Epoch 56/200, Loss: 15.526700, Train_MMSE: 0.01433, NMMSE: 0.017062, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:11:53] Epoch 57/200, Loss: 15.479374, Train_MMSE: 0.014288, NMMSE: 0.017091, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:12:42] Epoch 58/200, Loss: 15.452220, Train_MMSE: 0.014249, NMMSE: 0.017142, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:13:31] Epoch 59/200, Loss: 15.423591, Train_MMSE: 0.014214, NMMSE: 0.017155, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:14:20] Epoch 60/200, Loss: 15.408118, Train_MMSE: 0.014181, NMMSE: 0.017206, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:15:09] Epoch 61/200, Loss: 15.431695, Train_MMSE: 0.01415, NMMSE: 0.017264, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:15:58] Epoch 62/200, Loss: 15.321923, Train_MMSE: 0.014124, NMMSE: 0.017278, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:16:47] Epoch 63/200, Loss: 15.336205, Train_MMSE: 0.014092, NMMSE: 0.017325, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:17:36] Epoch 64/200, Loss: 15.353577, Train_MMSE: 0.014068, NMMSE: 0.017351, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:18:24] Epoch 65/200, Loss: 15.320574, Train_MMSE: 0.014046, NMMSE: 0.017377, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:19:08] Epoch 66/200, Loss: 15.237993, Train_MMSE: 0.014017, NMMSE: 0.017427, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:19:52] Epoch 67/200, Loss: 15.258708, Train_MMSE: 0.014001, NMMSE: 0.017445, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:20:36] Epoch 68/200, Loss: 15.252038, Train_MMSE: 0.013977, NMMSE: 0.017467, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:21:20] Epoch 69/200, Loss: 15.309537, Train_MMSE: 0.013959, NMMSE: 0.017477, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:22:04] Epoch 70/200, Loss: 15.198408, Train_MMSE: 0.013938, NMMSE: 0.017505, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:22:48] Epoch 71/200, Loss: 15.238019, Train_MMSE: 0.013919, NMMSE: 0.017539, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:23:32] Epoch 72/200, Loss: 15.208286, Train_MMSE: 0.013902, NMMSE: 0.017567, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:24:16] Epoch 73/200, Loss: 15.161567, Train_MMSE: 0.013884, NMMSE: 0.017584, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:24:59] Epoch 74/200, Loss: 15.241362, Train_MMSE: 0.013865, NMMSE: 0.017622, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:25:44] Epoch 75/200, Loss: 15.213078, Train_MMSE: 0.013852, NMMSE: 0.01763, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:26:28] Epoch 76/200, Loss: 15.205678, Train_MMSE: 0.013836, NMMSE: 0.017684, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:27:12] Epoch 77/200, Loss: 15.106737, Train_MMSE: 0.013817, NMMSE: 0.017694, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:27:56] Epoch 78/200, Loss: 15.114417, Train_MMSE: 0.013803, NMMSE: 0.017713, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:28:41] Epoch 79/200, Loss: 15.194685, Train_MMSE: 0.01379, NMMSE: 0.01776, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:29:25] Epoch 80/200, Loss: 15.167125, Train_MMSE: 0.013777, NMMSE: 0.017773, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:30:09] Epoch 81/200, Loss: 15.087962, Train_MMSE: 0.013765, NMMSE: 0.017785, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:30:53] Epoch 82/200, Loss: 15.116052, Train_MMSE: 0.013748, NMMSE: 0.017771, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:31:38] Epoch 83/200, Loss: 15.103559, Train_MMSE: 0.013738, NMMSE: 0.017809, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:32:22] Epoch 84/200, Loss: 15.118735, Train_MMSE: 0.013728, NMMSE: 0.017828, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:33:07] Epoch 85/200, Loss: 15.120994, Train_MMSE: 0.013714, NMMSE: 0.017848, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:33:51] Epoch 86/200, Loss: 15.082892, Train_MMSE: 0.013698, NMMSE: 0.017876, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:34:35] Epoch 87/200, Loss: 15.127598, Train_MMSE: 0.013688, NMMSE: 0.017914, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:35:20] Epoch 88/200, Loss: 15.060656, Train_MMSE: 0.013672, NMMSE: 0.017875, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:36:04] Epoch 89/200, Loss: 15.105924, Train_MMSE: 0.013668, NMMSE: 0.017916, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:36:48] Epoch 90/200, Loss: 15.066155, Train_MMSE: 0.013654, NMMSE: 0.01792, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:37:33] Epoch 91/200, Loss: 15.086968, Train_MMSE: 0.013649, NMMSE: 0.017915, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:38:17] Epoch 92/200, Loss: 15.094027, Train_MMSE: 0.013633, NMMSE: 0.017942, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:39:01] Epoch 93/200, Loss: 15.063629, Train_MMSE: 0.013621, NMMSE: 0.018008, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:39:46] Epoch 94/200, Loss: 14.976242, Train_MMSE: 0.013615, NMMSE: 0.01803, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:40:30] Epoch 95/200, Loss: 15.090325, Train_MMSE: 0.013607, NMMSE: 0.018011, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:41:14] Epoch 96/200, Loss: 14.976283, Train_MMSE: 0.013597, NMMSE: 0.017996, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:41:58] Epoch 97/200, Loss: 15.038079, Train_MMSE: 0.013586, NMMSE: 0.018039, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:42:42] Epoch 98/200, Loss: 14.971370, Train_MMSE: 0.013578, NMMSE: 0.018039, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:43:26] Epoch 99/200, Loss: 14.987987, Train_MMSE: 0.01357, NMMSE: 0.018052, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:44:10] Epoch 100/200, Loss: 14.997437, Train_MMSE: 0.013561, NMMSE: 0.018044, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:44:52] Epoch 101/200, Loss: 14.747052, Train_MMSE: 0.013344, NMMSE: 0.018081, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:45:35] Epoch 102/200, Loss: 14.670510, Train_MMSE: 0.013281, NMMSE: 0.01811, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:46:18] Epoch 103/200, Loss: 14.753003, Train_MMSE: 0.013266, NMMSE: 0.018131, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:47:01] Epoch 104/200, Loss: 14.683455, Train_MMSE: 0.013255, NMMSE: 0.01814, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:47:44] Epoch 105/200, Loss: 14.722053, Train_MMSE: 0.013246, NMMSE: 0.018153, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:48:27] Epoch 106/200, Loss: 14.735552, Train_MMSE: 0.013241, NMMSE: 0.018166, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:49:10] Epoch 107/200, Loss: 14.622725, Train_MMSE: 0.013238, NMMSE: 0.018176, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:49:52] Epoch 108/200, Loss: 14.727247, Train_MMSE: 0.013229, NMMSE: 0.018181, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:50:35] Epoch 109/200, Loss: 14.711489, Train_MMSE: 0.013231, NMMSE: 0.018193, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:51:18] Epoch 110/200, Loss: 14.576416, Train_MMSE: 0.013223, NMMSE: 0.018197, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:52:02] Epoch 111/200, Loss: 14.683331, Train_MMSE: 0.013221, NMMSE: 0.018205, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:52:46] Epoch 112/200, Loss: 14.663337, Train_MMSE: 0.013223, NMMSE: 0.018211, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:53:29] Epoch 113/200, Loss: 14.641388, Train_MMSE: 0.013214, NMMSE: 0.018213, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:54:12] Epoch 114/200, Loss: 14.639729, Train_MMSE: 0.01321, NMMSE: 0.018224, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:54:55] Epoch 115/200, Loss: 14.694814, Train_MMSE: 0.01321, NMMSE: 0.018232, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:55:38] Epoch 116/200, Loss: 14.703561, Train_MMSE: 0.013209, NMMSE: 0.01824, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:56:22] Epoch 117/200, Loss: 14.578997, Train_MMSE: 0.013203, NMMSE: 0.01825, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:57:06] Epoch 118/200, Loss: 14.686292, Train_MMSE: 0.013203, NMMSE: 0.018257, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:57:49] Epoch 119/200, Loss: 14.620780, Train_MMSE: 0.013202, NMMSE: 0.018262, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:58:33] Epoch 120/200, Loss: 14.672662, Train_MMSE: 0.013199, NMMSE: 0.018262, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:59:16] Epoch 121/200, Loss: 14.651371, Train_MMSE: 0.013195, NMMSE: 0.018258, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:59:59] Epoch 122/200, Loss: 14.643827, Train_MMSE: 0.013193, NMMSE: 0.018272, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:00:42] Epoch 123/200, Loss: 14.580265, Train_MMSE: 0.013191, NMMSE: 0.018281, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:01:26] Epoch 124/200, Loss: 14.552938, Train_MMSE: 0.013191, NMMSE: 0.018277, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:02:09] Epoch 125/200, Loss: 14.599080, Train_MMSE: 0.01319, NMMSE: 0.018287, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:02:53] Epoch 126/200, Loss: 14.580287, Train_MMSE: 0.013185, NMMSE: 0.018285, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:03:36] Epoch 127/200, Loss: 14.581068, Train_MMSE: 0.013181, NMMSE: 0.018306, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:04:20] Epoch 128/200, Loss: 14.586602, Train_MMSE: 0.013185, NMMSE: 0.018299, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:05:04] Epoch 129/200, Loss: 14.746541, Train_MMSE: 0.01318, NMMSE: 0.018299, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:05:48] Epoch 130/200, Loss: 14.521851, Train_MMSE: 0.013177, NMMSE: 0.018318, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:06:32] Epoch 131/200, Loss: 14.655189, Train_MMSE: 0.013175, NMMSE: 0.018313, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:07:16] Epoch 132/200, Loss: 14.597711, Train_MMSE: 0.013169, NMMSE: 0.018324, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:07:56] Epoch 133/200, Loss: 14.638942, Train_MMSE: 0.013169, NMMSE: 0.018324, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:08:32] Epoch 134/200, Loss: 14.643064, Train_MMSE: 0.013172, NMMSE: 0.018327, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:09:07] Epoch 135/200, Loss: 14.630140, Train_MMSE: 0.013168, NMMSE: 0.018331, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:09:44] Epoch 136/200, Loss: 14.593791, Train_MMSE: 0.013169, NMMSE: 0.018333, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:10:19] Epoch 137/200, Loss: 14.596051, Train_MMSE: 0.013165, NMMSE: 0.018347, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:10:55] Epoch 138/200, Loss: 14.547810, Train_MMSE: 0.013166, NMMSE: 0.01836, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:11:31] Epoch 139/200, Loss: 14.613048, Train_MMSE: 0.013166, NMMSE: 0.018353, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:12:07] Epoch 140/200, Loss: 14.552854, Train_MMSE: 0.01316, NMMSE: 0.018353, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:12:43] Epoch 141/200, Loss: 14.654493, Train_MMSE: 0.013153, NMMSE: 0.018358, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:13:19] Epoch 142/200, Loss: 14.611386, Train_MMSE: 0.013158, NMMSE: 0.018365, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:13:55] Epoch 143/200, Loss: 14.601704, Train_MMSE: 0.013155, NMMSE: 0.018355, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:14:31] Epoch 144/200, Loss: 14.589769, Train_MMSE: 0.013159, NMMSE: 0.01837, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:15:07] Epoch 145/200, Loss: 14.634696, Train_MMSE: 0.013151, NMMSE: 0.018379, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:15:43] Epoch 146/200, Loss: 14.610417, Train_MMSE: 0.013152, NMMSE: 0.018375, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:16:19] Epoch 147/200, Loss: 14.579371, Train_MMSE: 0.013147, NMMSE: 0.01838, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:16:55] Epoch 148/200, Loss: 14.641011, Train_MMSE: 0.01315, NMMSE: 0.018393, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:17:31] Epoch 149/200, Loss: 14.571394, Train_MMSE: 0.013145, NMMSE: 0.018378, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:18:07] Epoch 150/200, Loss: 14.566875, Train_MMSE: 0.013147, NMMSE: 0.01839, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:18:43] Epoch 151/200, Loss: 14.532159, Train_MMSE: 0.013111, NMMSE: 0.018394, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:19:19] Epoch 152/200, Loss: 14.542975, Train_MMSE: 0.013111, NMMSE: 0.018395, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:19:55] Epoch 153/200, Loss: 14.472941, Train_MMSE: 0.013114, NMMSE: 0.018394, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:20:31] Epoch 154/200, Loss: 14.564656, Train_MMSE: 0.013113, NMMSE: 0.018396, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:21:07] Epoch 155/200, Loss: 14.506633, Train_MMSE: 0.013111, NMMSE: 0.018398, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:21:43] Epoch 156/200, Loss: 14.553018, Train_MMSE: 0.013107, NMMSE: 0.018398, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:22:19] Epoch 157/200, Loss: 14.575520, Train_MMSE: 0.013106, NMMSE: 0.018399, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:22:54] Epoch 158/200, Loss: 14.562581, Train_MMSE: 0.013107, NMMSE: 0.018399, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:23:30] Epoch 159/200, Loss: 14.568503, Train_MMSE: 0.013106, NMMSE: 0.0184, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:24:06] Epoch 160/200, Loss: 14.523041, Train_MMSE: 0.013106, NMMSE: 0.018401, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:24:43] Epoch 161/200, Loss: 14.509678, Train_MMSE: 0.013105, NMMSE: 0.018403, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:25:19] Epoch 162/200, Loss: 14.578905, Train_MMSE: 0.01311, NMMSE: 0.018403, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:25:54] Epoch 163/200, Loss: 14.570404, Train_MMSE: 0.013107, NMMSE: 0.018402, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:26:31] Epoch 164/200, Loss: 14.507577, Train_MMSE: 0.013105, NMMSE: 0.018403, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:27:07] Epoch 165/200, Loss: 14.552933, Train_MMSE: 0.013106, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:27:43] Epoch 166/200, Loss: 14.526997, Train_MMSE: 0.013108, NMMSE: 0.018402, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:28:19] Epoch 167/200, Loss: 14.550836, Train_MMSE: 0.013108, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:28:55] Epoch 168/200, Loss: 14.554286, Train_MMSE: 0.013108, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:29:31] Epoch 169/200, Loss: 14.429844, Train_MMSE: 0.013104, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:30:07] Epoch 170/200, Loss: 14.463969, Train_MMSE: 0.013103, NMMSE: 0.018406, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:30:43] Epoch 171/200, Loss: 14.525992, Train_MMSE: 0.013104, NMMSE: 0.018407, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:31:20] Epoch 172/200, Loss: 14.542111, Train_MMSE: 0.013105, NMMSE: 0.018404, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:31:56] Epoch 173/200, Loss: 14.466212, Train_MMSE: 0.013107, NMMSE: 0.018408, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:32:33] Epoch 174/200, Loss: 14.467261, Train_MMSE: 0.013103, NMMSE: 0.018409, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:33:09] Epoch 175/200, Loss: 14.551185, Train_MMSE: 0.013108, NMMSE: 0.018407, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:33:46] Epoch 176/200, Loss: 14.516344, Train_MMSE: 0.013104, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:34:22] Epoch 177/200, Loss: 14.559110, Train_MMSE: 0.013106, NMMSE: 0.01841, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:34:59] Epoch 178/200, Loss: 14.552012, Train_MMSE: 0.013106, NMMSE: 0.018405, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:35:35] Epoch 179/200, Loss: 14.529541, Train_MMSE: 0.013103, NMMSE: 0.018413, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:36:12] Epoch 180/200, Loss: 14.513947, Train_MMSE: 0.013105, NMMSE: 0.018412, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:36:48] Epoch 181/200, Loss: 14.539612, Train_MMSE: 0.013104, NMMSE: 0.018411, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:37:24] Epoch 182/200, Loss: 14.542860, Train_MMSE: 0.013102, NMMSE: 0.018412, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:38:00] Epoch 183/200, Loss: 14.497491, Train_MMSE: 0.013106, NMMSE: 0.018413, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:38:36] Epoch 184/200, Loss: 14.474936, Train_MMSE: 0.013103, NMMSE: 0.018412, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:39:14] Epoch 185/200, Loss: 14.516115, Train_MMSE: 0.013106, NMMSE: 0.018415, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:39:50] Epoch 186/200, Loss: 14.598994, Train_MMSE: 0.013103, NMMSE: 0.018415, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:40:27] Epoch 187/200, Loss: 14.441138, Train_MMSE: 0.013107, NMMSE: 0.018415, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:41:03] Epoch 188/200, Loss: 14.482710, Train_MMSE: 0.013111, NMMSE: 0.018415, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:41:39] Epoch 189/200, Loss: 14.555264, Train_MMSE: 0.013103, NMMSE: 0.018416, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:42:15] Epoch 190/200, Loss: 14.518926, Train_MMSE: 0.013103, NMMSE: 0.018417, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:42:50] Epoch 191/200, Loss: 14.535416, Train_MMSE: 0.013106, NMMSE: 0.018418, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:43:26] Epoch 192/200, Loss: 14.507601, Train_MMSE: 0.013101, NMMSE: 0.018416, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:44:01] Epoch 193/200, Loss: 14.531058, Train_MMSE: 0.013097, NMMSE: 0.018419, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:44:31] Epoch 194/200, Loss: 14.584366, Train_MMSE: 0.013104, NMMSE: 0.018419, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:45:02] Epoch 195/200, Loss: 14.494985, Train_MMSE: 0.013101, NMMSE: 0.018421, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:45:32] Epoch 196/200, Loss: 14.553654, Train_MMSE: 0.013103, NMMSE: 0.01842, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:46:02] Epoch 197/200, Loss: 14.489090, Train_MMSE: 0.013098, NMMSE: 0.018419, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:46:32] Epoch 198/200, Loss: 14.508677, Train_MMSE: 0.013105, NMMSE: 0.018422, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:47:01] Epoch 199/200, Loss: 14.453633, Train_MMSE: 0.013102, NMMSE: 0.018421, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:47:23] Epoch 200/200, Loss: 14.522672, Train_MMSE: 0.0131, NMMSE: 0.018419, LS_NMSE: 0.016708, Lr: 1.0000000000000002e-06
