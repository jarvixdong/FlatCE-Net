Train.py PID: 38035

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.013322863042657928
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 1024, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S10_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S10_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/230225/CDRN_B3D18C64_test_Dataset_dB-10_N36_K4_L4_S10_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 50}},
             'optimizer': {'name': 'SGD',
                           'params': {'lr': 0.01,
                                      'momentum': 0.9,
                                      'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0.9
    nesterov: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f51685a84d0>
loss function:: L1Loss()
[2025-02-22 22:22:46] Epoch 1/200, Loss: 16.843994, Train_MMSE: 0.016422, NMMSE: 0.016692, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:23:39] Epoch 2/200, Loss: 16.758423, Train_MMSE: 0.016371, NMMSE: 0.016611, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:24:33] Epoch 3/200, Loss: 16.780544, Train_MMSE: 0.016291, NMMSE: 0.01653, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:25:28] Epoch 4/200, Loss: 16.711666, Train_MMSE: 0.016214, NMMSE: 0.016477, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:26:26] Epoch 5/200, Loss: 16.659815, Train_MMSE: 0.016154, NMMSE: 0.016445, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:27:22] Epoch 6/200, Loss: 16.632658, Train_MMSE: 0.016109, NMMSE: 0.016424, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:28:17] Epoch 7/200, Loss: 16.612579, Train_MMSE: 0.016075, NMMSE: 0.016409, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:29:15] Epoch 8/200, Loss: 16.651566, Train_MMSE: 0.01603, NMMSE: 0.0164, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:30:08] Epoch 9/200, Loss: 16.594498, Train_MMSE: 0.016005, NMMSE: 0.016397, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:31:02] Epoch 10/200, Loss: 16.547520, Train_MMSE: 0.015975, NMMSE: 0.016397, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:31:56] Epoch 11/200, Loss: 16.580990, Train_MMSE: 0.015944, NMMSE: 0.016397, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:32:50] Epoch 12/200, Loss: 16.500601, Train_MMSE: 0.015918, NMMSE: 0.016398, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:33:43] Epoch 13/200, Loss: 16.550226, Train_MMSE: 0.015894, NMMSE: 0.016412, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:34:36] Epoch 14/200, Loss: 16.456570, Train_MMSE: 0.01587, NMMSE: 0.01641, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:35:32] Epoch 15/200, Loss: 16.535593, Train_MMSE: 0.015853, NMMSE: 0.016409, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:36:28] Epoch 16/200, Loss: 16.460640, Train_MMSE: 0.015825, NMMSE: 0.016414, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:37:20] Epoch 17/200, Loss: 16.485653, Train_MMSE: 0.015806, NMMSE: 0.01642, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:38:13] Epoch 18/200, Loss: 16.431715, Train_MMSE: 0.015789, NMMSE: 0.016431, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:39:07] Epoch 19/200, Loss: 16.489010, Train_MMSE: 0.015765, NMMSE: 0.01644, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:40:01] Epoch 20/200, Loss: 16.454493, Train_MMSE: 0.015747, NMMSE: 0.016464, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:40:54] Epoch 21/200, Loss: 16.343058, Train_MMSE: 0.015723, NMMSE: 0.016458, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:41:48] Epoch 22/200, Loss: 16.418207, Train_MMSE: 0.015707, NMMSE: 0.016469, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:42:41] Epoch 23/200, Loss: 16.379295, Train_MMSE: 0.015682, NMMSE: 0.016481, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:43:33] Epoch 24/200, Loss: 16.337326, Train_MMSE: 0.015665, NMMSE: 0.016478, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:44:25] Epoch 25/200, Loss: 16.414690, Train_MMSE: 0.015644, NMMSE: 0.016484, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:45:16] Epoch 26/200, Loss: 16.435617, Train_MMSE: 0.015628, NMMSE: 0.016513, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:46:07] Epoch 27/200, Loss: 16.364244, Train_MMSE: 0.015602, NMMSE: 0.016528, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:46:57] Epoch 28/200, Loss: 16.338388, Train_MMSE: 0.015587, NMMSE: 0.016548, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:47:49] Epoch 29/200, Loss: 16.306259, Train_MMSE: 0.01557, NMMSE: 0.016522, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:48:43] Epoch 30/200, Loss: 16.358795, Train_MMSE: 0.015547, NMMSE: 0.016541, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:49:37] Epoch 31/200, Loss: 16.357706, Train_MMSE: 0.015529, NMMSE: 0.016538, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:50:31] Epoch 32/200, Loss: 16.358337, Train_MMSE: 0.015504, NMMSE: 0.01657, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:51:25] Epoch 33/200, Loss: 16.414024, Train_MMSE: 0.015488, NMMSE: 0.016548, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:52:20] Epoch 34/200, Loss: 16.284506, Train_MMSE: 0.015469, NMMSE: 0.016567, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:53:14] Epoch 35/200, Loss: 16.278126, Train_MMSE: 0.015453, NMMSE: 0.016555, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:54:09] Epoch 36/200, Loss: 16.300045, Train_MMSE: 0.015435, NMMSE: 0.016571, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:55:03] Epoch 37/200, Loss: 16.240709, Train_MMSE: 0.015408, NMMSE: 0.016573, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:55:58] Epoch 38/200, Loss: 16.264757, Train_MMSE: 0.015388, NMMSE: 0.016594, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:56:52] Epoch 39/200, Loss: 16.264627, Train_MMSE: 0.015371, NMMSE: 0.0166, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:57:47] Epoch 40/200, Loss: 16.253733, Train_MMSE: 0.015354, NMMSE: 0.016619, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:58:42] Epoch 41/200, Loss: 16.223999, Train_MMSE: 0.015333, NMMSE: 0.016651, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 22:59:36] Epoch 42/200, Loss: 16.212301, Train_MMSE: 0.015311, NMMSE: 0.016618, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:00:31] Epoch 43/200, Loss: 16.169168, Train_MMSE: 0.015286, NMMSE: 0.016634, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:01:26] Epoch 44/200, Loss: 16.260365, Train_MMSE: 0.015269, NMMSE: 0.01667, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:02:20] Epoch 45/200, Loss: 16.126036, Train_MMSE: 0.015248, NMMSE: 0.016623, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:03:14] Epoch 46/200, Loss: 16.254183, Train_MMSE: 0.015226, NMMSE: 0.016651, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:04:08] Epoch 47/200, Loss: 16.176947, Train_MMSE: 0.015208, NMMSE: 0.016675, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:05:02] Epoch 48/200, Loss: 16.216791, Train_MMSE: 0.015188, NMMSE: 0.016677, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:05:57] Epoch 49/200, Loss: 16.122755, Train_MMSE: 0.01517, NMMSE: 0.016701, LS_NMSE: 0.016708, Lr: 0.01
[2025-02-22 23:06:52] Epoch 50/200, Loss: 16.127909, Train_MMSE: 0.015152, NMMSE: 0.016661, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:07:47] Epoch 51/200, Loss: 15.809694, Train_MMSE: 0.014807, NMMSE: 0.01678, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:08:39] Epoch 52/200, Loss: 15.707685, Train_MMSE: 0.014615, NMMSE: 0.016865, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:09:28] Epoch 53/200, Loss: 15.700879, Train_MMSE: 0.014529, NMMSE: 0.016938, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:10:18] Epoch 54/200, Loss: 15.560157, Train_MMSE: 0.014463, NMMSE: 0.017002, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:11:07] Epoch 55/200, Loss: 15.543508, Train_MMSE: 0.014408, NMMSE: 0.017035, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:11:56] Epoch 56/200, Loss: 15.544868, Train_MMSE: 0.014362, NMMSE: 0.017077, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:12:45] Epoch 57/200, Loss: 15.483823, Train_MMSE: 0.014321, NMMSE: 0.017127, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:13:33] Epoch 58/200, Loss: 15.491925, Train_MMSE: 0.014287, NMMSE: 0.017157, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:14:22] Epoch 59/200, Loss: 15.457713, Train_MMSE: 0.01425, NMMSE: 0.017213, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:15:11] Epoch 60/200, Loss: 15.530027, Train_MMSE: 0.014218, NMMSE: 0.017247, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:15:59] Epoch 61/200, Loss: 15.377954, Train_MMSE: 0.014188, NMMSE: 0.017284, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:16:48] Epoch 62/200, Loss: 15.333849, Train_MMSE: 0.01416, NMMSE: 0.017284, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:17:37] Epoch 63/200, Loss: 15.362126, Train_MMSE: 0.014131, NMMSE: 0.017345, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:18:24] Epoch 64/200, Loss: 15.379674, Train_MMSE: 0.014111, NMMSE: 0.017362, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:19:07] Epoch 65/200, Loss: 15.393426, Train_MMSE: 0.014088, NMMSE: 0.017407, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:19:51] Epoch 66/200, Loss: 15.229811, Train_MMSE: 0.014066, NMMSE: 0.017438, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:20:35] Epoch 67/200, Loss: 15.356168, Train_MMSE: 0.014048, NMMSE: 0.017484, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:21:18] Epoch 68/200, Loss: 15.242536, Train_MMSE: 0.014022, NMMSE: 0.017507, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:22:02] Epoch 69/200, Loss: 15.323917, Train_MMSE: 0.014002, NMMSE: 0.01752, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:22:46] Epoch 70/200, Loss: 15.255159, Train_MMSE: 0.01398, NMMSE: 0.017547, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:23:30] Epoch 71/200, Loss: 15.206669, Train_MMSE: 0.013967, NMMSE: 0.017564, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:24:13] Epoch 72/200, Loss: 15.320389, Train_MMSE: 0.013948, NMMSE: 0.017593, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:24:57] Epoch 73/200, Loss: 15.235713, Train_MMSE: 0.013932, NMMSE: 0.017595, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:25:41] Epoch 74/200, Loss: 15.255382, Train_MMSE: 0.013912, NMMSE: 0.017683, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:26:24] Epoch 75/200, Loss: 15.193078, Train_MMSE: 0.013898, NMMSE: 0.01767, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:27:07] Epoch 76/200, Loss: 15.150576, Train_MMSE: 0.013884, NMMSE: 0.017713, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:27:50] Epoch 77/200, Loss: 15.182057, Train_MMSE: 0.013868, NMMSE: 0.017732, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:28:34] Epoch 78/200, Loss: 15.116904, Train_MMSE: 0.013856, NMMSE: 0.017739, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:29:17] Epoch 79/200, Loss: 15.121060, Train_MMSE: 0.013841, NMMSE: 0.017758, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:30:01] Epoch 80/200, Loss: 15.195312, Train_MMSE: 0.013825, NMMSE: 0.017737, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:30:44] Epoch 81/200, Loss: 15.066459, Train_MMSE: 0.013813, NMMSE: 0.017803, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:31:27] Epoch 82/200, Loss: 15.097920, Train_MMSE: 0.013801, NMMSE: 0.017808, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:32:10] Epoch 83/200, Loss: 15.178331, Train_MMSE: 0.013785, NMMSE: 0.017806, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:32:53] Epoch 84/200, Loss: 15.124586, Train_MMSE: 0.013774, NMMSE: 0.017832, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:33:37] Epoch 85/200, Loss: 15.075396, Train_MMSE: 0.013762, NMMSE: 0.017879, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:34:20] Epoch 86/200, Loss: 15.134620, Train_MMSE: 0.013751, NMMSE: 0.017867, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:35:04] Epoch 87/200, Loss: 15.018677, Train_MMSE: 0.013739, NMMSE: 0.017918, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:35:47] Epoch 88/200, Loss: 15.066668, Train_MMSE: 0.013729, NMMSE: 0.017883, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:36:30] Epoch 89/200, Loss: 15.129227, Train_MMSE: 0.013718, NMMSE: 0.01793, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:37:13] Epoch 90/200, Loss: 15.178063, Train_MMSE: 0.013708, NMMSE: 0.017961, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:37:56] Epoch 91/200, Loss: 15.058088, Train_MMSE: 0.013695, NMMSE: 0.017971, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:38:39] Epoch 92/200, Loss: 15.114962, Train_MMSE: 0.013687, NMMSE: 0.017967, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:39:22] Epoch 93/200, Loss: 15.062537, Train_MMSE: 0.013677, NMMSE: 0.018013, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:40:05] Epoch 94/200, Loss: 15.151723, Train_MMSE: 0.01367, NMMSE: 0.017981, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:40:48] Epoch 95/200, Loss: 14.974677, Train_MMSE: 0.013661, NMMSE: 0.018005, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:41:30] Epoch 96/200, Loss: 15.048425, Train_MMSE: 0.01365, NMMSE: 0.018018, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:42:14] Epoch 97/200, Loss: 15.084612, Train_MMSE: 0.01364, NMMSE: 0.01802, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:42:57] Epoch 98/200, Loss: 15.019974, Train_MMSE: 0.013631, NMMSE: 0.018068, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:43:39] Epoch 99/200, Loss: 15.010971, Train_MMSE: 0.013624, NMMSE: 0.018075, LS_NMSE: 0.016708, Lr: 0.001
[2025-02-22 23:44:22] Epoch 100/200, Loss: 15.020109, Train_MMSE: 0.013611, NMMSE: 0.018114, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:45:05] Epoch 101/200, Loss: 14.843562, Train_MMSE: 0.013401, NMMSE: 0.018119, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:45:47] Epoch 102/200, Loss: 14.651476, Train_MMSE: 0.013347, NMMSE: 0.018138, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:46:30] Epoch 103/200, Loss: 14.734290, Train_MMSE: 0.013328, NMMSE: 0.018164, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:47:13] Epoch 104/200, Loss: 14.767861, Train_MMSE: 0.013314, NMMSE: 0.018166, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:47:56] Epoch 105/200, Loss: 14.662808, Train_MMSE: 0.013309, NMMSE: 0.018181, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:48:39] Epoch 106/200, Loss: 14.726588, Train_MMSE: 0.013305, NMMSE: 0.018197, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:49:22] Epoch 107/200, Loss: 14.615086, Train_MMSE: 0.013301, NMMSE: 0.018205, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:50:05] Epoch 108/200, Loss: 14.706811, Train_MMSE: 0.013297, NMMSE: 0.018201, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:50:48] Epoch 109/200, Loss: 14.649996, Train_MMSE: 0.013291, NMMSE: 0.018219, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:51:31] Epoch 110/200, Loss: 14.713859, Train_MMSE: 0.013285, NMMSE: 0.018227, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:52:14] Epoch 111/200, Loss: 14.765191, Train_MMSE: 0.013286, NMMSE: 0.018226, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:52:58] Epoch 112/200, Loss: 14.591117, Train_MMSE: 0.01328, NMMSE: 0.018235, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:53:41] Epoch 113/200, Loss: 14.680371, Train_MMSE: 0.013281, NMMSE: 0.018244, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:54:24] Epoch 114/200, Loss: 14.620845, Train_MMSE: 0.013275, NMMSE: 0.018246, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:55:08] Epoch 115/200, Loss: 14.797465, Train_MMSE: 0.013273, NMMSE: 0.018263, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:55:50] Epoch 116/200, Loss: 14.724910, Train_MMSE: 0.01327, NMMSE: 0.018273, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:56:34] Epoch 117/200, Loss: 14.711576, Train_MMSE: 0.013267, NMMSE: 0.018276, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:57:18] Epoch 118/200, Loss: 14.667699, Train_MMSE: 0.013265, NMMSE: 0.018283, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:58:01] Epoch 119/200, Loss: 14.636215, Train_MMSE: 0.013264, NMMSE: 0.018281, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:58:44] Epoch 120/200, Loss: 14.598101, Train_MMSE: 0.013263, NMMSE: 0.018285, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-22 23:59:27] Epoch 121/200, Loss: 14.647110, Train_MMSE: 0.013262, NMMSE: 0.018289, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:00:10] Epoch 122/200, Loss: 14.686598, Train_MMSE: 0.013256, NMMSE: 0.018297, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:00:52] Epoch 123/200, Loss: 14.629154, Train_MMSE: 0.013259, NMMSE: 0.018299, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:01:35] Epoch 124/200, Loss: 14.675378, Train_MMSE: 0.013249, NMMSE: 0.018312, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:02:18] Epoch 125/200, Loss: 14.651881, Train_MMSE: 0.013254, NMMSE: 0.018315, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:03:02] Epoch 126/200, Loss: 14.691051, Train_MMSE: 0.013251, NMMSE: 0.018313, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:03:45] Epoch 127/200, Loss: 14.641900, Train_MMSE: 0.013246, NMMSE: 0.018321, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:04:28] Epoch 128/200, Loss: 14.623507, Train_MMSE: 0.013249, NMMSE: 0.018328, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:05:11] Epoch 129/200, Loss: 14.634092, Train_MMSE: 0.013243, NMMSE: 0.018336, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:05:55] Epoch 130/200, Loss: 14.625366, Train_MMSE: 0.01324, NMMSE: 0.01834, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:06:37] Epoch 131/200, Loss: 14.672775, Train_MMSE: 0.013239, NMMSE: 0.01835, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:07:20] Epoch 132/200, Loss: 14.681607, Train_MMSE: 0.013239, NMMSE: 0.018342, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:07:59] Epoch 133/200, Loss: 14.638184, Train_MMSE: 0.013233, NMMSE: 0.018346, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:08:35] Epoch 134/200, Loss: 14.658796, Train_MMSE: 0.013233, NMMSE: 0.018356, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:09:09] Epoch 135/200, Loss: 14.661146, Train_MMSE: 0.013235, NMMSE: 0.018356, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:09:45] Epoch 136/200, Loss: 14.633406, Train_MMSE: 0.013231, NMMSE: 0.018355, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:10:19] Epoch 137/200, Loss: 14.665806, Train_MMSE: 0.013232, NMMSE: 0.018365, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:10:54] Epoch 138/200, Loss: 14.630229, Train_MMSE: 0.013228, NMMSE: 0.018365, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:11:30] Epoch 139/200, Loss: 14.614774, Train_MMSE: 0.013227, NMMSE: 0.018376, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:12:05] Epoch 140/200, Loss: 14.616296, Train_MMSE: 0.013221, NMMSE: 0.01838, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:12:41] Epoch 141/200, Loss: 14.618011, Train_MMSE: 0.013228, NMMSE: 0.018378, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:13:16] Epoch 142/200, Loss: 14.642751, Train_MMSE: 0.013222, NMMSE: 0.018382, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:13:51] Epoch 143/200, Loss: 14.599647, Train_MMSE: 0.013219, NMMSE: 0.018397, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:14:26] Epoch 144/200, Loss: 14.598772, Train_MMSE: 0.013222, NMMSE: 0.018386, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:15:02] Epoch 145/200, Loss: 14.573744, Train_MMSE: 0.013219, NMMSE: 0.018391, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:15:37] Epoch 146/200, Loss: 14.585050, Train_MMSE: 0.013217, NMMSE: 0.018396, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:16:13] Epoch 147/200, Loss: 14.586711, Train_MMSE: 0.013214, NMMSE: 0.018402, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:16:48] Epoch 148/200, Loss: 14.605096, Train_MMSE: 0.013211, NMMSE: 0.018409, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:17:24] Epoch 149/200, Loss: 14.617781, Train_MMSE: 0.013209, NMMSE: 0.01841, LS_NMSE: 0.016708, Lr: 0.0001
[2025-02-23 00:17:59] Epoch 150/200, Loss: 14.653714, Train_MMSE: 0.013208, NMMSE: 0.018414, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:18:35] Epoch 151/200, Loss: 14.506740, Train_MMSE: 0.013182, NMMSE: 0.01842, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:19:11] Epoch 152/200, Loss: 14.547029, Train_MMSE: 0.013178, NMMSE: 0.018419, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:19:46] Epoch 153/200, Loss: 14.603590, Train_MMSE: 0.013176, NMMSE: 0.01842, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:20:22] Epoch 154/200, Loss: 14.563919, Train_MMSE: 0.013177, NMMSE: 0.018421, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:20:57] Epoch 155/200, Loss: 14.531610, Train_MMSE: 0.013173, NMMSE: 0.018422, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:21:33] Epoch 156/200, Loss: 14.613353, Train_MMSE: 0.013178, NMMSE: 0.018423, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:22:09] Epoch 157/200, Loss: 14.550932, Train_MMSE: 0.013177, NMMSE: 0.018424, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:22:44] Epoch 158/200, Loss: 14.682012, Train_MMSE: 0.013173, NMMSE: 0.018424, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:23:19] Epoch 159/200, Loss: 14.581423, Train_MMSE: 0.013174, NMMSE: 0.018423, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:23:55] Epoch 160/200, Loss: 14.571898, Train_MMSE: 0.013174, NMMSE: 0.018427, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:24:31] Epoch 161/200, Loss: 14.536427, Train_MMSE: 0.01317, NMMSE: 0.018425, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:25:06] Epoch 162/200, Loss: 14.509255, Train_MMSE: 0.013169, NMMSE: 0.018427, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:25:42] Epoch 163/200, Loss: 14.505757, Train_MMSE: 0.013171, NMMSE: 0.018425, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:26:18] Epoch 164/200, Loss: 14.582794, Train_MMSE: 0.013176, NMMSE: 0.018426, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:26:54] Epoch 165/200, Loss: 14.533437, Train_MMSE: 0.013174, NMMSE: 0.018427, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:27:30] Epoch 166/200, Loss: 14.510345, Train_MMSE: 0.013172, NMMSE: 0.018428, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:28:06] Epoch 167/200, Loss: 14.541404, Train_MMSE: 0.013173, NMMSE: 0.018428, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:28:42] Epoch 168/200, Loss: 14.598148, Train_MMSE: 0.013169, NMMSE: 0.018426, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:29:17] Epoch 169/200, Loss: 14.538452, Train_MMSE: 0.013173, NMMSE: 0.018429, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:29:53] Epoch 170/200, Loss: 14.667742, Train_MMSE: 0.013172, NMMSE: 0.018429, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:30:28] Epoch 171/200, Loss: 14.615278, Train_MMSE: 0.013175, NMMSE: 0.01843, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:31:03] Epoch 172/200, Loss: 14.569429, Train_MMSE: 0.013171, NMMSE: 0.018431, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:31:37] Epoch 173/200, Loss: 14.548662, Train_MMSE: 0.013165, NMMSE: 0.01843, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:32:12] Epoch 174/200, Loss: 14.599426, Train_MMSE: 0.013174, NMMSE: 0.018433, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:32:47] Epoch 175/200, Loss: 14.601820, Train_MMSE: 0.01317, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:33:22] Epoch 176/200, Loss: 14.497297, Train_MMSE: 0.01317, NMMSE: 0.018431, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:33:58] Epoch 177/200, Loss: 14.595635, Train_MMSE: 0.013172, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:34:33] Epoch 178/200, Loss: 14.547372, Train_MMSE: 0.013172, NMMSE: 0.018432, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:35:08] Epoch 179/200, Loss: 14.584956, Train_MMSE: 0.013168, NMMSE: 0.018432, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:35:43] Epoch 180/200, Loss: 14.566537, Train_MMSE: 0.013169, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:36:18] Epoch 181/200, Loss: 14.557593, Train_MMSE: 0.013169, NMMSE: 0.018432, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:36:53] Epoch 182/200, Loss: 14.587018, Train_MMSE: 0.013169, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:37:28] Epoch 183/200, Loss: 14.551291, Train_MMSE: 0.013169, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:38:03] Epoch 184/200, Loss: 14.549137, Train_MMSE: 0.013169, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:38:38] Epoch 185/200, Loss: 14.614853, Train_MMSE: 0.01317, NMMSE: 0.018436, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:39:13] Epoch 186/200, Loss: 14.541362, Train_MMSE: 0.013168, NMMSE: 0.018435, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:39:48] Epoch 187/200, Loss: 14.556409, Train_MMSE: 0.01317, NMMSE: 0.018438, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:40:24] Epoch 188/200, Loss: 14.557631, Train_MMSE: 0.013169, NMMSE: 0.018441, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:40:59] Epoch 189/200, Loss: 14.560549, Train_MMSE: 0.013165, NMMSE: 0.01844, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:41:34] Epoch 190/200, Loss: 14.431560, Train_MMSE: 0.013166, NMMSE: 0.018441, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:42:09] Epoch 191/200, Loss: 14.590726, Train_MMSE: 0.013165, NMMSE: 0.018443, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:42:45] Epoch 192/200, Loss: 14.615336, Train_MMSE: 0.01317, NMMSE: 0.018441, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:43:20] Epoch 193/200, Loss: 14.520444, Train_MMSE: 0.013166, NMMSE: 0.01844, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:43:55] Epoch 194/200, Loss: 14.495163, Train_MMSE: 0.013169, NMMSE: 0.018443, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:44:26] Epoch 195/200, Loss: 14.528907, Train_MMSE: 0.013163, NMMSE: 0.018439, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:44:57] Epoch 196/200, Loss: 14.542365, Train_MMSE: 0.013165, NMMSE: 0.01844, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:45:26] Epoch 197/200, Loss: 14.608958, Train_MMSE: 0.013166, NMMSE: 0.018442, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:45:56] Epoch 198/200, Loss: 14.616819, Train_MMSE: 0.013168, NMMSE: 0.018439, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:46:25] Epoch 199/200, Loss: 14.571004, Train_MMSE: 0.013168, NMMSE: 0.018439, LS_NMSE: 0.016708, Lr: 1e-05
[2025-02-23 00:46:55] Epoch 200/200, Loss: 14.514852, Train_MMSE: 0.013166, NMMSE: 0.018444, LS_NMSE: 0.016708, Lr: 1.0000000000000002e-06
