Train.py PID: 15467

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.023510711085663393
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S11_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S11_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S11_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f01725e7aa0>
loss function:: SmoothL1Loss()
[2025-02-24 15:05:35] Epoch 1/240, Loss: 20.947155, Train_MMSE: 0.02718, NMMSE: 0.028551, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:07:57] Epoch 2/240, Loss: 20.907925, Train_MMSE: 0.026421, NMMSE: 0.027943, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:10:23] Epoch 3/240, Loss: 20.629604, Train_MMSE: 0.02582, NMMSE: 0.027375, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:12:49] Epoch 4/240, Loss: 20.386900, Train_MMSE: 0.025348, NMMSE: 0.02696, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:15:14] Epoch 5/240, Loss: 20.296083, Train_MMSE: 0.025053, NMMSE: 0.02686, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:17:34] Epoch 6/240, Loss: 20.217014, Train_MMSE: 0.024865, NMMSE: 0.026688, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:19:58] Epoch 7/240, Loss: 20.154263, Train_MMSE: 0.024758, NMMSE: 0.026609, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:22:22] Epoch 8/240, Loss: 20.164728, Train_MMSE: 0.024687, NMMSE: 0.026449, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:24:46] Epoch 9/240, Loss: 20.208519, Train_MMSE: 0.024638, NMMSE: 0.026395, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:27:13] Epoch 10/240, Loss: 20.136972, Train_MMSE: 0.024579, NMMSE: 0.026346, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:29:40] Epoch 11/240, Loss: 20.014364, Train_MMSE: 0.02454, NMMSE: 0.026339, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:32:05] Epoch 12/240, Loss: 20.135784, Train_MMSE: 0.024503, NMMSE: 0.026379, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:34:30] Epoch 13/240, Loss: 20.142668, Train_MMSE: 0.024479, NMMSE: 0.02631, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:36:55] Epoch 14/240, Loss: 20.111794, Train_MMSE: 0.024446, NMMSE: 0.026316, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:39:21] Epoch 15/240, Loss: 20.018339, Train_MMSE: 0.024421, NMMSE: 0.02626, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:41:42] Epoch 16/240, Loss: 20.063622, Train_MMSE: 0.024399, NMMSE: 0.026297, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:44:10] Epoch 17/240, Loss: 20.044731, Train_MMSE: 0.024378, NMMSE: 0.026197, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:46:31] Epoch 18/240, Loss: 20.033566, Train_MMSE: 0.024361, NMMSE: 0.026301, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:48:54] Epoch 19/240, Loss: 19.924171, Train_MMSE: 0.024348, NMMSE: 0.026174, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:51:18] Epoch 20/240, Loss: 20.025116, Train_MMSE: 0.024334, NMMSE: 0.026119, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:53:40] Epoch 21/240, Loss: 20.048990, Train_MMSE: 0.02432, NMMSE: 0.026145, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:56:03] Epoch 22/240, Loss: 19.914011, Train_MMSE: 0.024308, NMMSE: 0.02616, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:58:27] Epoch 23/240, Loss: 20.091261, Train_MMSE: 0.024301, NMMSE: 0.026143, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:00:53] Epoch 24/240, Loss: 20.136164, Train_MMSE: 0.024298, NMMSE: 0.026085, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:03:15] Epoch 25/240, Loss: 20.033480, Train_MMSE: 0.024284, NMMSE: 0.026126, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:05:38] Epoch 26/240, Loss: 20.031628, Train_MMSE: 0.024273, NMMSE: 0.026121, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:08:01] Epoch 27/240, Loss: 19.969536, Train_MMSE: 0.02427, NMMSE: 0.026077, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:10:23] Epoch 28/240, Loss: 19.848576, Train_MMSE: 0.024265, NMMSE: 0.026067, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:12:49] Epoch 29/240, Loss: 20.130770, Train_MMSE: 0.024258, NMMSE: 0.026068, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:15:12] Epoch 30/240, Loss: 19.985727, Train_MMSE: 0.024251, NMMSE: 0.026029, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:17:35] Epoch 31/240, Loss: 19.985563, Train_MMSE: 0.024242, NMMSE: 0.026031, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:19:57] Epoch 32/240, Loss: 19.961027, Train_MMSE: 0.024244, NMMSE: 0.026071, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:22:20] Epoch 33/240, Loss: 19.936968, Train_MMSE: 0.024237, NMMSE: 0.026101, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:24:44] Epoch 34/240, Loss: 19.965206, Train_MMSE: 0.024236, NMMSE: 0.026094, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:27:08] Epoch 35/240, Loss: 19.908403, Train_MMSE: 0.024231, NMMSE: 0.026073, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:29:36] Epoch 36/240, Loss: 20.081564, Train_MMSE: 0.024227, NMMSE: 0.026098, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:31:58] Epoch 37/240, Loss: 19.887293, Train_MMSE: 0.024224, NMMSE: 0.026053, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:34:23] Epoch 38/240, Loss: 20.017338, Train_MMSE: 0.024221, NMMSE: 0.026019, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:36:48] Epoch 39/240, Loss: 19.896576, Train_MMSE: 0.024215, NMMSE: 0.026065, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:39:13] Epoch 40/240, Loss: 19.878248, Train_MMSE: 0.024214, NMMSE: 0.026018, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:41:33] Epoch 41/240, Loss: 20.144827, Train_MMSE: 0.024213, NMMSE: 0.026074, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:43:57] Epoch 42/240, Loss: 20.036118, Train_MMSE: 0.024211, NMMSE: 0.025968, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:46:20] Epoch 43/240, Loss: 19.982542, Train_MMSE: 0.024203, NMMSE: 0.025973, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:48:44] Epoch 44/240, Loss: 20.032274, Train_MMSE: 0.024202, NMMSE: 0.026105, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:51:09] Epoch 45/240, Loss: 19.894888, Train_MMSE: 0.024202, NMMSE: 0.026007, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:53:33] Epoch 46/240, Loss: 19.790854, Train_MMSE: 0.024199, NMMSE: 0.025962, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:55:56] Epoch 47/240, Loss: 20.024929, Train_MMSE: 0.024196, NMMSE: 0.026009, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:58:22] Epoch 48/240, Loss: 19.938049, Train_MMSE: 0.024197, NMMSE: 0.025984, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:00:44] Epoch 49/240, Loss: 19.998234, Train_MMSE: 0.024191, NMMSE: 0.026047, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:03:08] Epoch 50/240, Loss: 19.937981, Train_MMSE: 0.02419, NMMSE: 0.026014, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:05:31] Epoch 51/240, Loss: 20.032433, Train_MMSE: 0.024189, NMMSE: 0.02603, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:07:54] Epoch 52/240, Loss: 19.998117, Train_MMSE: 0.024182, NMMSE: 0.025934, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:10:18] Epoch 53/240, Loss: 19.921522, Train_MMSE: 0.024185, NMMSE: 0.025971, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:12:43] Epoch 54/240, Loss: 19.930166, Train_MMSE: 0.024181, NMMSE: 0.026013, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:15:06] Epoch 55/240, Loss: 20.030701, Train_MMSE: 0.024177, NMMSE: 0.026005, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:17:31] Epoch 56/240, Loss: 19.873615, Train_MMSE: 0.024176, NMMSE: 0.02595, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:19:52] Epoch 57/240, Loss: 19.954973, Train_MMSE: 0.024176, NMMSE: 0.025959, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:22:13] Epoch 58/240, Loss: 19.991282, Train_MMSE: 0.024177, NMMSE: 0.026045, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:24:36] Epoch 59/240, Loss: 19.933376, Train_MMSE: 0.024175, NMMSE: 0.025999, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 17:27:03] Epoch 60/240, Loss: 20.092846, Train_MMSE: 0.024169, NMMSE: 0.025994, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:29:23] Epoch 61/240, Loss: 19.847631, Train_MMSE: 0.02395, NMMSE: 0.025694, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:31:46] Epoch 62/240, Loss: 19.800085, Train_MMSE: 0.02391, NMMSE: 0.025694, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:34:11] Epoch 63/240, Loss: 19.777767, Train_MMSE: 0.023899, NMMSE: 0.025685, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:36:36] Epoch 64/240, Loss: 19.777224, Train_MMSE: 0.023892, NMMSE: 0.025687, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:38:57] Epoch 65/240, Loss: 19.704376, Train_MMSE: 0.023885, NMMSE: 0.025695, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:41:21] Epoch 66/240, Loss: 19.811466, Train_MMSE: 0.02388, NMMSE: 0.025698, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:43:57] Epoch 67/240, Loss: 19.795883, Train_MMSE: 0.023873, NMMSE: 0.025699, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:47:05] Epoch 68/240, Loss: 19.695351, Train_MMSE: 0.023869, NMMSE: 0.025702, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:50:01] Epoch 69/240, Loss: 19.883787, Train_MMSE: 0.023862, NMMSE: 0.0257, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:52:45] Epoch 70/240, Loss: 19.846813, Train_MMSE: 0.023861, NMMSE: 0.025707, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:55:27] Epoch 71/240, Loss: 19.650688, Train_MMSE: 0.023857, NMMSE: 0.025723, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:58:12] Epoch 72/240, Loss: 19.727922, Train_MMSE: 0.023854, NMMSE: 0.025733, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:00:53] Epoch 73/240, Loss: 19.710819, Train_MMSE: 0.023849, NMMSE: 0.025721, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:03:24] Epoch 74/240, Loss: 19.696695, Train_MMSE: 0.023848, NMMSE: 0.025725, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:05:59] Epoch 75/240, Loss: 19.894358, Train_MMSE: 0.023843, NMMSE: 0.025725, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:08:32] Epoch 76/240, Loss: 19.931726, Train_MMSE: 0.023841, NMMSE: 0.025725, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:11:09] Epoch 77/240, Loss: 19.749828, Train_MMSE: 0.023837, NMMSE: 0.025737, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:13:48] Epoch 78/240, Loss: 19.725935, Train_MMSE: 0.023834, NMMSE: 0.025732, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:16:25] Epoch 79/240, Loss: 19.785753, Train_MMSE: 0.023831, NMMSE: 0.025735, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:18:57] Epoch 80/240, Loss: 19.727211, Train_MMSE: 0.023829, NMMSE: 0.025738, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:21:26] Epoch 81/240, Loss: 19.795771, Train_MMSE: 0.023825, NMMSE: 0.025753, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:23:57] Epoch 82/240, Loss: 19.826420, Train_MMSE: 0.023821, NMMSE: 0.025742, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:26:27] Epoch 83/240, Loss: 19.726517, Train_MMSE: 0.023822, NMMSE: 0.025737, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:28:53] Epoch 84/240, Loss: 19.790207, Train_MMSE: 0.023821, NMMSE: 0.025748, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:31:06] Epoch 85/240, Loss: 19.693193, Train_MMSE: 0.023815, NMMSE: 0.025752, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:33:16] Epoch 86/240, Loss: 19.711525, Train_MMSE: 0.023814, NMMSE: 0.025769, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:35:26] Epoch 87/240, Loss: 19.693533, Train_MMSE: 0.023814, NMMSE: 0.025769, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:37:33] Epoch 88/240, Loss: 19.753656, Train_MMSE: 0.023811, NMMSE: 0.025761, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:39:44] Epoch 89/240, Loss: 19.755808, Train_MMSE: 0.02381, NMMSE: 0.025779, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:41:53] Epoch 90/240, Loss: 19.732841, Train_MMSE: 0.023807, NMMSE: 0.025792, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:44:06] Epoch 91/240, Loss: 19.701309, Train_MMSE: 0.023805, NMMSE: 0.02577, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:46:16] Epoch 92/240, Loss: 19.654486, Train_MMSE: 0.023802, NMMSE: 0.025768, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:48:27] Epoch 93/240, Loss: 19.876360, Train_MMSE: 0.023803, NMMSE: 0.025786, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:50:38] Epoch 94/240, Loss: 19.665924, Train_MMSE: 0.0238, NMMSE: 0.025772, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:52:50] Epoch 95/240, Loss: 19.868896, Train_MMSE: 0.023797, NMMSE: 0.025785, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:55:03] Epoch 96/240, Loss: 19.694990, Train_MMSE: 0.023798, NMMSE: 0.02579, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:57:13] Epoch 97/240, Loss: 19.772905, Train_MMSE: 0.023798, NMMSE: 0.025788, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 18:59:23] Epoch 98/240, Loss: 19.696848, Train_MMSE: 0.023793, NMMSE: 0.025782, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:01:31] Epoch 99/240, Loss: 19.797558, Train_MMSE: 0.023791, NMMSE: 0.025795, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:03:41] Epoch 100/240, Loss: 19.754780, Train_MMSE: 0.023791, NMMSE: 0.025805, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:05:50] Epoch 101/240, Loss: 19.626305, Train_MMSE: 0.023791, NMMSE: 0.025803, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:08:02] Epoch 102/240, Loss: 19.771379, Train_MMSE: 0.023788, NMMSE: 0.025801, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:10:12] Epoch 103/240, Loss: 19.695681, Train_MMSE: 0.023788, NMMSE: 0.025806, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:12:21] Epoch 104/240, Loss: 19.727608, Train_MMSE: 0.023785, NMMSE: 0.025803, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:14:32] Epoch 105/240, Loss: 19.686884, Train_MMSE: 0.023784, NMMSE: 0.025806, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:16:40] Epoch 106/240, Loss: 19.804199, Train_MMSE: 0.023784, NMMSE: 0.025812, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:18:50] Epoch 107/240, Loss: 19.762032, Train_MMSE: 0.023782, NMMSE: 0.025812, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:21:00] Epoch 108/240, Loss: 19.712582, Train_MMSE: 0.023781, NMMSE: 0.025815, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:23:09] Epoch 109/240, Loss: 19.672815, Train_MMSE: 0.02378, NMMSE: 0.025812, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:25:17] Epoch 110/240, Loss: 19.778965, Train_MMSE: 0.023776, NMMSE: 0.025831, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:27:26] Epoch 111/240, Loss: 19.867105, Train_MMSE: 0.023778, NMMSE: 0.025827, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:29:36] Epoch 112/240, Loss: 19.623016, Train_MMSE: 0.023776, NMMSE: 0.025836, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:31:46] Epoch 113/240, Loss: 19.763578, Train_MMSE: 0.023774, NMMSE: 0.025844, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:33:57] Epoch 114/240, Loss: 19.845573, Train_MMSE: 0.023774, NMMSE: 0.025838, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:36:07] Epoch 115/240, Loss: 19.764565, Train_MMSE: 0.02377, NMMSE: 0.025838, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:38:17] Epoch 116/240, Loss: 19.737038, Train_MMSE: 0.023772, NMMSE: 0.02583, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:40:29] Epoch 117/240, Loss: 19.710320, Train_MMSE: 0.023771, NMMSE: 0.025834, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:42:42] Epoch 118/240, Loss: 19.769621, Train_MMSE: 0.02377, NMMSE: 0.025835, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:44:52] Epoch 119/240, Loss: 19.866959, Train_MMSE: 0.023768, NMMSE: 0.025831, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 19:47:03] Epoch 120/240, Loss: 19.643717, Train_MMSE: 0.023767, NMMSE: 0.025872, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:49:14] Epoch 121/240, Loss: 19.742825, Train_MMSE: 0.023698, NMMSE: 0.025807, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:51:26] Epoch 122/240, Loss: 19.603363, Train_MMSE: 0.023691, NMMSE: 0.025814, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:53:32] Epoch 123/240, Loss: 19.707579, Train_MMSE: 0.02369, NMMSE: 0.025818, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:55:41] Epoch 124/240, Loss: 19.677155, Train_MMSE: 0.023686, NMMSE: 0.02582, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:57:50] Epoch 125/240, Loss: 19.790100, Train_MMSE: 0.023685, NMMSE: 0.025819, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:59:59] Epoch 126/240, Loss: 19.747648, Train_MMSE: 0.023684, NMMSE: 0.025823, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:02:07] Epoch 127/240, Loss: 19.627312, Train_MMSE: 0.023684, NMMSE: 0.025826, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:04:16] Epoch 128/240, Loss: 19.653086, Train_MMSE: 0.023684, NMMSE: 0.025828, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:06:27] Epoch 129/240, Loss: 19.679066, Train_MMSE: 0.023684, NMMSE: 0.025829, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:08:37] Epoch 130/240, Loss: 19.695637, Train_MMSE: 0.023683, NMMSE: 0.025828, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:10:47] Epoch 131/240, Loss: 19.711700, Train_MMSE: 0.023682, NMMSE: 0.025829, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:12:56] Epoch 132/240, Loss: 19.633568, Train_MMSE: 0.023682, NMMSE: 0.025833, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:15:07] Epoch 133/240, Loss: 19.629230, Train_MMSE: 0.023682, NMMSE: 0.025831, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:17:18] Epoch 134/240, Loss: 19.669786, Train_MMSE: 0.023683, NMMSE: 0.025833, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:19:28] Epoch 135/240, Loss: 19.594275, Train_MMSE: 0.023682, NMMSE: 0.025838, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:21:41] Epoch 136/240, Loss: 19.671879, Train_MMSE: 0.02368, NMMSE: 0.025839, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:23:51] Epoch 137/240, Loss: 19.677896, Train_MMSE: 0.023681, NMMSE: 0.025841, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:26:02] Epoch 138/240, Loss: 19.723513, Train_MMSE: 0.023681, NMMSE: 0.025837, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:28:10] Epoch 139/240, Loss: 19.629684, Train_MMSE: 0.023679, NMMSE: 0.02584, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:30:23] Epoch 140/240, Loss: 19.712248, Train_MMSE: 0.023682, NMMSE: 0.025845, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:32:30] Epoch 141/240, Loss: 19.594894, Train_MMSE: 0.02368, NMMSE: 0.025838, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:34:43] Epoch 142/240, Loss: 19.571562, Train_MMSE: 0.023678, NMMSE: 0.02584, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:36:55] Epoch 143/240, Loss: 19.718910, Train_MMSE: 0.023679, NMMSE: 0.025839, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:39:04] Epoch 144/240, Loss: 19.691439, Train_MMSE: 0.02368, NMMSE: 0.025844, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:41:16] Epoch 145/240, Loss: 19.669685, Train_MMSE: 0.023677, NMMSE: 0.025848, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:43:25] Epoch 146/240, Loss: 19.596018, Train_MMSE: 0.023679, NMMSE: 0.025846, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:45:36] Epoch 147/240, Loss: 19.713188, Train_MMSE: 0.023677, NMMSE: 0.025847, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:47:46] Epoch 148/240, Loss: 19.628569, Train_MMSE: 0.023678, NMMSE: 0.025845, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:49:59] Epoch 149/240, Loss: 19.583399, Train_MMSE: 0.023677, NMMSE: 0.025842, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:52:10] Epoch 150/240, Loss: 19.670391, Train_MMSE: 0.023676, NMMSE: 0.025848, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:54:19] Epoch 151/240, Loss: 19.686256, Train_MMSE: 0.023674, NMMSE: 0.025848, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:56:29] Epoch 152/240, Loss: 19.664951, Train_MMSE: 0.023676, NMMSE: 0.02585, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 20:58:39] Epoch 153/240, Loss: 19.706169, Train_MMSE: 0.023678, NMMSE: 0.025846, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:00:48] Epoch 154/240, Loss: 19.639303, Train_MMSE: 0.023677, NMMSE: 0.02585, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:02:59] Epoch 155/240, Loss: 19.630903, Train_MMSE: 0.023678, NMMSE: 0.025849, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:05:09] Epoch 156/240, Loss: 19.616873, Train_MMSE: 0.023676, NMMSE: 0.025847, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:07:12] Epoch 157/240, Loss: 19.492409, Train_MMSE: 0.023676, NMMSE: 0.025851, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:09:09] Epoch 158/240, Loss: 19.723900, Train_MMSE: 0.023676, NMMSE: 0.025854, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:10:59] Epoch 159/240, Loss: 19.572138, Train_MMSE: 0.023675, NMMSE: 0.025853, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:12:55] Epoch 160/240, Loss: 19.634176, Train_MMSE: 0.023675, NMMSE: 0.025851, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:14:47] Epoch 161/240, Loss: 19.753265, Train_MMSE: 0.023675, NMMSE: 0.025853, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:16:41] Epoch 162/240, Loss: 19.624851, Train_MMSE: 0.023673, NMMSE: 0.025851, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:18:30] Epoch 163/240, Loss: 19.706600, Train_MMSE: 0.023675, NMMSE: 0.025854, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:20:23] Epoch 164/240, Loss: 19.690798, Train_MMSE: 0.023674, NMMSE: 0.02586, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:22:15] Epoch 165/240, Loss: 19.551302, Train_MMSE: 0.023674, NMMSE: 0.025855, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:23:54] Epoch 166/240, Loss: 19.683584, Train_MMSE: 0.023673, NMMSE: 0.025856, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:25:38] Epoch 167/240, Loss: 19.614895, Train_MMSE: 0.023673, NMMSE: 0.025856, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:27:18] Epoch 168/240, Loss: 19.578362, Train_MMSE: 0.023672, NMMSE: 0.02586, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:28:59] Epoch 169/240, Loss: 19.784437, Train_MMSE: 0.023675, NMMSE: 0.025861, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:30:42] Epoch 170/240, Loss: 19.612425, Train_MMSE: 0.023673, NMMSE: 0.025861, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:32:21] Epoch 171/240, Loss: 19.592346, Train_MMSE: 0.023672, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:34:02] Epoch 172/240, Loss: 19.723158, Train_MMSE: 0.023674, NMMSE: 0.025857, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:35:43] Epoch 173/240, Loss: 19.675982, Train_MMSE: 0.023672, NMMSE: 0.025861, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:37:23] Epoch 174/240, Loss: 19.619913, Train_MMSE: 0.023673, NMMSE: 0.025857, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:39:03] Epoch 175/240, Loss: 19.671982, Train_MMSE: 0.023674, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:40:44] Epoch 176/240, Loss: 19.602468, Train_MMSE: 0.023673, NMMSE: 0.02586, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:42:24] Epoch 177/240, Loss: 19.674402, Train_MMSE: 0.023672, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:44:03] Epoch 178/240, Loss: 19.677544, Train_MMSE: 0.023671, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:45:39] Epoch 179/240, Loss: 19.754984, Train_MMSE: 0.023671, NMMSE: 0.025867, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 21:47:20] Epoch 180/240, Loss: 19.698776, Train_MMSE: 0.023672, NMMSE: 0.025871, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:49:00] Epoch 181/240, Loss: 19.566595, Train_MMSE: 0.023661, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:50:38] Epoch 182/240, Loss: 19.544727, Train_MMSE: 0.023659, NMMSE: 0.025861, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:52:17] Epoch 183/240, Loss: 19.721577, Train_MMSE: 0.02366, NMMSE: 0.025861, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:54:00] Epoch 184/240, Loss: 19.671078, Train_MMSE: 0.023658, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:55:41] Epoch 185/240, Loss: 19.734816, Train_MMSE: 0.02366, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:57:19] Epoch 186/240, Loss: 19.643764, Train_MMSE: 0.023659, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 21:58:59] Epoch 187/240, Loss: 19.558846, Train_MMSE: 0.02366, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:00:40] Epoch 188/240, Loss: 19.685963, Train_MMSE: 0.023659, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:02:23] Epoch 189/240, Loss: 19.699480, Train_MMSE: 0.023661, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:04:02] Epoch 190/240, Loss: 19.623997, Train_MMSE: 0.02366, NMMSE: 0.025862, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:05:44] Epoch 191/240, Loss: 19.589401, Train_MMSE: 0.023659, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:07:26] Epoch 192/240, Loss: 19.625759, Train_MMSE: 0.023661, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:09:06] Epoch 193/240, Loss: 19.634054, Train_MMSE: 0.023659, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:10:48] Epoch 194/240, Loss: 19.657143, Train_MMSE: 0.023659, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:12:29] Epoch 195/240, Loss: 19.573400, Train_MMSE: 0.023661, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:14:09] Epoch 196/240, Loss: 19.589474, Train_MMSE: 0.023658, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:15:54] Epoch 197/240, Loss: 19.499519, Train_MMSE: 0.023661, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:17:32] Epoch 198/240, Loss: 19.733269, Train_MMSE: 0.02366, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:19:13] Epoch 199/240, Loss: 19.671366, Train_MMSE: 0.02366, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:20:54] Epoch 200/240, Loss: 19.619751, Train_MMSE: 0.023662, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:22:32] Epoch 201/240, Loss: 19.537655, Train_MMSE: 0.02366, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:24:11] Epoch 202/240, Loss: 19.558922, Train_MMSE: 0.023662, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:25:51] Epoch 203/240, Loss: 19.776333, Train_MMSE: 0.02366, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:27:29] Epoch 204/240, Loss: 19.654585, Train_MMSE: 0.023661, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:29:06] Epoch 205/240, Loss: 19.592524, Train_MMSE: 0.023661, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:30:42] Epoch 206/240, Loss: 19.725391, Train_MMSE: 0.023659, NMMSE: 0.025863, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:32:20] Epoch 207/240, Loss: 19.639414, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:34:01] Epoch 208/240, Loss: 19.724361, Train_MMSE: 0.023661, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:35:41] Epoch 209/240, Loss: 19.724501, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:37:19] Epoch 210/240, Loss: 19.651680, Train_MMSE: 0.023659, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:39:00] Epoch 211/240, Loss: 19.650002, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:40:38] Epoch 212/240, Loss: 19.566286, Train_MMSE: 0.02366, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:42:17] Epoch 213/240, Loss: 19.604855, Train_MMSE: 0.023658, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:43:55] Epoch 214/240, Loss: 19.557903, Train_MMSE: 0.023661, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:45:33] Epoch 215/240, Loss: 19.705832, Train_MMSE: 0.023659, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:47:09] Epoch 216/240, Loss: 19.710890, Train_MMSE: 0.023661, NMMSE: 0.025864, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:48:48] Epoch 217/240, Loss: 19.727495, Train_MMSE: 0.023661, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:50:28] Epoch 218/240, Loss: 19.588209, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:52:09] Epoch 219/240, Loss: 19.564026, Train_MMSE: 0.023659, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:53:45] Epoch 220/240, Loss: 19.669464, Train_MMSE: 0.023658, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:55:18] Epoch 221/240, Loss: 19.668636, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:56:37] Epoch 222/240, Loss: 19.841997, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:57:59] Epoch 223/240, Loss: 19.587532, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 22:59:24] Epoch 224/240, Loss: 19.514584, Train_MMSE: 0.02366, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:00:48] Epoch 225/240, Loss: 19.730513, Train_MMSE: 0.02366, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:02:11] Epoch 226/240, Loss: 19.579603, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:03:33] Epoch 227/240, Loss: 19.658548, Train_MMSE: 0.023659, NMMSE: 0.025865, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:04:56] Epoch 228/240, Loss: 19.679853, Train_MMSE: 0.023657, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:06:21] Epoch 229/240, Loss: 19.624195, Train_MMSE: 0.02366, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:07:44] Epoch 230/240, Loss: 19.580854, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:09:06] Epoch 231/240, Loss: 19.689657, Train_MMSE: 0.023661, NMMSE: 0.025867, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:27] Epoch 232/240, Loss: 19.599031, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:11:34] Epoch 233/240, Loss: 19.754007, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:12:35] Epoch 234/240, Loss: 19.707087, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:13:36] Epoch 235/240, Loss: 19.602306, Train_MMSE: 0.023659, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:14:36] Epoch 236/240, Loss: 19.630718, Train_MMSE: 0.023658, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:15:35] Epoch 237/240, Loss: 19.673994, Train_MMSE: 0.023659, NMMSE: 0.025867, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:16:35] Epoch 238/240, Loss: 19.657104, Train_MMSE: 0.023658, NMMSE: 0.025866, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:17:35] Epoch 239/240, Loss: 19.557920, Train_MMSE: 0.023659, NMMSE: 0.025867, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 23:18:36] Epoch 240/240, Loss: 19.501938, Train_MMSE: 0.02366, NMMSE: 0.025867, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-07
