Train.py PID: 3366

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.04451867382223317
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/v2_train_Dataset_dB-15_N36_K4_L6_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/v2_test_Dataset_dB-15_N36_K4_L6_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_v2_test_Dataset_dB-15_N36_K4_L6_S9_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f00c21d9ee0>
loss function:: SmoothL1Loss()
[2025-02-24 15:50:55] Epoch 1/240, Loss: 34.072598, Train_MMSE: 0.088189, NMMSE: 0.06727, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 15:53:02] Epoch 2/240, Loss: 32.374264, Train_MMSE: 0.065614, NMMSE: 0.06089, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 15:55:35] Epoch 3/240, Loss: 31.704224, Train_MMSE: 0.061735, NMMSE: 0.058857, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 15:58:11] Epoch 4/240, Loss: 31.488749, Train_MMSE: 0.060151, NMMSE: 0.058298, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:00:49] Epoch 5/240, Loss: 30.601175, Train_MMSE: 0.059089, NMMSE: 0.057033, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:03:18] Epoch 6/240, Loss: 30.823282, Train_MMSE: 0.05823, NMMSE: 0.056612, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:05:42] Epoch 7/240, Loss: 30.623444, Train_MMSE: 0.057595, NMMSE: 0.05607, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:08:04] Epoch 8/240, Loss: 30.725748, Train_MMSE: 0.057091, NMMSE: 0.055885, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:10:28] Epoch 9/240, Loss: 30.579119, Train_MMSE: 0.056712, NMMSE: 0.055585, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:12:52] Epoch 10/240, Loss: 30.216286, Train_MMSE: 0.056355, NMMSE: 0.054913, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:15:16] Epoch 11/240, Loss: 30.219385, Train_MMSE: 0.056131, NMMSE: 0.054924, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:17:40] Epoch 12/240, Loss: 30.338888, Train_MMSE: 0.055899, NMMSE: 0.055364, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:20:04] Epoch 13/240, Loss: 30.138332, Train_MMSE: 0.0557, NMMSE: 0.054821, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:22:29] Epoch 14/240, Loss: 30.102182, Train_MMSE: 0.055541, NMMSE: 0.054507, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:24:55] Epoch 15/240, Loss: 30.088932, Train_MMSE: 0.055344, NMMSE: 0.054503, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:27:19] Epoch 16/240, Loss: 30.212349, Train_MMSE: 0.055203, NMMSE: 0.055005, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:29:42] Epoch 17/240, Loss: 30.116693, Train_MMSE: 0.055064, NMMSE: 0.054293, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:32:06] Epoch 18/240, Loss: 30.096287, Train_MMSE: 0.054949, NMMSE: 0.05437, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:34:31] Epoch 19/240, Loss: 29.823120, Train_MMSE: 0.054834, NMMSE: 0.054266, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:36:54] Epoch 20/240, Loss: 29.829607, Train_MMSE: 0.054719, NMMSE: 0.054237, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:39:19] Epoch 21/240, Loss: 29.665768, Train_MMSE: 0.054657, NMMSE: 0.054245, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:41:43] Epoch 22/240, Loss: 29.676998, Train_MMSE: 0.054528, NMMSE: 0.054353, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:44:09] Epoch 23/240, Loss: 29.886892, Train_MMSE: 0.054487, NMMSE: 0.054052, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:46:34] Epoch 24/240, Loss: 29.653355, Train_MMSE: 0.0544, NMMSE: 0.053856, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:48:59] Epoch 25/240, Loss: 29.875017, Train_MMSE: 0.054325, NMMSE: 0.053879, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:51:24] Epoch 26/240, Loss: 29.908913, Train_MMSE: 0.054276, NMMSE: 0.053983, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:53:49] Epoch 27/240, Loss: 29.678658, Train_MMSE: 0.05418, NMMSE: 0.053558, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:56:12] Epoch 28/240, Loss: 29.906654, Train_MMSE: 0.054122, NMMSE: 0.053807, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 16:58:35] Epoch 29/240, Loss: 29.663540, Train_MMSE: 0.054065, NMMSE: 0.053662, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:00:59] Epoch 30/240, Loss: 29.921457, Train_MMSE: 0.054014, NMMSE: 0.053547, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:03:23] Epoch 31/240, Loss: 29.426666, Train_MMSE: 0.053912, NMMSE: 0.053646, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:05:49] Epoch 32/240, Loss: 29.504442, Train_MMSE: 0.053858, NMMSE: 0.053504, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:08:13] Epoch 33/240, Loss: 29.639448, Train_MMSE: 0.053814, NMMSE: 0.053649, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:10:37] Epoch 34/240, Loss: 29.596981, Train_MMSE: 0.053739, NMMSE: 0.053245, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:13:02] Epoch 35/240, Loss: 29.717981, Train_MMSE: 0.05365, NMMSE: 0.053405, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:15:25] Epoch 36/240, Loss: 29.520670, Train_MMSE: 0.053528, NMMSE: 0.053342, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:17:49] Epoch 37/240, Loss: 29.788950, Train_MMSE: 0.053424, NMMSE: 0.053025, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:20:14] Epoch 38/240, Loss: 29.597067, Train_MMSE: 0.053327, NMMSE: 0.052947, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:22:37] Epoch 39/240, Loss: 29.430969, Train_MMSE: 0.053237, NMMSE: 0.052589, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:25:02] Epoch 40/240, Loss: 29.619961, Train_MMSE: 0.053134, NMMSE: 0.052593, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:27:25] Epoch 41/240, Loss: 29.523197, Train_MMSE: 0.053024, NMMSE: 0.052424, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:29:50] Epoch 42/240, Loss: 29.543505, Train_MMSE: 0.052938, NMMSE: 0.05258, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:32:14] Epoch 43/240, Loss: 29.344704, Train_MMSE: 0.052866, NMMSE: 0.052507, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:34:39] Epoch 44/240, Loss: 29.284309, Train_MMSE: 0.052758, NMMSE: 0.052326, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:37:03] Epoch 45/240, Loss: 29.256500, Train_MMSE: 0.052681, NMMSE: 0.052239, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:39:29] Epoch 46/240, Loss: 29.153439, Train_MMSE: 0.052608, NMMSE: 0.052332, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:41:53] Epoch 47/240, Loss: 29.374508, Train_MMSE: 0.052557, NMMSE: 0.052058, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:44:19] Epoch 48/240, Loss: 29.035427, Train_MMSE: 0.05245, NMMSE: 0.052213, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:46:43] Epoch 49/240, Loss: 29.133579, Train_MMSE: 0.052407, NMMSE: 0.051927, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:49:08] Epoch 50/240, Loss: 29.302782, Train_MMSE: 0.052318, NMMSE: 0.051741, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:51:32] Epoch 51/240, Loss: 29.322086, Train_MMSE: 0.052223, NMMSE: 0.05207, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:53:56] Epoch 52/240, Loss: 28.975843, Train_MMSE: 0.052157, NMMSE: 0.051742, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:56:21] Epoch 53/240, Loss: 28.980652, Train_MMSE: 0.052059, NMMSE: 0.05196, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 17:58:46] Epoch 54/240, Loss: 29.016466, Train_MMSE: 0.051966, NMMSE: 0.051597, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:01:13] Epoch 55/240, Loss: 29.281260, Train_MMSE: 0.051862, NMMSE: 0.05156, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:03:37] Epoch 56/240, Loss: 29.023373, Train_MMSE: 0.051753, NMMSE: 0.051157, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:06:02] Epoch 57/240, Loss: 29.140175, Train_MMSE: 0.051606, NMMSE: 0.051102, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:08:26] Epoch 58/240, Loss: 28.961496, Train_MMSE: 0.051513, NMMSE: 0.05087, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:10:51] Epoch 59/240, Loss: 28.895063, Train_MMSE: 0.051413, NMMSE: 0.050679, LS_NMSE: 0.114943, Lr: 0.001
[2025-02-24 18:13:17] Epoch 60/240, Loss: 28.865221, Train_MMSE: 0.051325, NMMSE: 0.05088, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:15:43] Epoch 61/240, Loss: 28.213112, Train_MMSE: 0.049353, NMMSE: 0.049074, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:18:06] Epoch 62/240, Loss: 28.127808, Train_MMSE: 0.048941, NMMSE: 0.048997, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:20:30] Epoch 63/240, Loss: 27.876734, Train_MMSE: 0.048758, NMMSE: 0.049093, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:22:56] Epoch 64/240, Loss: 27.784723, Train_MMSE: 0.048611, NMMSE: 0.049052, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:25:20] Epoch 65/240, Loss: 28.090698, Train_MMSE: 0.048483, NMMSE: 0.0491, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:27:44] Epoch 66/240, Loss: 28.137764, Train_MMSE: 0.048357, NMMSE: 0.049156, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:30:10] Epoch 67/240, Loss: 28.220985, Train_MMSE: 0.048248, NMMSE: 0.049214, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:32:34] Epoch 68/240, Loss: 27.917034, Train_MMSE: 0.048145, NMMSE: 0.04923, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:34:58] Epoch 69/240, Loss: 27.973763, Train_MMSE: 0.048032, NMMSE: 0.049358, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:37:24] Epoch 70/240, Loss: 27.936205, Train_MMSE: 0.047931, NMMSE: 0.049369, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:39:49] Epoch 71/240, Loss: 27.802526, Train_MMSE: 0.047841, NMMSE: 0.049349, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:42:15] Epoch 72/240, Loss: 27.772188, Train_MMSE: 0.047749, NMMSE: 0.049519, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:44:39] Epoch 73/240, Loss: 27.512548, Train_MMSE: 0.047658, NMMSE: 0.04952, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:47:05] Epoch 74/240, Loss: 27.713675, Train_MMSE: 0.047569, NMMSE: 0.049631, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:49:29] Epoch 75/240, Loss: 27.833906, Train_MMSE: 0.047491, NMMSE: 0.049605, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:51:54] Epoch 76/240, Loss: 27.892405, Train_MMSE: 0.0474, NMMSE: 0.049668, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:54:15] Epoch 77/240, Loss: 27.520796, Train_MMSE: 0.047321, NMMSE: 0.049749, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:56:39] Epoch 78/240, Loss: 27.709986, Train_MMSE: 0.047234, NMMSE: 0.049817, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 18:59:04] Epoch 79/240, Loss: 27.996378, Train_MMSE: 0.047161, NMMSE: 0.049833, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:01:27] Epoch 80/240, Loss: 27.648895, Train_MMSE: 0.04708, NMMSE: 0.049931, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:03:51] Epoch 81/240, Loss: 27.687178, Train_MMSE: 0.047002, NMMSE: 0.050032, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:06:15] Epoch 82/240, Loss: 27.817972, Train_MMSE: 0.046938, NMMSE: 0.050065, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:08:40] Epoch 83/240, Loss: 27.713434, Train_MMSE: 0.046866, NMMSE: 0.050125, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:11:04] Epoch 84/240, Loss: 27.359335, Train_MMSE: 0.046796, NMMSE: 0.050144, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:13:29] Epoch 85/240, Loss: 27.431044, Train_MMSE: 0.046731, NMMSE: 0.050322, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:15:54] Epoch 86/240, Loss: 27.416262, Train_MMSE: 0.046648, NMMSE: 0.05022, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:18:18] Epoch 87/240, Loss: 27.299112, Train_MMSE: 0.046591, NMMSE: 0.050323, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:20:43] Epoch 88/240, Loss: 27.351753, Train_MMSE: 0.046531, NMMSE: 0.05053, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:23:09] Epoch 89/240, Loss: 27.918232, Train_MMSE: 0.046451, NMMSE: 0.050439, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:25:34] Epoch 90/240, Loss: 27.407282, Train_MMSE: 0.046403, NMMSE: 0.050572, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:27:58] Epoch 91/240, Loss: 27.464546, Train_MMSE: 0.046332, NMMSE: 0.050552, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:30:25] Epoch 92/240, Loss: 27.573101, Train_MMSE: 0.046281, NMMSE: 0.050596, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:32:54] Epoch 93/240, Loss: 27.251394, Train_MMSE: 0.046223, NMMSE: 0.05067, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:35:18] Epoch 94/240, Loss: 27.267351, Train_MMSE: 0.046168, NMMSE: 0.05075, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:37:44] Epoch 95/240, Loss: 27.471567, Train_MMSE: 0.046111, NMMSE: 0.05073, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:40:09] Epoch 96/240, Loss: 27.282162, Train_MMSE: 0.046053, NMMSE: 0.050904, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:42:38] Epoch 97/240, Loss: 27.138866, Train_MMSE: 0.045992, NMMSE: 0.050973, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:45:03] Epoch 98/240, Loss: 27.060352, Train_MMSE: 0.045949, NMMSE: 0.050942, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:47:33] Epoch 99/240, Loss: 27.422583, Train_MMSE: 0.045891, NMMSE: 0.050987, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:49:58] Epoch 100/240, Loss: 27.172813, Train_MMSE: 0.04584, NMMSE: 0.05105, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:52:27] Epoch 101/240, Loss: 27.304670, Train_MMSE: 0.045784, NMMSE: 0.051107, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:54:54] Epoch 102/240, Loss: 27.230543, Train_MMSE: 0.045745, NMMSE: 0.051154, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:57:22] Epoch 103/240, Loss: 26.948387, Train_MMSE: 0.045695, NMMSE: 0.05117, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 19:59:47] Epoch 104/240, Loss: 27.067530, Train_MMSE: 0.045644, NMMSE: 0.051346, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:02:12] Epoch 105/240, Loss: 27.073860, Train_MMSE: 0.045591, NMMSE: 0.051426, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:04:38] Epoch 106/240, Loss: 27.196333, Train_MMSE: 0.045551, NMMSE: 0.051424, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:07:03] Epoch 107/240, Loss: 27.152933, Train_MMSE: 0.04551, NMMSE: 0.051488, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:09:34] Epoch 108/240, Loss: 27.061771, Train_MMSE: 0.045451, NMMSE: 0.05152, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:11:58] Epoch 109/240, Loss: 27.013653, Train_MMSE: 0.045412, NMMSE: 0.051486, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:14:22] Epoch 110/240, Loss: 27.198496, Train_MMSE: 0.045361, NMMSE: 0.051633, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:16:48] Epoch 111/240, Loss: 26.985012, Train_MMSE: 0.045333, NMMSE: 0.051719, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:19:13] Epoch 112/240, Loss: 27.007843, Train_MMSE: 0.045287, NMMSE: 0.051776, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:21:37] Epoch 113/240, Loss: 27.049067, Train_MMSE: 0.045246, NMMSE: 0.051808, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:24:02] Epoch 114/240, Loss: 27.001873, Train_MMSE: 0.045204, NMMSE: 0.051804, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:26:26] Epoch 115/240, Loss: 26.924906, Train_MMSE: 0.045171, NMMSE: 0.051888, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:28:50] Epoch 116/240, Loss: 26.974827, Train_MMSE: 0.045121, NMMSE: 0.05188, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:31:15] Epoch 117/240, Loss: 26.981892, Train_MMSE: 0.04509, NMMSE: 0.051988, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:33:38] Epoch 118/240, Loss: 27.035994, Train_MMSE: 0.045055, NMMSE: 0.051938, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:36:01] Epoch 119/240, Loss: 26.932673, Train_MMSE: 0.04502, NMMSE: 0.05182, LS_NMSE: 0.114943, Lr: 0.0001
[2025-02-24 20:38:26] Epoch 120/240, Loss: 27.130630, Train_MMSE: 0.044975, NMMSE: 0.051976, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:40:50] Epoch 121/240, Loss: 26.676586, Train_MMSE: 0.04418, NMMSE: 0.052029, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:43:13] Epoch 122/240, Loss: 26.493830, Train_MMSE: 0.044048, NMMSE: 0.052112, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:45:37] Epoch 123/240, Loss: 26.798018, Train_MMSE: 0.044008, NMMSE: 0.052174, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:48:01] Epoch 124/240, Loss: 26.411057, Train_MMSE: 0.043983, NMMSE: 0.052217, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:50:25] Epoch 125/240, Loss: 26.694807, Train_MMSE: 0.043969, NMMSE: 0.052213, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:52:48] Epoch 126/240, Loss: 26.945152, Train_MMSE: 0.043954, NMMSE: 0.05229, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:55:12] Epoch 127/240, Loss: 26.525896, Train_MMSE: 0.043944, NMMSE: 0.052333, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:57:36] Epoch 128/240, Loss: 26.628229, Train_MMSE: 0.043931, NMMSE: 0.052321, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 20:59:59] Epoch 129/240, Loss: 26.346378, Train_MMSE: 0.043922, NMMSE: 0.052326, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:02:24] Epoch 130/240, Loss: 26.378105, Train_MMSE: 0.043905, NMMSE: 0.052395, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:04:46] Epoch 131/240, Loss: 26.557726, Train_MMSE: 0.043894, NMMSE: 0.052379, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:07:10] Epoch 132/240, Loss: 26.526295, Train_MMSE: 0.043889, NMMSE: 0.0524, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:09:33] Epoch 133/240, Loss: 26.738718, Train_MMSE: 0.043875, NMMSE: 0.052428, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:11:56] Epoch 134/240, Loss: 26.621891, Train_MMSE: 0.04387, NMMSE: 0.052449, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:14:19] Epoch 135/240, Loss: 26.593546, Train_MMSE: 0.04386, NMMSE: 0.052481, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:16:42] Epoch 136/240, Loss: 26.465513, Train_MMSE: 0.043855, NMMSE: 0.052468, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:19:05] Epoch 137/240, Loss: 26.497198, Train_MMSE: 0.04384, NMMSE: 0.052474, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:21:30] Epoch 138/240, Loss: 26.527033, Train_MMSE: 0.043831, NMMSE: 0.05249, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:23:54] Epoch 139/240, Loss: 26.575283, Train_MMSE: 0.043824, NMMSE: 0.052487, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:26:18] Epoch 140/240, Loss: 26.284187, Train_MMSE: 0.043817, NMMSE: 0.052474, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:28:41] Epoch 141/240, Loss: 26.496483, Train_MMSE: 0.043814, NMMSE: 0.052519, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:31:04] Epoch 142/240, Loss: 26.475718, Train_MMSE: 0.043805, NMMSE: 0.052536, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:33:28] Epoch 143/240, Loss: 26.452522, Train_MMSE: 0.043795, NMMSE: 0.052552, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:35:52] Epoch 144/240, Loss: 26.451088, Train_MMSE: 0.043792, NMMSE: 0.052595, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:38:15] Epoch 145/240, Loss: 26.640787, Train_MMSE: 0.043782, NMMSE: 0.052592, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:40:40] Epoch 146/240, Loss: 26.469454, Train_MMSE: 0.043777, NMMSE: 0.052649, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:43:02] Epoch 147/240, Loss: 26.521257, Train_MMSE: 0.043773, NMMSE: 0.052605, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:45:26] Epoch 148/240, Loss: 26.376366, Train_MMSE: 0.043762, NMMSE: 0.052637, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:47:49] Epoch 149/240, Loss: 26.383146, Train_MMSE: 0.043759, NMMSE: 0.052635, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:50:13] Epoch 150/240, Loss: 26.324776, Train_MMSE: 0.043745, NMMSE: 0.052641, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:52:37] Epoch 151/240, Loss: 26.536219, Train_MMSE: 0.043744, NMMSE: 0.052664, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:55:00] Epoch 152/240, Loss: 26.425753, Train_MMSE: 0.043737, NMMSE: 0.052647, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:57:23] Epoch 153/240, Loss: 26.772074, Train_MMSE: 0.043729, NMMSE: 0.05267, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 21:59:46] Epoch 154/240, Loss: 26.328451, Train_MMSE: 0.043726, NMMSE: 0.052712, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:02:11] Epoch 155/240, Loss: 26.627178, Train_MMSE: 0.043714, NMMSE: 0.052709, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:04:35] Epoch 156/240, Loss: 26.399845, Train_MMSE: 0.043715, NMMSE: 0.052709, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:06:59] Epoch 157/240, Loss: 26.363556, Train_MMSE: 0.043706, NMMSE: 0.052707, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:09:23] Epoch 158/240, Loss: 26.575890, Train_MMSE: 0.043698, NMMSE: 0.052736, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:11:48] Epoch 159/240, Loss: 26.365396, Train_MMSE: 0.043692, NMMSE: 0.052765, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:14:12] Epoch 160/240, Loss: 26.503393, Train_MMSE: 0.043685, NMMSE: 0.052758, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:16:36] Epoch 161/240, Loss: 26.332325, Train_MMSE: 0.043681, NMMSE: 0.052807, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:19:01] Epoch 162/240, Loss: 26.506098, Train_MMSE: 0.043677, NMMSE: 0.052816, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:21:24] Epoch 163/240, Loss: 26.525692, Train_MMSE: 0.043674, NMMSE: 0.052767, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:23:48] Epoch 164/240, Loss: 26.470600, Train_MMSE: 0.043663, NMMSE: 0.052783, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:26:12] Epoch 165/240, Loss: 26.547041, Train_MMSE: 0.043661, NMMSE: 0.052789, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:28:36] Epoch 166/240, Loss: 26.277170, Train_MMSE: 0.043654, NMMSE: 0.05281, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:31:01] Epoch 167/240, Loss: 26.408072, Train_MMSE: 0.043649, NMMSE: 0.052788, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:33:26] Epoch 168/240, Loss: 26.598560, Train_MMSE: 0.043644, NMMSE: 0.052803, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:35:50] Epoch 169/240, Loss: 26.493792, Train_MMSE: 0.043638, NMMSE: 0.052812, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:38:13] Epoch 170/240, Loss: 26.663061, Train_MMSE: 0.04363, NMMSE: 0.052848, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:40:38] Epoch 171/240, Loss: 26.513191, Train_MMSE: 0.043624, NMMSE: 0.052823, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:43:00] Epoch 172/240, Loss: 26.553787, Train_MMSE: 0.043619, NMMSE: 0.052849, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:45:24] Epoch 173/240, Loss: 26.500599, Train_MMSE: 0.043615, NMMSE: 0.052903, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:47:47] Epoch 174/240, Loss: 26.602335, Train_MMSE: 0.04361, NMMSE: 0.052903, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:50:09] Epoch 175/240, Loss: 26.337040, Train_MMSE: 0.043608, NMMSE: 0.052907, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:52:32] Epoch 176/240, Loss: 26.454630, Train_MMSE: 0.043599, NMMSE: 0.052923, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:54:56] Epoch 177/240, Loss: 26.362534, Train_MMSE: 0.043598, NMMSE: 0.05291, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:57:18] Epoch 178/240, Loss: 26.561924, Train_MMSE: 0.04359, NMMSE: 0.052944, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 22:59:43] Epoch 179/240, Loss: 26.415308, Train_MMSE: 0.043585, NMMSE: 0.052899, LS_NMSE: 0.114943, Lr: 1e-05
[2025-02-24 23:02:07] Epoch 180/240, Loss: 26.468338, Train_MMSE: 0.043582, NMMSE: 0.052958, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:04:32] Epoch 181/240, Loss: 26.426832, Train_MMSE: 0.043462, NMMSE: 0.052896, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:06:56] Epoch 182/240, Loss: 26.512001, Train_MMSE: 0.04346, NMMSE: 0.052951, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:09:21] Epoch 183/240, Loss: 26.428665, Train_MMSE: 0.043454, NMMSE: 0.052891, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:11:45] Epoch 184/240, Loss: 26.421904, Train_MMSE: 0.043454, NMMSE: 0.052988, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:14:07] Epoch 185/240, Loss: 26.470924, Train_MMSE: 0.04345, NMMSE: 0.052959, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:16:31] Epoch 186/240, Loss: 26.300541, Train_MMSE: 0.043452, NMMSE: 0.052938, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:18:55] Epoch 187/240, Loss: 26.364050, Train_MMSE: 0.043451, NMMSE: 0.052954, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:21:18] Epoch 188/240, Loss: 26.393799, Train_MMSE: 0.043449, NMMSE: 0.052964, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:23:43] Epoch 189/240, Loss: 26.325048, Train_MMSE: 0.043451, NMMSE: 0.052975, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:26:05] Epoch 190/240, Loss: 26.467260, Train_MMSE: 0.043452, NMMSE: 0.052978, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:28:30] Epoch 191/240, Loss: 26.537760, Train_MMSE: 0.043449, NMMSE: 0.05298, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:30:54] Epoch 192/240, Loss: 25.968752, Train_MMSE: 0.04345, NMMSE: 0.052952, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:33:17] Epoch 193/240, Loss: 26.372841, Train_MMSE: 0.043449, NMMSE: 0.052974, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:35:41] Epoch 194/240, Loss: 26.210394, Train_MMSE: 0.043448, NMMSE: 0.05297, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:38:04] Epoch 195/240, Loss: 26.313684, Train_MMSE: 0.043445, NMMSE: 0.052985, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:40:31] Epoch 196/240, Loss: 26.789726, Train_MMSE: 0.043443, NMMSE: 0.052951, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:42:54] Epoch 197/240, Loss: 26.294825, Train_MMSE: 0.043445, NMMSE: 0.053007, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:45:18] Epoch 198/240, Loss: 26.384827, Train_MMSE: 0.043446, NMMSE: 0.05298, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:47:43] Epoch 199/240, Loss: 26.311562, Train_MMSE: 0.043442, NMMSE: 0.052983, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:50:07] Epoch 200/240, Loss: 26.421877, Train_MMSE: 0.04344, NMMSE: 0.053009, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:52:31] Epoch 201/240, Loss: 26.189617, Train_MMSE: 0.043441, NMMSE: 0.052968, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:54:56] Epoch 202/240, Loss: 26.505001, Train_MMSE: 0.043444, NMMSE: 0.053007, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:57:19] Epoch 203/240, Loss: 26.485668, Train_MMSE: 0.043443, NMMSE: 0.05297, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-24 23:59:45] Epoch 204/240, Loss: 26.223856, Train_MMSE: 0.04344, NMMSE: 0.05298, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:02:09] Epoch 205/240, Loss: 26.473402, Train_MMSE: 0.04344, NMMSE: 0.052982, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:04:33] Epoch 206/240, Loss: 26.459126, Train_MMSE: 0.043435, NMMSE: 0.052969, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:06:59] Epoch 207/240, Loss: 26.564552, Train_MMSE: 0.043437, NMMSE: 0.052993, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:09:24] Epoch 208/240, Loss: 26.462744, Train_MMSE: 0.043439, NMMSE: 0.052972, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:11:47] Epoch 209/240, Loss: 26.414526, Train_MMSE: 0.043437, NMMSE: 0.052973, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:14:13] Epoch 210/240, Loss: 26.226719, Train_MMSE: 0.043438, NMMSE: 0.05297, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:16:36] Epoch 211/240, Loss: 26.434231, Train_MMSE: 0.04344, NMMSE: 0.05295, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:19:02] Epoch 212/240, Loss: 26.276838, Train_MMSE: 0.043436, NMMSE: 0.053011, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:21:28] Epoch 213/240, Loss: 26.328836, Train_MMSE: 0.043438, NMMSE: 0.052949, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:23:51] Epoch 214/240, Loss: 26.562439, Train_MMSE: 0.04344, NMMSE: 0.052971, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:26:17] Epoch 215/240, Loss: 26.315334, Train_MMSE: 0.043433, NMMSE: 0.052993, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:28:41] Epoch 216/240, Loss: 26.116257, Train_MMSE: 0.043431, NMMSE: 0.053007, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:31:04] Epoch 217/240, Loss: 26.492472, Train_MMSE: 0.043431, NMMSE: 0.052985, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:33:30] Epoch 218/240, Loss: 26.398184, Train_MMSE: 0.043429, NMMSE: 0.052965, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:55] Epoch 219/240, Loss: 26.401028, Train_MMSE: 0.043426, NMMSE: 0.052993, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:20] Epoch 220/240, Loss: 26.244387, Train_MMSE: 0.043435, NMMSE: 0.053006, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:45] Epoch 221/240, Loss: 26.376511, Train_MMSE: 0.043432, NMMSE: 0.053007, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:11] Epoch 222/240, Loss: 26.484585, Train_MMSE: 0.043429, NMMSE: 0.053046, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:37] Epoch 223/240, Loss: 26.514469, Train_MMSE: 0.04343, NMMSE: 0.052983, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:02] Epoch 224/240, Loss: 26.413116, Train_MMSE: 0.043429, NMMSE: 0.053043, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:50:26] Epoch 225/240, Loss: 26.268097, Train_MMSE: 0.04343, NMMSE: 0.052997, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:52:51] Epoch 226/240, Loss: 26.379309, Train_MMSE: 0.043431, NMMSE: 0.052973, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:55:14] Epoch 227/240, Loss: 26.417046, Train_MMSE: 0.043426, NMMSE: 0.05297, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 00:57:38] Epoch 228/240, Loss: 26.206289, Train_MMSE: 0.043428, NMMSE: 0.052953, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:00:01] Epoch 229/240, Loss: 26.185110, Train_MMSE: 0.043429, NMMSE: 0.053049, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:02:25] Epoch 230/240, Loss: 26.295959, Train_MMSE: 0.04343, NMMSE: 0.053033, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:04:49] Epoch 231/240, Loss: 26.313002, Train_MMSE: 0.043425, NMMSE: 0.053009, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:07:13] Epoch 232/240, Loss: 26.704281, Train_MMSE: 0.043421, NMMSE: 0.052972, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:09:37] Epoch 233/240, Loss: 26.364285, Train_MMSE: 0.043427, NMMSE: 0.053018, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:12:00] Epoch 234/240, Loss: 26.412094, Train_MMSE: 0.043427, NMMSE: 0.053013, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:14:06] Epoch 235/240, Loss: 26.382381, Train_MMSE: 0.043422, NMMSE: 0.053071, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:15:52] Epoch 236/240, Loss: 26.231083, Train_MMSE: 0.043424, NMMSE: 0.053021, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:17:39] Epoch 237/240, Loss: 26.370222, Train_MMSE: 0.043424, NMMSE: 0.053063, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:19:23] Epoch 238/240, Loss: 26.417645, Train_MMSE: 0.043425, NMMSE: 0.052999, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:20:58] Epoch 239/240, Loss: 26.464182, Train_MMSE: 0.043424, NMMSE: 0.052994, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-06
[2025-02-25 01:22:07] Epoch 240/240, Loss: 26.383360, Train_MMSE: 0.043421, NMMSE: 0.053003, LS_NMSE: 0.114943, Lr: 1.0000000000000002e-07
