Train.py PID: 15804

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.055640889662573356
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S9_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f2c4b43d190>
loss function:: SmoothL1Loss()
[2025-02-24 15:05:29] Epoch 1/240, Loss: 39.593155, Train_MMSE: 0.118247, NMMSE: 0.097873, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:07:40] Epoch 2/240, Loss: 35.717449, Train_MMSE: 0.083585, NMMSE: 0.081766, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:09:54] Epoch 3/240, Loss: 35.628288, Train_MMSE: 0.077252, NMMSE: 0.079428, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:12:05] Epoch 4/240, Loss: 34.952251, Train_MMSE: 0.075525, NMMSE: 0.078305, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:14:17] Epoch 5/240, Loss: 34.901180, Train_MMSE: 0.074501, NMMSE: 0.077382, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:16:30] Epoch 6/240, Loss: 34.598984, Train_MMSE: 0.073823, NMMSE: 0.077178, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:18:41] Epoch 7/240, Loss: 34.478626, Train_MMSE: 0.073281, NMMSE: 0.076873, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:20:51] Epoch 8/240, Loss: 34.604004, Train_MMSE: 0.072748, NMMSE: 0.076564, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:23:08] Epoch 9/240, Loss: 34.534409, Train_MMSE: 0.072245, NMMSE: 0.075429, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:25:21] Epoch 10/240, Loss: 34.644939, Train_MMSE: 0.071806, NMMSE: 0.075223, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:27:33] Epoch 11/240, Loss: 34.259426, Train_MMSE: 0.071344, NMMSE: 0.074986, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:29:46] Epoch 12/240, Loss: 34.175892, Train_MMSE: 0.070961, NMMSE: 0.074307, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:31:57] Epoch 13/240, Loss: 34.091152, Train_MMSE: 0.070556, NMMSE: 0.074547, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:34:11] Epoch 14/240, Loss: 34.183498, Train_MMSE: 0.070209, NMMSE: 0.073564, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:36:24] Epoch 15/240, Loss: 33.965897, Train_MMSE: 0.069864, NMMSE: 0.073597, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:38:37] Epoch 16/240, Loss: 33.843575, Train_MMSE: 0.069525, NMMSE: 0.07352, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:40:49] Epoch 17/240, Loss: 33.875420, Train_MMSE: 0.069188, NMMSE: 0.073184, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:43:01] Epoch 18/240, Loss: 33.806622, Train_MMSE: 0.068921, NMMSE: 0.07291, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:45:13] Epoch 19/240, Loss: 33.726608, Train_MMSE: 0.068593, NMMSE: 0.072444, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:47:26] Epoch 20/240, Loss: 33.580379, Train_MMSE: 0.068373, NMMSE: 0.072224, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:49:41] Epoch 21/240, Loss: 33.707329, Train_MMSE: 0.068097, NMMSE: 0.072121, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:51:53] Epoch 22/240, Loss: 33.268822, Train_MMSE: 0.067863, NMMSE: 0.071609, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:54:07] Epoch 23/240, Loss: 33.672504, Train_MMSE: 0.067598, NMMSE: 0.071642, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:56:20] Epoch 24/240, Loss: 33.382545, Train_MMSE: 0.06746, NMMSE: 0.071298, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 15:58:33] Epoch 25/240, Loss: 33.414104, Train_MMSE: 0.06721, NMMSE: 0.071243, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:00:45] Epoch 26/240, Loss: 33.155624, Train_MMSE: 0.067059, NMMSE: 0.071125, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:02:59] Epoch 27/240, Loss: 33.283760, Train_MMSE: 0.066893, NMMSE: 0.070562, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:05:14] Epoch 28/240, Loss: 32.764694, Train_MMSE: 0.066743, NMMSE: 0.071313, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:07:25] Epoch 29/240, Loss: 33.113506, Train_MMSE: 0.066597, NMMSE: 0.071075, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:09:37] Epoch 30/240, Loss: 33.105865, Train_MMSE: 0.066436, NMMSE: 0.07059, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:11:49] Epoch 31/240, Loss: 32.768059, Train_MMSE: 0.066351, NMMSE: 0.070736, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:14:00] Epoch 32/240, Loss: 32.952110, Train_MMSE: 0.066211, NMMSE: 0.069951, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:16:11] Epoch 33/240, Loss: 32.872360, Train_MMSE: 0.066111, NMMSE: 0.070101, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:18:23] Epoch 34/240, Loss: 33.102592, Train_MMSE: 0.065983, NMMSE: 0.070035, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:20:35] Epoch 35/240, Loss: 33.049145, Train_MMSE: 0.065922, NMMSE: 0.070429, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:22:49] Epoch 36/240, Loss: 33.335327, Train_MMSE: 0.065846, NMMSE: 0.06992, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:25:01] Epoch 37/240, Loss: 32.786465, Train_MMSE: 0.065733, NMMSE: 0.069769, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:27:13] Epoch 38/240, Loss: 32.677406, Train_MMSE: 0.065653, NMMSE: 0.069788, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:29:25] Epoch 39/240, Loss: 32.424763, Train_MMSE: 0.065566, NMMSE: 0.069417, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:31:37] Epoch 40/240, Loss: 32.895012, Train_MMSE: 0.065535, NMMSE: 0.069986, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:33:50] Epoch 41/240, Loss: 32.780701, Train_MMSE: 0.065427, NMMSE: 0.069588, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:36:03] Epoch 42/240, Loss: 32.695747, Train_MMSE: 0.065387, NMMSE: 0.06966, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:38:14] Epoch 43/240, Loss: 32.294189, Train_MMSE: 0.065301, NMMSE: 0.069458, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:40:26] Epoch 44/240, Loss: 33.067993, Train_MMSE: 0.065203, NMMSE: 0.069346, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:42:38] Epoch 45/240, Loss: 32.626225, Train_MMSE: 0.065147, NMMSE: 0.069718, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:44:50] Epoch 46/240, Loss: 32.932232, Train_MMSE: 0.065109, NMMSE: 0.069147, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:47:01] Epoch 47/240, Loss: 32.898376, Train_MMSE: 0.065051, NMMSE: 0.069201, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:49:14] Epoch 48/240, Loss: 33.118305, Train_MMSE: 0.064996, NMMSE: 0.068983, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:51:26] Epoch 49/240, Loss: 32.754353, Train_MMSE: 0.064902, NMMSE: 0.068939, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:53:37] Epoch 50/240, Loss: 32.615425, Train_MMSE: 0.064833, NMMSE: 0.069208, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:55:51] Epoch 51/240, Loss: 32.752003, Train_MMSE: 0.064777, NMMSE: 0.069061, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 16:58:05] Epoch 52/240, Loss: 32.496975, Train_MMSE: 0.064727, NMMSE: 0.069059, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:00:16] Epoch 53/240, Loss: 32.757118, Train_MMSE: 0.064686, NMMSE: 0.068893, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:02:29] Epoch 54/240, Loss: 32.935604, Train_MMSE: 0.064631, NMMSE: 0.068801, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:04:40] Epoch 55/240, Loss: 32.806622, Train_MMSE: 0.064576, NMMSE: 0.068871, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:06:51] Epoch 56/240, Loss: 32.364281, Train_MMSE: 0.064516, NMMSE: 0.068533, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:09:03] Epoch 57/240, Loss: 32.484077, Train_MMSE: 0.064482, NMMSE: 0.068695, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:11:15] Epoch 58/240, Loss: 32.463913, Train_MMSE: 0.064434, NMMSE: 0.069132, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:13:27] Epoch 59/240, Loss: 32.465919, Train_MMSE: 0.064358, NMMSE: 0.068358, LS_NMSE: 0.1768, Lr: 0.001
[2025-02-24 17:15:39] Epoch 60/240, Loss: 32.819351, Train_MMSE: 0.064316, NMMSE: 0.068827, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:17:50] Epoch 61/240, Loss: 31.492645, Train_MMSE: 0.061801, NMMSE: 0.066331, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:20:01] Epoch 62/240, Loss: 31.565006, Train_MMSE: 0.061263, NMMSE: 0.066369, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:22:13] Epoch 63/240, Loss: 31.524055, Train_MMSE: 0.06103, NMMSE: 0.066476, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:24:25] Epoch 64/240, Loss: 31.528263, Train_MMSE: 0.060851, NMMSE: 0.066519, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:26:36] Epoch 65/240, Loss: 31.895817, Train_MMSE: 0.060695, NMMSE: 0.066679, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:28:52] Epoch 66/240, Loss: 31.189217, Train_MMSE: 0.060557, NMMSE: 0.066678, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:31:03] Epoch 67/240, Loss: 31.524988, Train_MMSE: 0.060421, NMMSE: 0.066797, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:33:14] Epoch 68/240, Loss: 31.455231, Train_MMSE: 0.060304, NMMSE: 0.066834, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:35:25] Epoch 69/240, Loss: 31.187164, Train_MMSE: 0.060183, NMMSE: 0.066878, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:37:37] Epoch 70/240, Loss: 31.057930, Train_MMSE: 0.060072, NMMSE: 0.066957, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:39:49] Epoch 71/240, Loss: 31.317499, Train_MMSE: 0.059974, NMMSE: 0.067079, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:42:01] Epoch 72/240, Loss: 31.287910, Train_MMSE: 0.059877, NMMSE: 0.067133, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:44:15] Epoch 73/240, Loss: 31.163687, Train_MMSE: 0.059789, NMMSE: 0.067221, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:46:31] Epoch 74/240, Loss: 31.227909, Train_MMSE: 0.059692, NMMSE: 0.0673, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:48:45] Epoch 75/240, Loss: 31.189758, Train_MMSE: 0.059602, NMMSE: 0.067293, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:51:06] Epoch 76/240, Loss: 30.962732, Train_MMSE: 0.059514, NMMSE: 0.067455, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:53:22] Epoch 77/240, Loss: 31.050364, Train_MMSE: 0.059431, NMMSE: 0.067478, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:55:45] Epoch 78/240, Loss: 31.423086, Train_MMSE: 0.059354, NMMSE: 0.067555, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 17:58:09] Epoch 79/240, Loss: 30.872322, Train_MMSE: 0.059284, NMMSE: 0.067595, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:00:33] Epoch 80/240, Loss: 31.080807, Train_MMSE: 0.05919, NMMSE: 0.067687, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:02:50] Epoch 81/240, Loss: 31.281565, Train_MMSE: 0.059122, NMMSE: 0.067762, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:05:14] Epoch 82/240, Loss: 30.917698, Train_MMSE: 0.059051, NMMSE: 0.067859, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:07:31] Epoch 83/240, Loss: 31.112104, Train_MMSE: 0.058977, NMMSE: 0.067891, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:09:53] Epoch 84/240, Loss: 30.891672, Train_MMSE: 0.058899, NMMSE: 0.067996, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:12:16] Epoch 85/240, Loss: 31.015802, Train_MMSE: 0.058837, NMMSE: 0.067986, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:14:38] Epoch 86/240, Loss: 30.919508, Train_MMSE: 0.058763, NMMSE: 0.068043, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:16:58] Epoch 87/240, Loss: 30.757059, Train_MMSE: 0.0587, NMMSE: 0.068057, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:19:13] Epoch 88/240, Loss: 30.806208, Train_MMSE: 0.05864, NMMSE: 0.068069, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:21:27] Epoch 89/240, Loss: 30.924316, Train_MMSE: 0.058576, NMMSE: 0.068314, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:23:42] Epoch 90/240, Loss: 30.997156, Train_MMSE: 0.058512, NMMSE: 0.068414, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:25:59] Epoch 91/240, Loss: 31.070190, Train_MMSE: 0.05846, NMMSE: 0.068366, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:28:15] Epoch 92/240, Loss: 30.871847, Train_MMSE: 0.058395, NMMSE: 0.06835, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:30:17] Epoch 93/240, Loss: 30.571445, Train_MMSE: 0.058339, NMMSE: 0.068531, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:32:14] Epoch 94/240, Loss: 30.929935, Train_MMSE: 0.058287, NMMSE: 0.068664, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:34:13] Epoch 95/240, Loss: 30.703430, Train_MMSE: 0.058221, NMMSE: 0.068684, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:36:10] Epoch 96/240, Loss: 30.819239, Train_MMSE: 0.05817, NMMSE: 0.068712, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:38:06] Epoch 97/240, Loss: 30.910744, Train_MMSE: 0.058113, NMMSE: 0.068796, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:40:02] Epoch 98/240, Loss: 30.760685, Train_MMSE: 0.058067, NMMSE: 0.068705, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:42:06] Epoch 99/240, Loss: 30.762690, Train_MMSE: 0.058006, NMMSE: 0.068936, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:44:03] Epoch 100/240, Loss: 30.900827, Train_MMSE: 0.057962, NMMSE: 0.068871, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:46:01] Epoch 101/240, Loss: 30.402414, Train_MMSE: 0.057922, NMMSE: 0.069011, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:47:58] Epoch 102/240, Loss: 30.657600, Train_MMSE: 0.057868, NMMSE: 0.068943, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:49:55] Epoch 103/240, Loss: 30.737356, Train_MMSE: 0.057806, NMMSE: 0.068938, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:51:53] Epoch 104/240, Loss: 30.433981, Train_MMSE: 0.057759, NMMSE: 0.069056, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:53:51] Epoch 105/240, Loss: 30.418709, Train_MMSE: 0.057721, NMMSE: 0.069084, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:55:48] Epoch 106/240, Loss: 30.500322, Train_MMSE: 0.057676, NMMSE: 0.069207, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:57:44] Epoch 107/240, Loss: 30.606211, Train_MMSE: 0.057631, NMMSE: 0.069334, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 18:59:42] Epoch 108/240, Loss: 30.762665, Train_MMSE: 0.057584, NMMSE: 0.069228, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:01:39] Epoch 109/240, Loss: 30.553091, Train_MMSE: 0.057542, NMMSE: 0.069393, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:03:37] Epoch 110/240, Loss: 30.647844, Train_MMSE: 0.057498, NMMSE: 0.069447, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:05:37] Epoch 111/240, Loss: 30.538643, Train_MMSE: 0.057455, NMMSE: 0.069479, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:07:34] Epoch 112/240, Loss: 30.394026, Train_MMSE: 0.057397, NMMSE: 0.069455, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:09:33] Epoch 113/240, Loss: 30.779213, Train_MMSE: 0.057368, NMMSE: 0.06948, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:11:29] Epoch 114/240, Loss: 30.571663, Train_MMSE: 0.057325, NMMSE: 0.069563, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:13:27] Epoch 115/240, Loss: 30.368046, Train_MMSE: 0.057298, NMMSE: 0.069658, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:15:24] Epoch 116/240, Loss: 30.662666, Train_MMSE: 0.057247, NMMSE: 0.069659, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:17:21] Epoch 117/240, Loss: 30.523333, Train_MMSE: 0.057206, NMMSE: 0.069772, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:19:18] Epoch 118/240, Loss: 30.437403, Train_MMSE: 0.057171, NMMSE: 0.069693, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:21:16] Epoch 119/240, Loss: 30.487104, Train_MMSE: 0.057144, NMMSE: 0.069776, LS_NMSE: 0.1768, Lr: 0.0001
[2025-02-24 19:23:16] Epoch 120/240, Loss: 30.380304, Train_MMSE: 0.057102, NMMSE: 0.069813, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:25:13] Epoch 121/240, Loss: 29.812555, Train_MMSE: 0.056135, NMMSE: 0.069733, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:27:15] Epoch 122/240, Loss: 30.098179, Train_MMSE: 0.05599, NMMSE: 0.069845, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:29:12] Epoch 123/240, Loss: 29.962473, Train_MMSE: 0.05595, NMMSE: 0.06993, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:31:08] Epoch 124/240, Loss: 30.000380, Train_MMSE: 0.055925, NMMSE: 0.069929, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:33:06] Epoch 125/240, Loss: 29.679193, Train_MMSE: 0.055904, NMMSE: 0.07, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:35:04] Epoch 126/240, Loss: 29.835840, Train_MMSE: 0.055888, NMMSE: 0.070045, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:37:05] Epoch 127/240, Loss: 29.941641, Train_MMSE: 0.055873, NMMSE: 0.070051, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:39:02] Epoch 128/240, Loss: 29.705362, Train_MMSE: 0.055861, NMMSE: 0.070126, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:40:58] Epoch 129/240, Loss: 29.999481, Train_MMSE: 0.055845, NMMSE: 0.070072, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:42:55] Epoch 130/240, Loss: 29.933920, Train_MMSE: 0.055833, NMMSE: 0.070127, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:44:51] Epoch 131/240, Loss: 29.827267, Train_MMSE: 0.055823, NMMSE: 0.070181, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:46:50] Epoch 132/240, Loss: 30.379910, Train_MMSE: 0.055811, NMMSE: 0.070193, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:48:45] Epoch 133/240, Loss: 29.988779, Train_MMSE: 0.055798, NMMSE: 0.070191, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:50:43] Epoch 134/240, Loss: 29.957819, Train_MMSE: 0.055792, NMMSE: 0.070213, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:52:39] Epoch 135/240, Loss: 29.792053, Train_MMSE: 0.055779, NMMSE: 0.070231, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:54:35] Epoch 136/240, Loss: 29.856676, Train_MMSE: 0.055775, NMMSE: 0.070268, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:56:32] Epoch 137/240, Loss: 29.871861, Train_MMSE: 0.05577, NMMSE: 0.070259, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 19:58:28] Epoch 138/240, Loss: 30.093336, Train_MMSE: 0.055755, NMMSE: 0.070289, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:00:24] Epoch 139/240, Loss: 29.917027, Train_MMSE: 0.055747, NMMSE: 0.070312, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:02:21] Epoch 140/240, Loss: 29.925816, Train_MMSE: 0.055739, NMMSE: 0.070333, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:04:18] Epoch 141/240, Loss: 29.774088, Train_MMSE: 0.05573, NMMSE: 0.070349, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:06:16] Epoch 142/240, Loss: 29.923170, Train_MMSE: 0.055729, NMMSE: 0.07033, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:08:13] Epoch 143/240, Loss: 29.961830, Train_MMSE: 0.055715, NMMSE: 0.070349, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:10:10] Epoch 144/240, Loss: 29.783339, Train_MMSE: 0.055707, NMMSE: 0.070353, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:12:10] Epoch 145/240, Loss: 30.021950, Train_MMSE: 0.0557, NMMSE: 0.070352, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:14:06] Epoch 146/240, Loss: 29.984535, Train_MMSE: 0.055694, NMMSE: 0.070426, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:16:03] Epoch 147/240, Loss: 30.104069, Train_MMSE: 0.055686, NMMSE: 0.070405, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:18:01] Epoch 148/240, Loss: 29.686747, Train_MMSE: 0.055676, NMMSE: 0.070407, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:20:00] Epoch 149/240, Loss: 29.811152, Train_MMSE: 0.055667, NMMSE: 0.070429, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:22:01] Epoch 150/240, Loss: 29.958330, Train_MMSE: 0.055664, NMMSE: 0.07042, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:23:57] Epoch 151/240, Loss: 29.992945, Train_MMSE: 0.05566, NMMSE: 0.070452, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:25:56] Epoch 152/240, Loss: 29.628550, Train_MMSE: 0.055649, NMMSE: 0.070484, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:27:55] Epoch 153/240, Loss: 29.863613, Train_MMSE: 0.055642, NMMSE: 0.070522, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:29:54] Epoch 154/240, Loss: 29.921770, Train_MMSE: 0.055636, NMMSE: 0.070466, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:31:53] Epoch 155/240, Loss: 29.832289, Train_MMSE: 0.055632, NMMSE: 0.070487, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:33:52] Epoch 156/240, Loss: 29.952955, Train_MMSE: 0.055627, NMMSE: 0.070539, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:35:49] Epoch 157/240, Loss: 29.912336, Train_MMSE: 0.055622, NMMSE: 0.070574, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:37:47] Epoch 158/240, Loss: 29.588644, Train_MMSE: 0.055611, NMMSE: 0.0705, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:39:45] Epoch 159/240, Loss: 30.088419, Train_MMSE: 0.055607, NMMSE: 0.070557, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:41:44] Epoch 160/240, Loss: 29.891975, Train_MMSE: 0.055598, NMMSE: 0.070563, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:43:41] Epoch 161/240, Loss: 29.604713, Train_MMSE: 0.055591, NMMSE: 0.070563, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:45:42] Epoch 162/240, Loss: 29.811594, Train_MMSE: 0.055587, NMMSE: 0.070556, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:47:42] Epoch 163/240, Loss: 29.812378, Train_MMSE: 0.055585, NMMSE: 0.070603, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:49:43] Epoch 164/240, Loss: 29.706421, Train_MMSE: 0.055574, NMMSE: 0.070583, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:51:42] Epoch 165/240, Loss: 29.532827, Train_MMSE: 0.055568, NMMSE: 0.070591, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:53:39] Epoch 166/240, Loss: 29.922903, Train_MMSE: 0.055564, NMMSE: 0.070649, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:55:37] Epoch 167/240, Loss: 29.881016, Train_MMSE: 0.055556, NMMSE: 0.070609, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:57:35] Epoch 168/240, Loss: 29.709055, Train_MMSE: 0.055548, NMMSE: 0.070587, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 20:59:33] Epoch 169/240, Loss: 29.688990, Train_MMSE: 0.055544, NMMSE: 0.07065, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:01:33] Epoch 170/240, Loss: 29.626350, Train_MMSE: 0.055536, NMMSE: 0.070638, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:03:34] Epoch 171/240, Loss: 29.985125, Train_MMSE: 0.055538, NMMSE: 0.070646, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:05:31] Epoch 172/240, Loss: 29.681101, Train_MMSE: 0.055525, NMMSE: 0.070665, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:07:24] Epoch 173/240, Loss: 30.068661, Train_MMSE: 0.05552, NMMSE: 0.070659, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:09:02] Epoch 174/240, Loss: 30.032898, Train_MMSE: 0.055514, NMMSE: 0.070675, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:10:43] Epoch 175/240, Loss: 29.823751, Train_MMSE: 0.055513, NMMSE: 0.070677, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:12:23] Epoch 176/240, Loss: 29.892740, Train_MMSE: 0.055506, NMMSE: 0.070692, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:14:09] Epoch 177/240, Loss: 29.869791, Train_MMSE: 0.055499, NMMSE: 0.0707, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:15:49] Epoch 178/240, Loss: 29.997707, Train_MMSE: 0.055499, NMMSE: 0.070744, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:17:35] Epoch 179/240, Loss: 29.586298, Train_MMSE: 0.05549, NMMSE: 0.070726, LS_NMSE: 0.1768, Lr: 1e-05
[2025-02-24 21:19:18] Epoch 180/240, Loss: 29.819300, Train_MMSE: 0.055488, NMMSE: 0.070732, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:20:58] Epoch 181/240, Loss: 29.670588, Train_MMSE: 0.055353, NMMSE: 0.07073, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:22:35] Epoch 182/240, Loss: 29.817553, Train_MMSE: 0.055338, NMMSE: 0.070719, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:24:08] Epoch 183/240, Loss: 29.763004, Train_MMSE: 0.055333, NMMSE: 0.070755, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:25:47] Epoch 184/240, Loss: 29.628672, Train_MMSE: 0.055329, NMMSE: 0.07078, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:27:25] Epoch 185/240, Loss: 29.728550, Train_MMSE: 0.05533, NMMSE: 0.070772, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:29:03] Epoch 186/240, Loss: 29.579920, Train_MMSE: 0.055337, NMMSE: 0.070717, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:30:41] Epoch 187/240, Loss: 29.580645, Train_MMSE: 0.055329, NMMSE: 0.070731, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:32:20] Epoch 188/240, Loss: 29.618324, Train_MMSE: 0.055331, NMMSE: 0.070742, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:33:57] Epoch 189/240, Loss: 29.876064, Train_MMSE: 0.055326, NMMSE: 0.070775, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:35:29] Epoch 190/240, Loss: 29.794424, Train_MMSE: 0.055327, NMMSE: 0.070748, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:37:04] Epoch 191/240, Loss: 29.621965, Train_MMSE: 0.055326, NMMSE: 0.070787, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:38:43] Epoch 192/240, Loss: 29.921787, Train_MMSE: 0.055326, NMMSE: 0.070786, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:40:16] Epoch 193/240, Loss: 29.683302, Train_MMSE: 0.055326, NMMSE: 0.07075, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:41:54] Epoch 194/240, Loss: 29.797251, Train_MMSE: 0.055328, NMMSE: 0.070755, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:43:32] Epoch 195/240, Loss: 30.052782, Train_MMSE: 0.055325, NMMSE: 0.070738, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:45:10] Epoch 196/240, Loss: 29.903463, Train_MMSE: 0.055325, NMMSE: 0.07075, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:46:44] Epoch 197/240, Loss: 29.659525, Train_MMSE: 0.055328, NMMSE: 0.070739, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:48:22] Epoch 198/240, Loss: 29.787605, Train_MMSE: 0.055323, NMMSE: 0.070746, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:49:59] Epoch 199/240, Loss: 29.690226, Train_MMSE: 0.055324, NMMSE: 0.070739, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:51:33] Epoch 200/240, Loss: 29.612026, Train_MMSE: 0.055327, NMMSE: 0.070752, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:53:12] Epoch 201/240, Loss: 29.965755, Train_MMSE: 0.055322, NMMSE: 0.070744, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:54:51] Epoch 202/240, Loss: 30.213797, Train_MMSE: 0.055321, NMMSE: 0.070735, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:56:28] Epoch 203/240, Loss: 29.593880, Train_MMSE: 0.055322, NMMSE: 0.070782, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:58:09] Epoch 204/240, Loss: 29.438185, Train_MMSE: 0.055323, NMMSE: 0.070798, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 21:59:44] Epoch 205/240, Loss: 29.887186, Train_MMSE: 0.055321, NMMSE: 0.070761, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:01:19] Epoch 206/240, Loss: 29.811266, Train_MMSE: 0.055317, NMMSE: 0.070765, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:02:52] Epoch 207/240, Loss: 29.799507, Train_MMSE: 0.05532, NMMSE: 0.070768, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:04:25] Epoch 208/240, Loss: 29.798269, Train_MMSE: 0.055319, NMMSE: 0.070786, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:06:02] Epoch 209/240, Loss: 29.851858, Train_MMSE: 0.055316, NMMSE: 0.070764, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:07:37] Epoch 210/240, Loss: 29.818094, Train_MMSE: 0.055319, NMMSE: 0.070778, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:09:11] Epoch 211/240, Loss: 29.812021, Train_MMSE: 0.055314, NMMSE: 0.070803, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:10:50] Epoch 212/240, Loss: 29.812857, Train_MMSE: 0.055316, NMMSE: 0.070754, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:12:29] Epoch 213/240, Loss: 30.041914, Train_MMSE: 0.055315, NMMSE: 0.070773, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:14:06] Epoch 214/240, Loss: 29.637966, Train_MMSE: 0.055315, NMMSE: 0.070808, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:15:46] Epoch 215/240, Loss: 29.746834, Train_MMSE: 0.055317, NMMSE: 0.070837, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:17:22] Epoch 216/240, Loss: 29.638666, Train_MMSE: 0.05532, NMMSE: 0.070809, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:19:03] Epoch 217/240, Loss: 29.588985, Train_MMSE: 0.055315, NMMSE: 0.070797, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:20:42] Epoch 218/240, Loss: 29.666786, Train_MMSE: 0.055309, NMMSE: 0.070798, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:22:20] Epoch 219/240, Loss: 29.592659, Train_MMSE: 0.055314, NMMSE: 0.070798, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:23:55] Epoch 220/240, Loss: 29.914011, Train_MMSE: 0.055308, NMMSE: 0.070771, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:25:28] Epoch 221/240, Loss: 29.979523, Train_MMSE: 0.05531, NMMSE: 0.070819, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:27:06] Epoch 222/240, Loss: 29.744331, Train_MMSE: 0.05531, NMMSE: 0.070801, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:28:42] Epoch 223/240, Loss: 29.582605, Train_MMSE: 0.055314, NMMSE: 0.070814, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:30:17] Epoch 224/240, Loss: 29.693306, Train_MMSE: 0.055309, NMMSE: 0.070796, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:31:54] Epoch 225/240, Loss: 29.597134, Train_MMSE: 0.055308, NMMSE: 0.070788, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:33:31] Epoch 226/240, Loss: 29.886614, Train_MMSE: 0.055307, NMMSE: 0.070805, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:35:08] Epoch 227/240, Loss: 29.910143, Train_MMSE: 0.055307, NMMSE: 0.070799, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:36:40] Epoch 228/240, Loss: 29.632952, Train_MMSE: 0.055305, NMMSE: 0.070797, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:38:17] Epoch 229/240, Loss: 29.843149, Train_MMSE: 0.055304, NMMSE: 0.070873, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:39:54] Epoch 230/240, Loss: 29.653128, Train_MMSE: 0.055303, NMMSE: 0.070837, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:41:27] Epoch 231/240, Loss: 29.629982, Train_MMSE: 0.055305, NMMSE: 0.070813, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:43:01] Epoch 232/240, Loss: 29.822662, Train_MMSE: 0.0553, NMMSE: 0.070831, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:44:40] Epoch 233/240, Loss: 29.516060, Train_MMSE: 0.055306, NMMSE: 0.070801, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:46:16] Epoch 234/240, Loss: 29.507406, Train_MMSE: 0.055305, NMMSE: 0.070834, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:47:46] Epoch 235/240, Loss: 29.755819, Train_MMSE: 0.055305, NMMSE: 0.07083, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:49:25] Epoch 236/240, Loss: 29.787027, Train_MMSE: 0.055302, NMMSE: 0.070785, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:51:01] Epoch 237/240, Loss: 29.851770, Train_MMSE: 0.0553, NMMSE: 0.070854, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:52:39] Epoch 238/240, Loss: 29.858438, Train_MMSE: 0.0553, NMMSE: 0.070824, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:54:13] Epoch 239/240, Loss: 29.888994, Train_MMSE: 0.055301, NMMSE: 0.070791, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-06
[2025-02-24 22:55:40] Epoch 240/240, Loss: 29.760233, Train_MMSE: 0.055305, NMMSE: 0.070818, LS_NMSE: 0.1768, Lr: 1.0000000000000002e-07
