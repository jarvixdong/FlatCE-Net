Train.py PID: 4784

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.06314184112037872
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L3_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L3_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/flatCE_L3C32_test_Dataset_dB-15_N36_K4_L5_S9_Setup20_Reliz1000.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f4c23ead2b0>
loss function:: SmoothL1Loss()
[2025-02-24 15:55:11] Epoch 1/240, Loss: 80.561356, Train_MMSE: 0.717926, NMMSE: 0.544874, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 15:57:46] Epoch 2/240, Loss: 36.190456, Train_MMSE: 0.22925, NMMSE: 0.076121, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:00:23] Epoch 3/240, Loss: 35.323166, Train_MMSE: 0.077498, NMMSE: 0.074323, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:02:55] Epoch 4/240, Loss: 35.210232, Train_MMSE: 0.075642, NMMSE: 0.073791, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:05:22] Epoch 5/240, Loss: 35.371754, Train_MMSE: 0.074833, NMMSE: 0.073177, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:07:48] Epoch 6/240, Loss: 34.654346, Train_MMSE: 0.074332, NMMSE: 0.071646, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:10:15] Epoch 7/240, Loss: 34.730389, Train_MMSE: 0.073901, NMMSE: 0.072034, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:12:41] Epoch 8/240, Loss: 35.030174, Train_MMSE: 0.073629, NMMSE: 0.072249, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:15:08] Epoch 9/240, Loss: 34.485447, Train_MMSE: 0.073427, NMMSE: 0.071872, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:17:34] Epoch 10/240, Loss: 34.043213, Train_MMSE: 0.073248, NMMSE: 0.071256, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:20:00] Epoch 11/240, Loss: 34.405560, Train_MMSE: 0.073103, NMMSE: 0.071199, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:22:26] Epoch 12/240, Loss: 34.520306, Train_MMSE: 0.072976, NMMSE: 0.071294, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:24:51] Epoch 13/240, Loss: 34.379395, Train_MMSE: 0.072916, NMMSE: 0.071268, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:27:16] Epoch 14/240, Loss: 34.728779, Train_MMSE: 0.072786, NMMSE: 0.071569, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:29:41] Epoch 15/240, Loss: 34.313618, Train_MMSE: 0.072725, NMMSE: 0.071359, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:32:05] Epoch 16/240, Loss: 34.398762, Train_MMSE: 0.072624, NMMSE: 0.071122, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:34:30] Epoch 17/240, Loss: 34.768768, Train_MMSE: 0.072634, NMMSE: 0.071167, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:36:54] Epoch 18/240, Loss: 34.494045, Train_MMSE: 0.072534, NMMSE: 0.070724, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:39:17] Epoch 19/240, Loss: 34.309814, Train_MMSE: 0.07246, NMMSE: 0.071407, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:41:42] Epoch 20/240, Loss: 34.462463, Train_MMSE: 0.072416, NMMSE: 0.071055, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:44:06] Epoch 21/240, Loss: 34.357174, Train_MMSE: 0.072374, NMMSE: 0.070817, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:46:31] Epoch 22/240, Loss: 34.508518, Train_MMSE: 0.072348, NMMSE: 0.07151, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:48:55] Epoch 23/240, Loss: 34.352448, Train_MMSE: 0.072285, NMMSE: 0.070767, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:51:19] Epoch 24/240, Loss: 34.139454, Train_MMSE: 0.072261, NMMSE: 0.070669, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:53:44] Epoch 25/240, Loss: 34.365906, Train_MMSE: 0.072211, NMMSE: 0.070809, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:56:08] Epoch 26/240, Loss: 34.750462, Train_MMSE: 0.072194, NMMSE: 0.071156, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 16:58:32] Epoch 27/240, Loss: 34.426994, Train_MMSE: 0.072171, NMMSE: 0.0709, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:00:55] Epoch 28/240, Loss: 34.392212, Train_MMSE: 0.072064, NMMSE: 0.070548, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:03:19] Epoch 29/240, Loss: 34.089485, Train_MMSE: 0.072086, NMMSE: 0.070883, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:05:42] Epoch 30/240, Loss: 34.147842, Train_MMSE: 0.072055, NMMSE: 0.07134, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:08:07] Epoch 31/240, Loss: 34.588127, Train_MMSE: 0.07206, NMMSE: 0.070253, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:10:30] Epoch 32/240, Loss: 34.249836, Train_MMSE: 0.072025, NMMSE: 0.070873, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:12:55] Epoch 33/240, Loss: 34.017403, Train_MMSE: 0.071982, NMMSE: 0.070563, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:15:20] Epoch 34/240, Loss: 34.421318, Train_MMSE: 0.071953, NMMSE: 0.070789, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:17:45] Epoch 35/240, Loss: 34.314037, Train_MMSE: 0.071935, NMMSE: 0.070471, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:20:07] Epoch 36/240, Loss: 34.127590, Train_MMSE: 0.071921, NMMSE: 0.070607, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:22:31] Epoch 37/240, Loss: 33.851562, Train_MMSE: 0.071881, NMMSE: 0.070995, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:24:55] Epoch 38/240, Loss: 34.429661, Train_MMSE: 0.071882, NMMSE: 0.070721, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:27:19] Epoch 39/240, Loss: 34.168598, Train_MMSE: 0.071854, NMMSE: 0.071145, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:29:44] Epoch 40/240, Loss: 33.966064, Train_MMSE: 0.07186, NMMSE: 0.070505, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:32:08] Epoch 41/240, Loss: 34.203392, Train_MMSE: 0.071815, NMMSE: 0.070802, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:34:31] Epoch 42/240, Loss: 34.761833, Train_MMSE: 0.071796, NMMSE: 0.070842, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:36:55] Epoch 43/240, Loss: 34.321388, Train_MMSE: 0.071791, NMMSE: 0.071344, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:39:20] Epoch 44/240, Loss: 34.474937, Train_MMSE: 0.071739, NMMSE: 0.070298, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:41:45] Epoch 45/240, Loss: 34.445660, Train_MMSE: 0.071756, NMMSE: 0.070693, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:44:09] Epoch 46/240, Loss: 34.103386, Train_MMSE: 0.071732, NMMSE: 0.070655, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:46:33] Epoch 47/240, Loss: 33.970493, Train_MMSE: 0.071748, NMMSE: 0.070462, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:48:57] Epoch 48/240, Loss: 34.360409, Train_MMSE: 0.071705, NMMSE: 0.07061, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:51:22] Epoch 49/240, Loss: 34.063026, Train_MMSE: 0.071717, NMMSE: 0.070522, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:53:47] Epoch 50/240, Loss: 34.235825, Train_MMSE: 0.071658, NMMSE: 0.070881, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:56:11] Epoch 51/240, Loss: 34.165501, Train_MMSE: 0.071671, NMMSE: 0.070945, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 17:58:37] Epoch 52/240, Loss: 34.117908, Train_MMSE: 0.07162, NMMSE: 0.070819, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:01:07] Epoch 53/240, Loss: 34.484806, Train_MMSE: 0.071644, NMMSE: 0.070439, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:03:30] Epoch 54/240, Loss: 34.260452, Train_MMSE: 0.071605, NMMSE: 0.070295, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:05:56] Epoch 55/240, Loss: 34.115162, Train_MMSE: 0.071582, NMMSE: 0.070688, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:08:20] Epoch 56/240, Loss: 34.488621, Train_MMSE: 0.071552, NMMSE: 0.070649, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:10:44] Epoch 57/240, Loss: 34.324177, Train_MMSE: 0.071601, NMMSE: 0.070258, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:13:09] Epoch 58/240, Loss: 34.275208, Train_MMSE: 0.071579, NMMSE: 0.070852, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:15:33] Epoch 59/240, Loss: 34.119007, Train_MMSE: 0.071518, NMMSE: 0.070266, LS_NMSE: 0.234713, Lr: 0.001
[2025-02-24 18:17:57] Epoch 60/240, Loss: 33.940273, Train_MMSE: 0.071532, NMMSE: 0.070558, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:20:22] Epoch 61/240, Loss: 33.918762, Train_MMSE: 0.070133, NMMSE: 0.069338, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:22:47] Epoch 62/240, Loss: 33.783752, Train_MMSE: 0.069951, NMMSE: 0.069405, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:25:12] Epoch 63/240, Loss: 34.393852, Train_MMSE: 0.069884, NMMSE: 0.069494, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:27:36] Epoch 64/240, Loss: 33.942417, Train_MMSE: 0.069838, NMMSE: 0.06947, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:29:59] Epoch 65/240, Loss: 33.524006, Train_MMSE: 0.069802, NMMSE: 0.069603, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:32:24] Epoch 66/240, Loss: 34.020306, Train_MMSE: 0.069779, NMMSE: 0.069724, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:34:49] Epoch 67/240, Loss: 33.196079, Train_MMSE: 0.069762, NMMSE: 0.069626, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:37:15] Epoch 68/240, Loss: 33.839382, Train_MMSE: 0.069722, NMMSE: 0.069715, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:39:40] Epoch 69/240, Loss: 33.842480, Train_MMSE: 0.069697, NMMSE: 0.069691, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:42:03] Epoch 70/240, Loss: 33.486511, Train_MMSE: 0.069671, NMMSE: 0.069744, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:44:28] Epoch 71/240, Loss: 33.576038, Train_MMSE: 0.069644, NMMSE: 0.069781, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:46:52] Epoch 72/240, Loss: 33.916733, Train_MMSE: 0.069625, NMMSE: 0.069839, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:49:17] Epoch 73/240, Loss: 33.883064, Train_MMSE: 0.069596, NMMSE: 0.069748, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:51:43] Epoch 74/240, Loss: 33.822544, Train_MMSE: 0.069584, NMMSE: 0.069903, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:54:06] Epoch 75/240, Loss: 33.744797, Train_MMSE: 0.069547, NMMSE: 0.070032, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:56:29] Epoch 76/240, Loss: 33.398804, Train_MMSE: 0.069543, NMMSE: 0.06987, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 18:58:53] Epoch 77/240, Loss: 33.500988, Train_MMSE: 0.069525, NMMSE: 0.069906, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:01:15] Epoch 78/240, Loss: 33.656998, Train_MMSE: 0.069512, NMMSE: 0.069928, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:03:37] Epoch 79/240, Loss: 33.764973, Train_MMSE: 0.069477, NMMSE: 0.07004, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:06:00] Epoch 80/240, Loss: 33.391701, Train_MMSE: 0.069457, NMMSE: 0.070036, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:08:24] Epoch 81/240, Loss: 33.669876, Train_MMSE: 0.069447, NMMSE: 0.070134, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:10:47] Epoch 82/240, Loss: 33.382114, Train_MMSE: 0.069411, NMMSE: 0.070145, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:13:11] Epoch 83/240, Loss: 33.703693, Train_MMSE: 0.069415, NMMSE: 0.069969, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:15:34] Epoch 84/240, Loss: 33.795605, Train_MMSE: 0.069377, NMMSE: 0.070073, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:17:57] Epoch 85/240, Loss: 33.579792, Train_MMSE: 0.069371, NMMSE: 0.070075, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:20:20] Epoch 86/240, Loss: 33.627625, Train_MMSE: 0.069344, NMMSE: 0.070186, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:22:44] Epoch 87/240, Loss: 33.530052, Train_MMSE: 0.069327, NMMSE: 0.070125, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:25:08] Epoch 88/240, Loss: 33.528000, Train_MMSE: 0.069309, NMMSE: 0.070138, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:27:32] Epoch 89/240, Loss: 33.002720, Train_MMSE: 0.069319, NMMSE: 0.070264, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:29:55] Epoch 90/240, Loss: 33.598843, Train_MMSE: 0.069269, NMMSE: 0.070209, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:32:19] Epoch 91/240, Loss: 33.562893, Train_MMSE: 0.069254, NMMSE: 0.070272, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:34:42] Epoch 92/240, Loss: 33.588650, Train_MMSE: 0.069241, NMMSE: 0.07032, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:37:06] Epoch 93/240, Loss: 33.801067, Train_MMSE: 0.069242, NMMSE: 0.070236, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:39:29] Epoch 94/240, Loss: 33.378555, Train_MMSE: 0.069215, NMMSE: 0.070232, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:41:52] Epoch 95/240, Loss: 33.305412, Train_MMSE: 0.069194, NMMSE: 0.070358, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:44:15] Epoch 96/240, Loss: 33.708752, Train_MMSE: 0.069183, NMMSE: 0.070293, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:46:38] Epoch 97/240, Loss: 33.347992, Train_MMSE: 0.069169, NMMSE: 0.070298, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:49:02] Epoch 98/240, Loss: 33.284779, Train_MMSE: 0.069155, NMMSE: 0.070368, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:51:26] Epoch 99/240, Loss: 33.444496, Train_MMSE: 0.069141, NMMSE: 0.070409, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:53:51] Epoch 100/240, Loss: 33.617062, Train_MMSE: 0.069131, NMMSE: 0.070391, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:56:15] Epoch 101/240, Loss: 33.618435, Train_MMSE: 0.069119, NMMSE: 0.070418, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 19:58:39] Epoch 102/240, Loss: 33.539124, Train_MMSE: 0.069084, NMMSE: 0.070377, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:01:04] Epoch 103/240, Loss: 33.739494, Train_MMSE: 0.069065, NMMSE: 0.070385, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:03:28] Epoch 104/240, Loss: 33.539566, Train_MMSE: 0.069048, NMMSE: 0.07046, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:05:51] Epoch 105/240, Loss: 33.718903, Train_MMSE: 0.069036, NMMSE: 0.070495, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:08:15] Epoch 106/240, Loss: 33.471157, Train_MMSE: 0.069032, NMMSE: 0.070535, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:10:38] Epoch 107/240, Loss: 33.424603, Train_MMSE: 0.069013, NMMSE: 0.070571, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:13:00] Epoch 108/240, Loss: 33.312363, Train_MMSE: 0.06899, NMMSE: 0.070539, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:15:21] Epoch 109/240, Loss: 33.242619, Train_MMSE: 0.068983, NMMSE: 0.070622, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:17:44] Epoch 110/240, Loss: 33.254745, Train_MMSE: 0.068962, NMMSE: 0.070693, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:20:06] Epoch 111/240, Loss: 33.398426, Train_MMSE: 0.068964, NMMSE: 0.070693, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:22:29] Epoch 112/240, Loss: 33.349464, Train_MMSE: 0.068944, NMMSE: 0.070654, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:24:52] Epoch 113/240, Loss: 33.431080, Train_MMSE: 0.068925, NMMSE: 0.07079, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:27:16] Epoch 114/240, Loss: 33.143524, Train_MMSE: 0.068934, NMMSE: 0.070694, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:29:37] Epoch 115/240, Loss: 33.251720, Train_MMSE: 0.068912, NMMSE: 0.07069, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:31:59] Epoch 116/240, Loss: 33.418484, Train_MMSE: 0.068886, NMMSE: 0.07071, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:34:21] Epoch 117/240, Loss: 33.910748, Train_MMSE: 0.068879, NMMSE: 0.070739, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:36:43] Epoch 118/240, Loss: 33.520916, Train_MMSE: 0.068853, NMMSE: 0.070731, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:39:03] Epoch 119/240, Loss: 33.782505, Train_MMSE: 0.068851, NMMSE: 0.070862, LS_NMSE: 0.234713, Lr: 0.0001
[2025-02-24 20:41:26] Epoch 120/240, Loss: 33.476341, Train_MMSE: 0.068828, NMMSE: 0.07084, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:43:48] Epoch 121/240, Loss: 33.469620, Train_MMSE: 0.068469, NMMSE: 0.070715, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:46:09] Epoch 122/240, Loss: 33.158875, Train_MMSE: 0.068407, NMMSE: 0.07077, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:48:31] Epoch 123/240, Loss: 33.263275, Train_MMSE: 0.06839, NMMSE: 0.070774, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:50:52] Epoch 124/240, Loss: 33.575603, Train_MMSE: 0.068402, NMMSE: 0.070787, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:53:13] Epoch 125/240, Loss: 33.262249, Train_MMSE: 0.06839, NMMSE: 0.070844, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:55:33] Epoch 126/240, Loss: 33.211567, Train_MMSE: 0.068366, NMMSE: 0.07083, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 20:57:54] Epoch 127/240, Loss: 33.394672, Train_MMSE: 0.068366, NMMSE: 0.07081, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:00:15] Epoch 128/240, Loss: 33.508327, Train_MMSE: 0.068366, NMMSE: 0.07085, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:02:37] Epoch 129/240, Loss: 33.406902, Train_MMSE: 0.068366, NMMSE: 0.070862, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:04:58] Epoch 130/240, Loss: 33.611084, Train_MMSE: 0.068353, NMMSE: 0.070851, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:07:19] Epoch 131/240, Loss: 33.528332, Train_MMSE: 0.068364, NMMSE: 0.070892, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:09:40] Epoch 132/240, Loss: 33.367161, Train_MMSE: 0.068351, NMMSE: 0.070874, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:12:02] Epoch 133/240, Loss: 33.589363, Train_MMSE: 0.068362, NMMSE: 0.070872, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:14:23] Epoch 134/240, Loss: 33.401333, Train_MMSE: 0.068356, NMMSE: 0.07087, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:16:45] Epoch 135/240, Loss: 33.509251, Train_MMSE: 0.068347, NMMSE: 0.070895, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:19:06] Epoch 136/240, Loss: 33.048428, Train_MMSE: 0.068324, NMMSE: 0.070929, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:21:28] Epoch 137/240, Loss: 33.265949, Train_MMSE: 0.068328, NMMSE: 0.070909, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:23:51] Epoch 138/240, Loss: 33.269466, Train_MMSE: 0.068348, NMMSE: 0.070906, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:26:12] Epoch 139/240, Loss: 33.733295, Train_MMSE: 0.068332, NMMSE: 0.070921, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:28:35] Epoch 140/240, Loss: 33.821491, Train_MMSE: 0.068324, NMMSE: 0.070934, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:30:56] Epoch 141/240, Loss: 33.092072, Train_MMSE: 0.068322, NMMSE: 0.070937, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:33:18] Epoch 142/240, Loss: 33.208809, Train_MMSE: 0.068322, NMMSE: 0.07096, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:35:40] Epoch 143/240, Loss: 33.438480, Train_MMSE: 0.068326, NMMSE: 0.070945, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:38:01] Epoch 144/240, Loss: 33.255253, Train_MMSE: 0.068331, NMMSE: 0.07093, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:40:23] Epoch 145/240, Loss: 33.309406, Train_MMSE: 0.068322, NMMSE: 0.070941, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:42:45] Epoch 146/240, Loss: 33.937885, Train_MMSE: 0.068312, NMMSE: 0.07095, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:45:07] Epoch 147/240, Loss: 33.225571, Train_MMSE: 0.068308, NMMSE: 0.070975, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:47:28] Epoch 148/240, Loss: 33.399490, Train_MMSE: 0.068304, NMMSE: 0.07097, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:49:49] Epoch 149/240, Loss: 33.472523, Train_MMSE: 0.068293, NMMSE: 0.071013, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:52:10] Epoch 150/240, Loss: 33.443134, Train_MMSE: 0.06831, NMMSE: 0.070988, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:54:30] Epoch 151/240, Loss: 33.560184, Train_MMSE: 0.068309, NMMSE: 0.070991, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:56:52] Epoch 152/240, Loss: 33.513287, Train_MMSE: 0.068307, NMMSE: 0.07099, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 21:59:14] Epoch 153/240, Loss: 33.505726, Train_MMSE: 0.068293, NMMSE: 0.070992, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:01:36] Epoch 154/240, Loss: 33.365582, Train_MMSE: 0.068295, NMMSE: 0.070991, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:03:58] Epoch 155/240, Loss: 33.275684, Train_MMSE: 0.068287, NMMSE: 0.071022, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:06:19] Epoch 156/240, Loss: 33.118515, Train_MMSE: 0.068298, NMMSE: 0.071051, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:08:40] Epoch 157/240, Loss: 33.211178, Train_MMSE: 0.068276, NMMSE: 0.071005, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:11:01] Epoch 158/240, Loss: 33.433411, Train_MMSE: 0.068281, NMMSE: 0.071061, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:13:23] Epoch 159/240, Loss: 33.395309, Train_MMSE: 0.06828, NMMSE: 0.071012, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:15:43] Epoch 160/240, Loss: 33.523708, Train_MMSE: 0.068269, NMMSE: 0.071009, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:18:06] Epoch 161/240, Loss: 33.375710, Train_MMSE: 0.068284, NMMSE: 0.070983, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:20:27] Epoch 162/240, Loss: 33.729683, Train_MMSE: 0.068279, NMMSE: 0.071042, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:22:48] Epoch 163/240, Loss: 33.300354, Train_MMSE: 0.06827, NMMSE: 0.071037, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:25:10] Epoch 164/240, Loss: 33.578804, Train_MMSE: 0.068268, NMMSE: 0.071015, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:27:32] Epoch 165/240, Loss: 33.297073, Train_MMSE: 0.068282, NMMSE: 0.071056, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:29:55] Epoch 166/240, Loss: 33.295216, Train_MMSE: 0.06827, NMMSE: 0.071063, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:32:15] Epoch 167/240, Loss: 33.373615, Train_MMSE: 0.068256, NMMSE: 0.071048, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:34:37] Epoch 168/240, Loss: 33.279667, Train_MMSE: 0.068243, NMMSE: 0.071077, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:36:59] Epoch 169/240, Loss: 33.721607, Train_MMSE: 0.068247, NMMSE: 0.071093, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:39:20] Epoch 170/240, Loss: 33.302261, Train_MMSE: 0.068249, NMMSE: 0.071081, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:41:43] Epoch 171/240, Loss: 33.395584, Train_MMSE: 0.068252, NMMSE: 0.071118, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:44:03] Epoch 172/240, Loss: 33.130268, Train_MMSE: 0.068252, NMMSE: 0.071074, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:46:25] Epoch 173/240, Loss: 33.318462, Train_MMSE: 0.06824, NMMSE: 0.071082, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:48:45] Epoch 174/240, Loss: 33.418598, Train_MMSE: 0.068236, NMMSE: 0.071099, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:51:07] Epoch 175/240, Loss: 33.249073, Train_MMSE: 0.068231, NMMSE: 0.071066, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:53:29] Epoch 176/240, Loss: 33.397778, Train_MMSE: 0.068234, NMMSE: 0.071087, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:55:51] Epoch 177/240, Loss: 33.023502, Train_MMSE: 0.068256, NMMSE: 0.07112, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 22:58:13] Epoch 178/240, Loss: 33.282829, Train_MMSE: 0.068245, NMMSE: 0.071088, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 23:00:36] Epoch 179/240, Loss: 33.366791, Train_MMSE: 0.068241, NMMSE: 0.071099, LS_NMSE: 0.234713, Lr: 1e-05
[2025-02-24 23:02:57] Epoch 180/240, Loss: 33.417542, Train_MMSE: 0.068221, NMMSE: 0.071089, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:05:19] Epoch 181/240, Loss: 33.293571, Train_MMSE: 0.068184, NMMSE: 0.071127, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:07:41] Epoch 182/240, Loss: 33.234234, Train_MMSE: 0.068179, NMMSE: 0.071115, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:03] Epoch 183/240, Loss: 33.334507, Train_MMSE: 0.068158, NMMSE: 0.071111, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:12:24] Epoch 184/240, Loss: 33.445995, Train_MMSE: 0.068179, NMMSE: 0.071101, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:14:45] Epoch 185/240, Loss: 33.416737, Train_MMSE: 0.068163, NMMSE: 0.071102, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:17:08] Epoch 186/240, Loss: 33.540596, Train_MMSE: 0.068176, NMMSE: 0.07111, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:19:28] Epoch 187/240, Loss: 33.327335, Train_MMSE: 0.068159, NMMSE: 0.071144, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:21:50] Epoch 188/240, Loss: 33.345284, Train_MMSE: 0.068175, NMMSE: 0.071116, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:24:12] Epoch 189/240, Loss: 33.062038, Train_MMSE: 0.068158, NMMSE: 0.071154, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:26:33] Epoch 190/240, Loss: 33.207966, Train_MMSE: 0.068174, NMMSE: 0.071131, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:28:56] Epoch 191/240, Loss: 33.044579, Train_MMSE: 0.068172, NMMSE: 0.071139, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:31:16] Epoch 192/240, Loss: 33.222752, Train_MMSE: 0.068165, NMMSE: 0.071106, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:33:38] Epoch 193/240, Loss: 33.496307, Train_MMSE: 0.06816, NMMSE: 0.07114, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:35:58] Epoch 194/240, Loss: 33.204365, Train_MMSE: 0.068157, NMMSE: 0.071135, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:38:20] Epoch 195/240, Loss: 33.195454, Train_MMSE: 0.068173, NMMSE: 0.07112, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:40:42] Epoch 196/240, Loss: 33.315357, Train_MMSE: 0.068161, NMMSE: 0.07114, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:43:04] Epoch 197/240, Loss: 33.418179, Train_MMSE: 0.068169, NMMSE: 0.071137, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:45:27] Epoch 198/240, Loss: 33.613663, Train_MMSE: 0.068158, NMMSE: 0.07113, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:47:50] Epoch 199/240, Loss: 33.256622, Train_MMSE: 0.068175, NMMSE: 0.071182, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:50:12] Epoch 200/240, Loss: 33.695255, Train_MMSE: 0.068177, NMMSE: 0.071118, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:52:34] Epoch 201/240, Loss: 33.193909, Train_MMSE: 0.068168, NMMSE: 0.071139, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:54:57] Epoch 202/240, Loss: 33.216167, Train_MMSE: 0.068161, NMMSE: 0.071134, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:57:19] Epoch 203/240, Loss: 33.072067, Train_MMSE: 0.068159, NMMSE: 0.071135, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-24 23:59:42] Epoch 204/240, Loss: 33.309269, Train_MMSE: 0.068164, NMMSE: 0.071131, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:02:04] Epoch 205/240, Loss: 33.209587, Train_MMSE: 0.068152, NMMSE: 0.071111, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:04:26] Epoch 206/240, Loss: 33.208225, Train_MMSE: 0.068176, NMMSE: 0.071121, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:06:49] Epoch 207/240, Loss: 33.183414, Train_MMSE: 0.068135, NMMSE: 0.071127, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:09:11] Epoch 208/240, Loss: 33.203453, Train_MMSE: 0.068164, NMMSE: 0.071141, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:11:33] Epoch 209/240, Loss: 33.319405, Train_MMSE: 0.06816, NMMSE: 0.07113, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:13:56] Epoch 210/240, Loss: 33.107258, Train_MMSE: 0.068174, NMMSE: 0.071125, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:16:16] Epoch 211/240, Loss: 33.326981, Train_MMSE: 0.068181, NMMSE: 0.071136, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:18:39] Epoch 212/240, Loss: 33.278957, Train_MMSE: 0.068138, NMMSE: 0.071119, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:21:00] Epoch 213/240, Loss: 33.324577, Train_MMSE: 0.068167, NMMSE: 0.071131, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:23:21] Epoch 214/240, Loss: 33.326904, Train_MMSE: 0.068173, NMMSE: 0.071151, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:25:43] Epoch 215/240, Loss: 33.472218, Train_MMSE: 0.068153, NMMSE: 0.07113, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:28:04] Epoch 216/240, Loss: 33.291470, Train_MMSE: 0.068166, NMMSE: 0.071116, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:30:26] Epoch 217/240, Loss: 33.184689, Train_MMSE: 0.068157, NMMSE: 0.071128, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:32:51] Epoch 218/240, Loss: 33.502022, Train_MMSE: 0.068162, NMMSE: 0.07112, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:13] Epoch 219/240, Loss: 33.246426, Train_MMSE: 0.068174, NMMSE: 0.071133, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:37:35] Epoch 220/240, Loss: 33.726406, Train_MMSE: 0.068166, NMMSE: 0.07113, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:59] Epoch 221/240, Loss: 33.608051, Train_MMSE: 0.068159, NMMSE: 0.071133, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:21] Epoch 222/240, Loss: 33.690769, Train_MMSE: 0.068155, NMMSE: 0.071128, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:44] Epoch 223/240, Loss: 33.203346, Train_MMSE: 0.068157, NMMSE: 0.071176, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:06] Epoch 224/240, Loss: 33.657024, Train_MMSE: 0.068157, NMMSE: 0.071157, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:49:27] Epoch 225/240, Loss: 33.238457, Train_MMSE: 0.068154, NMMSE: 0.071151, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:51:50] Epoch 226/240, Loss: 33.555260, Train_MMSE: 0.068164, NMMSE: 0.071122, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:54:11] Epoch 227/240, Loss: 33.410320, Train_MMSE: 0.068145, NMMSE: 0.071173, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:56:33] Epoch 228/240, Loss: 33.564808, Train_MMSE: 0.068163, NMMSE: 0.071124, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 00:58:55] Epoch 229/240, Loss: 33.077778, Train_MMSE: 0.068148, NMMSE: 0.071141, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:01:16] Epoch 230/240, Loss: 32.988125, Train_MMSE: 0.068133, NMMSE: 0.071171, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:03:39] Epoch 231/240, Loss: 33.357174, Train_MMSE: 0.068147, NMMSE: 0.071154, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:06:01] Epoch 232/240, Loss: 33.521946, Train_MMSE: 0.068165, NMMSE: 0.071138, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:08:22] Epoch 233/240, Loss: 33.359116, Train_MMSE: 0.068159, NMMSE: 0.07115, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:10:45] Epoch 234/240, Loss: 33.442078, Train_MMSE: 0.06815, NMMSE: 0.071141, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:13:03] Epoch 235/240, Loss: 33.305031, Train_MMSE: 0.068156, NMMSE: 0.0712, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:14:53] Epoch 236/240, Loss: 33.069225, Train_MMSE: 0.06816, NMMSE: 0.071124, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:16:39] Epoch 237/240, Loss: 33.306675, Train_MMSE: 0.068156, NMMSE: 0.071189, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:18:24] Epoch 238/240, Loss: 33.491432, Train_MMSE: 0.068144, NMMSE: 0.07115, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:20:08] Epoch 239/240, Loss: 33.160473, Train_MMSE: 0.068152, NMMSE: 0.071185, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-06
[2025-02-25 01:21:34] Epoch 240/240, Loss: 33.210213, Train_MMSE: 0.068143, NMMSE: 0.071139, LS_NMSE: 0.234713, Lr: 1.0000000000000002e-07
