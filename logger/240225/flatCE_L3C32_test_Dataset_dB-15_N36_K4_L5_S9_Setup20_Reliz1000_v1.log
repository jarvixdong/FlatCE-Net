Train.py PID: 10977

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.04812557817749213
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L5_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L5_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/flatCE_L3C32_test_Dataset_dB-15_N36_K4_L5_S9_Setup20_Reliz1000_v1.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'MSELoss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.01, 'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fc6e766e420>
loss function:: MSELoss()
[2025-02-25 12:39:20] Epoch 1/240, Loss: 1911.075806, Train_MMSE: 0.174006, NMMSE: 0.066673, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:39:37] Epoch 2/240, Loss: 1814.782104, Train_MMSE: 0.066403, NMMSE: 0.064696, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:39:53] Epoch 3/240, Loss: 1784.068970, Train_MMSE: 0.064415, NMMSE: 0.062507, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:40:10] Epoch 4/240, Loss: 1724.034790, Train_MMSE: 0.063611, NMMSE: 0.063673, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:40:26] Epoch 5/240, Loss: 1715.000366, Train_MMSE: 0.063103, NMMSE: 0.062235, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:40:42] Epoch 6/240, Loss: 1711.267944, Train_MMSE: 0.06277, NMMSE: 0.06084, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:40:59] Epoch 7/240, Loss: 1702.440063, Train_MMSE: 0.062627, NMMSE: 0.060379, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:41:15] Epoch 8/240, Loss: 1784.337891, Train_MMSE: 0.062389, NMMSE: 0.061287, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:41:34] Epoch 9/240, Loss: 1706.961060, Train_MMSE: 0.062262, NMMSE: 0.060375, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:41:50] Epoch 10/240, Loss: 1736.797363, Train_MMSE: 0.062111, NMMSE: 0.060938, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:42:06] Epoch 11/240, Loss: 1720.571045, Train_MMSE: 0.062085, NMMSE: 0.06062, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:42:22] Epoch 12/240, Loss: 1742.451660, Train_MMSE: 0.061891, NMMSE: 0.061213, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:42:38] Epoch 13/240, Loss: 1677.652222, Train_MMSE: 0.061872, NMMSE: 0.06041, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:42:55] Epoch 14/240, Loss: 1711.714478, Train_MMSE: 0.06178, NMMSE: 0.061288, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:43:11] Epoch 15/240, Loss: 1676.519165, Train_MMSE: 0.061737, NMMSE: 0.060229, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:43:27] Epoch 16/240, Loss: 1701.015381, Train_MMSE: 0.061692, NMMSE: 0.060504, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:43:43] Epoch 17/240, Loss: 1681.864624, Train_MMSE: 0.061622, NMMSE: 0.060592, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:43:59] Epoch 18/240, Loss: 1702.748901, Train_MMSE: 0.061495, NMMSE: 0.060237, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:44:15] Epoch 19/240, Loss: 1696.717407, Train_MMSE: 0.06141, NMMSE: 0.059543, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:44:31] Epoch 20/240, Loss: 1692.624878, Train_MMSE: 0.061436, NMMSE: 0.06007, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:44:47] Epoch 21/240, Loss: 1711.747192, Train_MMSE: 0.061415, NMMSE: 0.059936, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:45:03] Epoch 22/240, Loss: 1698.778687, Train_MMSE: 0.061394, NMMSE: 0.060946, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:45:20] Epoch 23/240, Loss: 1700.383423, Train_MMSE: 0.061331, NMMSE: 0.05977, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:45:36] Epoch 24/240, Loss: 1686.598022, Train_MMSE: 0.061279, NMMSE: 0.060061, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:45:52] Epoch 25/240, Loss: 1661.460938, Train_MMSE: 0.061247, NMMSE: 0.061108, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:46:08] Epoch 26/240, Loss: 1695.636719, Train_MMSE: 0.061244, NMMSE: 0.05975, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:46:24] Epoch 27/240, Loss: 1690.148560, Train_MMSE: 0.061221, NMMSE: 0.060165, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:46:41] Epoch 28/240, Loss: 1714.763306, Train_MMSE: 0.061151, NMMSE: 0.059868, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:46:57] Epoch 29/240, Loss: 1669.668701, Train_MMSE: 0.061095, NMMSE: 0.060074, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:47:13] Epoch 30/240, Loss: 1687.121704, Train_MMSE: 0.061081, NMMSE: 0.060424, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:47:29] Epoch 31/240, Loss: 1665.808105, Train_MMSE: 0.061123, NMMSE: 0.060955, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:47:46] Epoch 32/240, Loss: 1673.478149, Train_MMSE: 0.061057, NMMSE: 0.060399, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:48:02] Epoch 33/240, Loss: 1683.104126, Train_MMSE: 0.061022, NMMSE: 0.059436, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:48:18] Epoch 34/240, Loss: 1642.678589, Train_MMSE: 0.061016, NMMSE: 0.05972, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:48:34] Epoch 35/240, Loss: 1684.380493, Train_MMSE: 0.060959, NMMSE: 0.05955, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:48:51] Epoch 36/240, Loss: 1668.427368, Train_MMSE: 0.060988, NMMSE: 0.059766, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:49:07] Epoch 37/240, Loss: 1653.770874, Train_MMSE: 0.060911, NMMSE: 0.059724, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:49:24] Epoch 38/240, Loss: 1722.304688, Train_MMSE: 0.060936, NMMSE: 0.059641, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:49:40] Epoch 39/240, Loss: 1688.470581, Train_MMSE: 0.060905, NMMSE: 0.0617, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:49:57] Epoch 40/240, Loss: 1655.094238, Train_MMSE: 0.060861, NMMSE: 0.060032, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:50:13] Epoch 41/240, Loss: 1695.364746, Train_MMSE: 0.060807, NMMSE: 0.059906, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:50:29] Epoch 42/240, Loss: 1649.591431, Train_MMSE: 0.060895, NMMSE: 0.060102, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:50:45] Epoch 43/240, Loss: 1659.763062, Train_MMSE: 0.060824, NMMSE: 0.0596, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:51:02] Epoch 44/240, Loss: 1687.925171, Train_MMSE: 0.060799, NMMSE: 0.059631, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:51:18] Epoch 45/240, Loss: 1652.816284, Train_MMSE: 0.060746, NMMSE: 0.060399, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:51:34] Epoch 46/240, Loss: 1689.771729, Train_MMSE: 0.06076, NMMSE: 0.059831, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:51:51] Epoch 47/240, Loss: 1694.272095, Train_MMSE: 0.060754, NMMSE: 0.059955, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:52:07] Epoch 48/240, Loss: 1681.278931, Train_MMSE: 0.060739, NMMSE: 0.05996, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:52:24] Epoch 49/240, Loss: 1668.416870, Train_MMSE: 0.060737, NMMSE: 0.060367, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:52:40] Epoch 50/240, Loss: 1698.551025, Train_MMSE: 0.060723, NMMSE: 0.060239, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:52:56] Epoch 51/240, Loss: 1666.480469, Train_MMSE: 0.060707, NMMSE: 0.059511, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:53:13] Epoch 52/240, Loss: 1659.027710, Train_MMSE: 0.060685, NMMSE: 0.059902, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:53:29] Epoch 53/240, Loss: 1707.062866, Train_MMSE: 0.060649, NMMSE: 0.060259, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:53:46] Epoch 54/240, Loss: 1634.076416, Train_MMSE: 0.060634, NMMSE: 0.059642, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:54:02] Epoch 55/240, Loss: 1628.085327, Train_MMSE: 0.060603, NMMSE: 0.059729, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:54:19] Epoch 56/240, Loss: 1670.995972, Train_MMSE: 0.060587, NMMSE: 0.059952, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:54:35] Epoch 57/240, Loss: 1674.777222, Train_MMSE: 0.060628, NMMSE: 0.061032, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:54:52] Epoch 58/240, Loss: 1678.484741, Train_MMSE: 0.060563, NMMSE: 0.059809, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:55:08] Epoch 59/240, Loss: 1693.875000, Train_MMSE: 0.060556, NMMSE: 0.060027, LS_NMSE: 0.141043, Lr: 0.01
[2025-02-25 12:55:25] Epoch 60/240, Loss: 1711.397827, Train_MMSE: 0.060565, NMMSE: 0.059756, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:55:41] Epoch 61/240, Loss: 1620.051392, Train_MMSE: 0.059441, NMMSE: 0.058748, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:55:58] Epoch 62/240, Loss: 1598.722900, Train_MMSE: 0.059265, NMMSE: 0.058817, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:56:15] Epoch 63/240, Loss: 1615.316772, Train_MMSE: 0.059216, NMMSE: 0.058871, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:56:31] Epoch 64/240, Loss: 1619.899902, Train_MMSE: 0.059167, NMMSE: 0.058863, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:56:48] Epoch 65/240, Loss: 1677.283691, Train_MMSE: 0.05915, NMMSE: 0.058892, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:57:04] Epoch 66/240, Loss: 1633.211792, Train_MMSE: 0.059125, NMMSE: 0.058948, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:57:21] Epoch 67/240, Loss: 1637.312744, Train_MMSE: 0.059085, NMMSE: 0.058995, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:57:37] Epoch 68/240, Loss: 1633.061890, Train_MMSE: 0.05905, NMMSE: 0.058985, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:57:53] Epoch 69/240, Loss: 1616.359741, Train_MMSE: 0.059041, NMMSE: 0.059022, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:58:09] Epoch 70/240, Loss: 1610.002319, Train_MMSE: 0.059024, NMMSE: 0.059025, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:58:25] Epoch 71/240, Loss: 1647.054932, Train_MMSE: 0.058984, NMMSE: 0.059074, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:58:41] Epoch 72/240, Loss: 1606.801636, Train_MMSE: 0.058966, NMMSE: 0.059096, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:58:57] Epoch 73/240, Loss: 1620.050537, Train_MMSE: 0.058942, NMMSE: 0.059081, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:59:14] Epoch 74/240, Loss: 1613.376587, Train_MMSE: 0.058919, NMMSE: 0.059219, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:59:30] Epoch 75/240, Loss: 1617.973389, Train_MMSE: 0.058913, NMMSE: 0.059171, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 12:59:46] Epoch 76/240, Loss: 1602.475098, Train_MMSE: 0.058885, NMMSE: 0.059241, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:00:02] Epoch 77/240, Loss: 1619.953857, Train_MMSE: 0.058876, NMMSE: 0.059192, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:00:19] Epoch 78/240, Loss: 1587.897339, Train_MMSE: 0.058856, NMMSE: 0.059276, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:00:35] Epoch 79/240, Loss: 1624.043457, Train_MMSE: 0.05883, NMMSE: 0.059351, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:00:52] Epoch 80/240, Loss: 1608.213501, Train_MMSE: 0.058809, NMMSE: 0.059384, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:01:08] Epoch 81/240, Loss: 1627.413940, Train_MMSE: 0.058797, NMMSE: 0.059295, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:01:25] Epoch 82/240, Loss: 1625.029541, Train_MMSE: 0.058774, NMMSE: 0.059284, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:01:41] Epoch 83/240, Loss: 1623.131104, Train_MMSE: 0.058763, NMMSE: 0.059364, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:01:57] Epoch 84/240, Loss: 1613.198975, Train_MMSE: 0.058743, NMMSE: 0.059364, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:02:14] Epoch 85/240, Loss: 1593.361084, Train_MMSE: 0.058724, NMMSE: 0.059426, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:02:30] Epoch 86/240, Loss: 1571.813721, Train_MMSE: 0.058711, NMMSE: 0.059463, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:02:46] Epoch 87/240, Loss: 1607.243652, Train_MMSE: 0.058702, NMMSE: 0.059403, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:03:03] Epoch 88/240, Loss: 1613.817505, Train_MMSE: 0.058661, NMMSE: 0.059414, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:03:19] Epoch 89/240, Loss: 1605.735474, Train_MMSE: 0.058644, NMMSE: 0.059492, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:03:35] Epoch 90/240, Loss: 1594.143188, Train_MMSE: 0.058625, NMMSE: 0.059425, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:03:51] Epoch 91/240, Loss: 1601.095337, Train_MMSE: 0.058626, NMMSE: 0.059535, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:04:07] Epoch 92/240, Loss: 1612.324463, Train_MMSE: 0.058599, NMMSE: 0.059541, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:04:24] Epoch 93/240, Loss: 1610.585571, Train_MMSE: 0.058582, NMMSE: 0.059541, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:04:40] Epoch 94/240, Loss: 1627.995728, Train_MMSE: 0.058578, NMMSE: 0.059703, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:04:57] Epoch 95/240, Loss: 1619.879761, Train_MMSE: 0.058531, NMMSE: 0.059528, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:05:18] Epoch 96/240, Loss: 1620.081299, Train_MMSE: 0.058525, NMMSE: 0.059775, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:05:41] Epoch 97/240, Loss: 1608.684204, Train_MMSE: 0.058513, NMMSE: 0.059597, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:06:08] Epoch 98/240, Loss: 1613.259644, Train_MMSE: 0.058489, NMMSE: 0.059618, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:06:44] Epoch 99/240, Loss: 1643.043579, Train_MMSE: 0.058479, NMMSE: 0.059578, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:07:19] Epoch 100/240, Loss: 1614.172119, Train_MMSE: 0.058455, NMMSE: 0.059784, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:07:56] Epoch 101/240, Loss: 1589.836426, Train_MMSE: 0.058441, NMMSE: 0.059733, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:08:31] Epoch 102/240, Loss: 1601.913086, Train_MMSE: 0.058423, NMMSE: 0.059729, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:09:06] Epoch 103/240, Loss: 1606.858765, Train_MMSE: 0.058403, NMMSE: 0.059816, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:09:40] Epoch 104/240, Loss: 1585.993896, Train_MMSE: 0.058389, NMMSE: 0.059813, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:10:15] Epoch 105/240, Loss: 1610.395752, Train_MMSE: 0.058362, NMMSE: 0.059862, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:10:49] Epoch 106/240, Loss: 1589.725220, Train_MMSE: 0.058357, NMMSE: 0.059727, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:11:24] Epoch 107/240, Loss: 1589.573242, Train_MMSE: 0.058326, NMMSE: 0.059836, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:11:58] Epoch 108/240, Loss: 1590.811157, Train_MMSE: 0.058351, NMMSE: 0.059777, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:12:33] Epoch 109/240, Loss: 1568.179932, Train_MMSE: 0.058303, NMMSE: 0.059774, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:13:07] Epoch 110/240, Loss: 1585.271362, Train_MMSE: 0.05832, NMMSE: 0.059856, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:13:43] Epoch 111/240, Loss: 1583.250000, Train_MMSE: 0.058292, NMMSE: 0.06005, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:14:18] Epoch 112/240, Loss: 1620.908813, Train_MMSE: 0.058261, NMMSE: 0.05993, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:14:54] Epoch 113/240, Loss: 1596.296509, Train_MMSE: 0.058225, NMMSE: 0.059882, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:15:28] Epoch 114/240, Loss: 1597.379639, Train_MMSE: 0.058212, NMMSE: 0.059836, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:16:04] Epoch 115/240, Loss: 1600.462524, Train_MMSE: 0.058222, NMMSE: 0.060063, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:16:39] Epoch 116/240, Loss: 1616.994873, Train_MMSE: 0.058185, NMMSE: 0.060115, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:17:13] Epoch 117/240, Loss: 1600.880615, Train_MMSE: 0.058164, NMMSE: 0.060122, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:17:47] Epoch 118/240, Loss: 1601.053223, Train_MMSE: 0.058171, NMMSE: 0.060049, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:18:22] Epoch 119/240, Loss: 1613.749634, Train_MMSE: 0.058161, NMMSE: 0.060319, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-25 13:18:56] Epoch 120/240, Loss: 1610.117554, Train_MMSE: 0.058144, NMMSE: 0.060113, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:19:30] Epoch 121/240, Loss: 1626.343750, Train_MMSE: 0.057782, NMMSE: 0.060207, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:20:05] Epoch 122/240, Loss: 1564.306641, Train_MMSE: 0.057705, NMMSE: 0.060255, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:20:40] Epoch 123/240, Loss: 1591.993774, Train_MMSE: 0.057673, NMMSE: 0.060278, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:21:14] Epoch 124/240, Loss: 1567.931519, Train_MMSE: 0.057681, NMMSE: 0.060324, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:21:49] Epoch 125/240, Loss: 1568.579834, Train_MMSE: 0.057662, NMMSE: 0.060357, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:22:23] Epoch 126/240, Loss: 1589.890137, Train_MMSE: 0.057658, NMMSE: 0.060367, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:22:58] Epoch 127/240, Loss: 1601.921387, Train_MMSE: 0.057643, NMMSE: 0.060392, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:23:33] Epoch 128/240, Loss: 1604.558228, Train_MMSE: 0.057652, NMMSE: 0.060375, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:24:07] Epoch 129/240, Loss: 1585.297119, Train_MMSE: 0.057637, NMMSE: 0.060397, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:24:41] Epoch 130/240, Loss: 1569.760986, Train_MMSE: 0.057624, NMMSE: 0.060464, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:25:16] Epoch 131/240, Loss: 1626.396484, Train_MMSE: 0.057614, NMMSE: 0.06038, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:25:50] Epoch 132/240, Loss: 1582.424561, Train_MMSE: 0.057624, NMMSE: 0.060432, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:26:24] Epoch 133/240, Loss: 1603.521118, Train_MMSE: 0.057599, NMMSE: 0.060411, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:26:59] Epoch 134/240, Loss: 1589.654907, Train_MMSE: 0.05762, NMMSE: 0.060401, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:27:33] Epoch 135/240, Loss: 1591.119385, Train_MMSE: 0.057597, NMMSE: 0.060425, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:28:08] Epoch 136/240, Loss: 1574.492798, Train_MMSE: 0.057602, NMMSE: 0.060439, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:28:42] Epoch 137/240, Loss: 1579.987549, Train_MMSE: 0.057585, NMMSE: 0.06049, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:29:17] Epoch 138/240, Loss: 1583.276123, Train_MMSE: 0.057578, NMMSE: 0.060466, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:29:51] Epoch 139/240, Loss: 1571.347290, Train_MMSE: 0.057584, NMMSE: 0.060468, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:30:26] Epoch 140/240, Loss: 1597.290771, Train_MMSE: 0.05757, NMMSE: 0.060538, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:31:00] Epoch 141/240, Loss: 1587.268799, Train_MMSE: 0.057569, NMMSE: 0.06047, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:31:35] Epoch 142/240, Loss: 1571.732544, Train_MMSE: 0.057556, NMMSE: 0.060505, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:32:09] Epoch 143/240, Loss: 1566.440308, Train_MMSE: 0.057574, NMMSE: 0.060549, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:32:44] Epoch 144/240, Loss: 1560.185181, Train_MMSE: 0.057567, NMMSE: 0.060515, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:33:18] Epoch 145/240, Loss: 1551.712646, Train_MMSE: 0.057558, NMMSE: 0.060497, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:33:53] Epoch 146/240, Loss: 1580.702515, Train_MMSE: 0.057538, NMMSE: 0.060468, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:34:29] Epoch 147/240, Loss: 1583.382324, Train_MMSE: 0.057541, NMMSE: 0.060523, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:35:03] Epoch 148/240, Loss: 1569.027344, Train_MMSE: 0.057541, NMMSE: 0.060489, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:35:42] Epoch 149/240, Loss: 1590.787598, Train_MMSE: 0.057533, NMMSE: 0.060519, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-25 13:36:24] Epoch 150/240, Loss: 1586.520142, Train_MMSE: 0.057533, NMMSE: 0.060521, LS_NMSE: 0.141043, Lr: 0.0001
