Train.py PID: 2553

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.04812557817749213
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L5_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L5_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L5_S9_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f5e5a466120>
loss function:: SmoothL1Loss()
[2025-02-24 15:47:24] Epoch 1/240, Loss: 38.314999, Train_MMSE: 0.108387, NMMSE: 0.086498, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 15:48:42] Epoch 2/240, Loss: 35.638905, Train_MMSE: 0.083602, NMMSE: 0.076866, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 15:50:34] Epoch 3/240, Loss: 34.662037, Train_MMSE: 0.077525, NMMSE: 0.073942, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 15:52:37] Epoch 4/240, Loss: 34.714001, Train_MMSE: 0.07514, NMMSE: 0.072311, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 15:55:03] Epoch 5/240, Loss: 34.052818, Train_MMSE: 0.073881, NMMSE: 0.071657, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 15:57:38] Epoch 6/240, Loss: 34.328491, Train_MMSE: 0.073009, NMMSE: 0.07116, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:00:12] Epoch 7/240, Loss: 34.080589, Train_MMSE: 0.072392, NMMSE: 0.070657, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:02:42] Epoch 8/240, Loss: 33.674511, Train_MMSE: 0.071931, NMMSE: 0.071268, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:05:05] Epoch 9/240, Loss: 33.709442, Train_MMSE: 0.071618, NMMSE: 0.069991, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:07:27] Epoch 10/240, Loss: 33.619453, Train_MMSE: 0.071278, NMMSE: 0.070107, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:09:50] Epoch 11/240, Loss: 33.317726, Train_MMSE: 0.071014, NMMSE: 0.069714, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:12:14] Epoch 12/240, Loss: 33.068954, Train_MMSE: 0.070729, NMMSE: 0.070013, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:14:37] Epoch 13/240, Loss: 33.790180, Train_MMSE: 0.070538, NMMSE: 0.069194, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:17:01] Epoch 14/240, Loss: 33.114067, Train_MMSE: 0.070321, NMMSE: 0.069286, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:19:23] Epoch 15/240, Loss: 33.720959, Train_MMSE: 0.070122, NMMSE: 0.069167, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:21:47] Epoch 16/240, Loss: 33.195171, Train_MMSE: 0.069979, NMMSE: 0.069066, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:24:11] Epoch 17/240, Loss: 33.329967, Train_MMSE: 0.069803, NMMSE: 0.068866, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:26:34] Epoch 18/240, Loss: 33.414913, Train_MMSE: 0.069679, NMMSE: 0.068995, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:28:56] Epoch 19/240, Loss: 32.904457, Train_MMSE: 0.069543, NMMSE: 0.068688, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:31:20] Epoch 20/240, Loss: 33.201130, Train_MMSE: 0.069388, NMMSE: 0.068511, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:33:43] Epoch 21/240, Loss: 33.476387, Train_MMSE: 0.069232, NMMSE: 0.068559, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:36:06] Epoch 22/240, Loss: 33.035587, Train_MMSE: 0.069111, NMMSE: 0.068353, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:38:29] Epoch 23/240, Loss: 32.854134, Train_MMSE: 0.06902, NMMSE: 0.068333, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:40:53] Epoch 24/240, Loss: 33.101933, Train_MMSE: 0.068892, NMMSE: 0.068464, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:43:17] Epoch 25/240, Loss: 32.900028, Train_MMSE: 0.06879, NMMSE: 0.068271, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:45:40] Epoch 26/240, Loss: 33.019867, Train_MMSE: 0.068663, NMMSE: 0.068267, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:48:03] Epoch 27/240, Loss: 33.271748, Train_MMSE: 0.068558, NMMSE: 0.068471, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:50:26] Epoch 28/240, Loss: 32.580444, Train_MMSE: 0.068474, NMMSE: 0.068152, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:52:50] Epoch 29/240, Loss: 33.180977, Train_MMSE: 0.068376, NMMSE: 0.067893, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:55:14] Epoch 30/240, Loss: 32.824226, Train_MMSE: 0.068245, NMMSE: 0.067773, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:57:37] Epoch 31/240, Loss: 32.703526, Train_MMSE: 0.068156, NMMSE: 0.067951, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 16:59:59] Epoch 32/240, Loss: 32.961533, Train_MMSE: 0.068027, NMMSE: 0.0676, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:02:23] Epoch 33/240, Loss: 32.781025, Train_MMSE: 0.067954, NMMSE: 0.067597, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:04:46] Epoch 34/240, Loss: 32.762852, Train_MMSE: 0.067834, NMMSE: 0.067699, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:07:10] Epoch 35/240, Loss: 32.424942, Train_MMSE: 0.067733, NMMSE: 0.067508, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:09:32] Epoch 36/240, Loss: 33.060452, Train_MMSE: 0.067626, NMMSE: 0.0677, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:11:56] Epoch 37/240, Loss: 32.670002, Train_MMSE: 0.067531, NMMSE: 0.06734, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:14:20] Epoch 38/240, Loss: 32.850079, Train_MMSE: 0.067446, NMMSE: 0.067523, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:16:42] Epoch 39/240, Loss: 32.706326, Train_MMSE: 0.067323, NMMSE: 0.067018, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:19:06] Epoch 40/240, Loss: 32.285984, Train_MMSE: 0.067252, NMMSE: 0.067106, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:21:28] Epoch 41/240, Loss: 32.593082, Train_MMSE: 0.067124, NMMSE: 0.067286, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:23:52] Epoch 42/240, Loss: 32.544918, Train_MMSE: 0.067075, NMMSE: 0.066973, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:26:16] Epoch 43/240, Loss: 32.683743, Train_MMSE: 0.066982, NMMSE: 0.067029, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:28:39] Epoch 44/240, Loss: 32.644760, Train_MMSE: 0.066915, NMMSE: 0.067183, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:31:04] Epoch 45/240, Loss: 32.659252, Train_MMSE: 0.066801, NMMSE: 0.06683, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:33:28] Epoch 46/240, Loss: 32.680950, Train_MMSE: 0.066728, NMMSE: 0.067025, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:35:52] Epoch 47/240, Loss: 32.646675, Train_MMSE: 0.066616, NMMSE: 0.067145, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:38:17] Epoch 48/240, Loss: 32.811581, Train_MMSE: 0.066586, NMMSE: 0.066444, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:40:42] Epoch 49/240, Loss: 32.465694, Train_MMSE: 0.066471, NMMSE: 0.066472, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:43:04] Epoch 50/240, Loss: 32.498871, Train_MMSE: 0.066425, NMMSE: 0.066388, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:45:29] Epoch 51/240, Loss: 32.642178, Train_MMSE: 0.066329, NMMSE: 0.066521, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:47:52] Epoch 52/240, Loss: 32.328728, Train_MMSE: 0.066266, NMMSE: 0.066426, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:50:16] Epoch 53/240, Loss: 32.337986, Train_MMSE: 0.066178, NMMSE: 0.066017, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:52:39] Epoch 54/240, Loss: 32.336128, Train_MMSE: 0.066131, NMMSE: 0.066245, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:55:02] Epoch 55/240, Loss: 32.254456, Train_MMSE: 0.066066, NMMSE: 0.066523, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:57:25] Epoch 56/240, Loss: 32.349113, Train_MMSE: 0.065997, NMMSE: 0.06646, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 17:59:47] Epoch 57/240, Loss: 32.764423, Train_MMSE: 0.065916, NMMSE: 0.06592, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 18:02:10] Epoch 58/240, Loss: 32.384857, Train_MMSE: 0.065867, NMMSE: 0.066244, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 18:04:32] Epoch 59/240, Loss: 32.309616, Train_MMSE: 0.065812, NMMSE: 0.06627, LS_NMSE: 0.141043, Lr: 0.001
[2025-02-24 18:06:56] Epoch 60/240, Loss: 32.565735, Train_MMSE: 0.065735, NMMSE: 0.066116, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:09:19] Epoch 61/240, Loss: 31.707743, Train_MMSE: 0.063197, NMMSE: 0.064008, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:11:44] Epoch 62/240, Loss: 31.449503, Train_MMSE: 0.062597, NMMSE: 0.064027, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:14:06] Epoch 63/240, Loss: 31.358341, Train_MMSE: 0.062329, NMMSE: 0.064186, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:16:30] Epoch 64/240, Loss: 31.164253, Train_MMSE: 0.06212, NMMSE: 0.064279, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:18:54] Epoch 65/240, Loss: 31.113447, Train_MMSE: 0.061952, NMMSE: 0.064359, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:21:21] Epoch 66/240, Loss: 31.137604, Train_MMSE: 0.061784, NMMSE: 0.064461, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:23:44] Epoch 67/240, Loss: 31.200775, Train_MMSE: 0.061645, NMMSE: 0.064465, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:26:08] Epoch 68/240, Loss: 31.102386, Train_MMSE: 0.061517, NMMSE: 0.064567, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:28:32] Epoch 69/240, Loss: 30.960358, Train_MMSE: 0.06138, NMMSE: 0.064598, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:30:55] Epoch 70/240, Loss: 30.956713, Train_MMSE: 0.061255, NMMSE: 0.064666, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:33:18] Epoch 71/240, Loss: 31.406420, Train_MMSE: 0.061147, NMMSE: 0.064761, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:35:42] Epoch 72/240, Loss: 30.943668, Train_MMSE: 0.061024, NMMSE: 0.064791, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:38:06] Epoch 73/240, Loss: 30.847591, Train_MMSE: 0.060926, NMMSE: 0.064917, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:40:29] Epoch 74/240, Loss: 31.158220, Train_MMSE: 0.060823, NMMSE: 0.064933, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:42:53] Epoch 75/240, Loss: 30.905107, Train_MMSE: 0.060727, NMMSE: 0.065046, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:45:16] Epoch 76/240, Loss: 30.580109, Train_MMSE: 0.060632, NMMSE: 0.065109, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:47:41] Epoch 77/240, Loss: 30.798096, Train_MMSE: 0.060533, NMMSE: 0.065282, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:50:05] Epoch 78/240, Loss: 30.996634, Train_MMSE: 0.06045, NMMSE: 0.065346, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:52:28] Epoch 79/240, Loss: 30.766649, Train_MMSE: 0.060354, NMMSE: 0.065383, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:54:49] Epoch 80/240, Loss: 30.674608, Train_MMSE: 0.06027, NMMSE: 0.065432, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:57:11] Epoch 81/240, Loss: 30.842607, Train_MMSE: 0.060195, NMMSE: 0.065442, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 18:59:34] Epoch 82/240, Loss: 30.943394, Train_MMSE: 0.060107, NMMSE: 0.065554, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:01:56] Epoch 83/240, Loss: 30.596521, Train_MMSE: 0.060021, NMMSE: 0.065648, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:04:20] Epoch 84/240, Loss: 30.802969, Train_MMSE: 0.059945, NMMSE: 0.065687, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:06:42] Epoch 85/240, Loss: 30.899405, Train_MMSE: 0.059872, NMMSE: 0.065891, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:09:06] Epoch 86/240, Loss: 30.624125, Train_MMSE: 0.059791, NMMSE: 0.065805, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:11:29] Epoch 87/240, Loss: 30.521519, Train_MMSE: 0.059712, NMMSE: 0.06593, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:13:53] Epoch 88/240, Loss: 30.441854, Train_MMSE: 0.059651, NMMSE: 0.066091, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:16:16] Epoch 89/240, Loss: 30.412737, Train_MMSE: 0.059576, NMMSE: 0.066051, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:18:40] Epoch 90/240, Loss: 30.615824, Train_MMSE: 0.059511, NMMSE: 0.066151, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:21:03] Epoch 91/240, Loss: 30.528093, Train_MMSE: 0.059443, NMMSE: 0.066135, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:23:27] Epoch 92/240, Loss: 30.429670, Train_MMSE: 0.059372, NMMSE: 0.066237, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:25:51] Epoch 93/240, Loss: 30.529671, Train_MMSE: 0.059306, NMMSE: 0.066297, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:28:14] Epoch 94/240, Loss: 30.675104, Train_MMSE: 0.059241, NMMSE: 0.06627, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:30:39] Epoch 95/240, Loss: 30.286825, Train_MMSE: 0.059177, NMMSE: 0.066445, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:33:05] Epoch 96/240, Loss: 30.416033, Train_MMSE: 0.059121, NMMSE: 0.066456, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:35:29] Epoch 97/240, Loss: 30.401363, Train_MMSE: 0.059042, NMMSE: 0.066569, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:37:54] Epoch 98/240, Loss: 30.387537, Train_MMSE: 0.058994, NMMSE: 0.066628, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:40:17] Epoch 99/240, Loss: 30.366709, Train_MMSE: 0.058931, NMMSE: 0.066688, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:42:42] Epoch 100/240, Loss: 30.398462, Train_MMSE: 0.05888, NMMSE: 0.066624, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:45:05] Epoch 101/240, Loss: 30.699080, Train_MMSE: 0.058818, NMMSE: 0.066748, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:47:30] Epoch 102/240, Loss: 30.174408, Train_MMSE: 0.058757, NMMSE: 0.066974, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:49:55] Epoch 103/240, Loss: 30.283020, Train_MMSE: 0.058693, NMMSE: 0.066929, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:52:20] Epoch 104/240, Loss: 30.245701, Train_MMSE: 0.058632, NMMSE: 0.066899, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:54:44] Epoch 105/240, Loss: 30.417044, Train_MMSE: 0.058581, NMMSE: 0.067027, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:57:07] Epoch 106/240, Loss: 30.489960, Train_MMSE: 0.058527, NMMSE: 0.067084, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 19:59:30] Epoch 107/240, Loss: 30.159515, Train_MMSE: 0.058485, NMMSE: 0.067134, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:01:54] Epoch 108/240, Loss: 30.478777, Train_MMSE: 0.058428, NMMSE: 0.067221, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:04:18] Epoch 109/240, Loss: 30.049601, Train_MMSE: 0.058378, NMMSE: 0.067217, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:06:42] Epoch 110/240, Loss: 30.264757, Train_MMSE: 0.058313, NMMSE: 0.067259, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:09:06] Epoch 111/240, Loss: 30.261580, Train_MMSE: 0.058271, NMMSE: 0.06737, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:11:28] Epoch 112/240, Loss: 30.269775, Train_MMSE: 0.058226, NMMSE: 0.067415, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:13:54] Epoch 113/240, Loss: 29.952074, Train_MMSE: 0.058174, NMMSE: 0.067418, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:16:16] Epoch 114/240, Loss: 30.129494, Train_MMSE: 0.05813, NMMSE: 0.067494, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:18:38] Epoch 115/240, Loss: 30.167551, Train_MMSE: 0.058066, NMMSE: 0.067521, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:21:01] Epoch 116/240, Loss: 30.177027, Train_MMSE: 0.058024, NMMSE: 0.0677, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:23:23] Epoch 117/240, Loss: 30.094099, Train_MMSE: 0.057983, NMMSE: 0.067653, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:25:45] Epoch 118/240, Loss: 30.425028, Train_MMSE: 0.05793, NMMSE: 0.067771, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:28:08] Epoch 119/240, Loss: 30.042206, Train_MMSE: 0.057891, NMMSE: 0.067828, LS_NMSE: 0.141043, Lr: 0.0001
[2025-02-24 20:30:31] Epoch 120/240, Loss: 30.038548, Train_MMSE: 0.057857, NMMSE: 0.067798, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:32:53] Epoch 121/240, Loss: 29.681858, Train_MMSE: 0.056846, NMMSE: 0.067771, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:35:15] Epoch 122/240, Loss: 29.544237, Train_MMSE: 0.056687, NMMSE: 0.067928, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:37:36] Epoch 123/240, Loss: 29.581553, Train_MMSE: 0.056638, NMMSE: 0.067966, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:39:57] Epoch 124/240, Loss: 29.731148, Train_MMSE: 0.056608, NMMSE: 0.06805, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:42:19] Epoch 125/240, Loss: 29.645180, Train_MMSE: 0.056583, NMMSE: 0.068136, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:44:40] Epoch 126/240, Loss: 29.517866, Train_MMSE: 0.056569, NMMSE: 0.068166, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:47:01] Epoch 127/240, Loss: 29.856209, Train_MMSE: 0.056549, NMMSE: 0.068155, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:49:20] Epoch 128/240, Loss: 29.757948, Train_MMSE: 0.056529, NMMSE: 0.068187, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:51:43] Epoch 129/240, Loss: 29.728235, Train_MMSE: 0.056518, NMMSE: 0.06819, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:54:04] Epoch 130/240, Loss: 29.834673, Train_MMSE: 0.056503, NMMSE: 0.068222, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:56:26] Epoch 131/240, Loss: 29.686750, Train_MMSE: 0.056489, NMMSE: 0.068223, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 20:58:46] Epoch 132/240, Loss: 29.421801, Train_MMSE: 0.056483, NMMSE: 0.068307, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:01:07] Epoch 133/240, Loss: 29.819315, Train_MMSE: 0.056465, NMMSE: 0.068343, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:03:28] Epoch 134/240, Loss: 29.600145, Train_MMSE: 0.056456, NMMSE: 0.068353, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:05:50] Epoch 135/240, Loss: 29.651039, Train_MMSE: 0.056443, NMMSE: 0.068371, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:08:11] Epoch 136/240, Loss: 29.743801, Train_MMSE: 0.056434, NMMSE: 0.068354, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:10:32] Epoch 137/240, Loss: 29.540236, Train_MMSE: 0.056423, NMMSE: 0.068415, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:12:53] Epoch 138/240, Loss: 29.457663, Train_MMSE: 0.056413, NMMSE: 0.068485, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:15:13] Epoch 139/240, Loss: 29.584242, Train_MMSE: 0.056403, NMMSE: 0.06848, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:17:35] Epoch 140/240, Loss: 29.610229, Train_MMSE: 0.056392, NMMSE: 0.068487, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:19:57] Epoch 141/240, Loss: 29.608179, Train_MMSE: 0.056385, NMMSE: 0.068452, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:22:18] Epoch 142/240, Loss: 29.414730, Train_MMSE: 0.056376, NMMSE: 0.068556, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:24:39] Epoch 143/240, Loss: 29.178341, Train_MMSE: 0.05637, NMMSE: 0.068548, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:27:01] Epoch 144/240, Loss: 29.385424, Train_MMSE: 0.056356, NMMSE: 0.068569, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:29:23] Epoch 145/240, Loss: 29.628256, Train_MMSE: 0.056346, NMMSE: 0.0685, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:31:44] Epoch 146/240, Loss: 29.545895, Train_MMSE: 0.056337, NMMSE: 0.06854, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:34:06] Epoch 147/240, Loss: 29.620802, Train_MMSE: 0.056328, NMMSE: 0.068515, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:36:27] Epoch 148/240, Loss: 29.943237, Train_MMSE: 0.056327, NMMSE: 0.068571, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:38:48] Epoch 149/240, Loss: 29.572950, Train_MMSE: 0.056319, NMMSE: 0.068642, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:41:08] Epoch 150/240, Loss: 29.581984, Train_MMSE: 0.056299, NMMSE: 0.068617, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:43:30] Epoch 151/240, Loss: 29.499397, Train_MMSE: 0.056294, NMMSE: 0.068674, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:45:51] Epoch 152/240, Loss: 29.715261, Train_MMSE: 0.05629, NMMSE: 0.068657, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:48:13] Epoch 153/240, Loss: 29.819868, Train_MMSE: 0.056276, NMMSE: 0.068653, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:50:36] Epoch 154/240, Loss: 29.443979, Train_MMSE: 0.056267, NMMSE: 0.068669, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:52:57] Epoch 155/240, Loss: 29.387339, Train_MMSE: 0.056263, NMMSE: 0.068691, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:55:18] Epoch 156/240, Loss: 29.667366, Train_MMSE: 0.056255, NMMSE: 0.06869, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 21:57:40] Epoch 157/240, Loss: 29.750662, Train_MMSE: 0.056246, NMMSE: 0.068675, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:00:01] Epoch 158/240, Loss: 29.564806, Train_MMSE: 0.05624, NMMSE: 0.068675, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:02:22] Epoch 159/240, Loss: 29.528286, Train_MMSE: 0.056235, NMMSE: 0.068719, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:04:43] Epoch 160/240, Loss: 29.555393, Train_MMSE: 0.056225, NMMSE: 0.068715, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:07:06] Epoch 161/240, Loss: 29.557343, Train_MMSE: 0.056215, NMMSE: 0.068728, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:09:26] Epoch 162/240, Loss: 29.630625, Train_MMSE: 0.056208, NMMSE: 0.06881, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:11:48] Epoch 163/240, Loss: 29.393446, Train_MMSE: 0.056203, NMMSE: 0.06875, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:14:10] Epoch 164/240, Loss: 29.382895, Train_MMSE: 0.056195, NMMSE: 0.068769, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:16:33] Epoch 165/240, Loss: 29.502533, Train_MMSE: 0.056186, NMMSE: 0.068786, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:18:54] Epoch 166/240, Loss: 29.438089, Train_MMSE: 0.056183, NMMSE: 0.068774, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:21:15] Epoch 167/240, Loss: 29.259212, Train_MMSE: 0.056172, NMMSE: 0.068849, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:23:38] Epoch 168/240, Loss: 29.446302, Train_MMSE: 0.056167, NMMSE: 0.068791, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:25:58] Epoch 169/240, Loss: 29.698381, Train_MMSE: 0.056162, NMMSE: 0.068819, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:28:19] Epoch 170/240, Loss: 29.605473, Train_MMSE: 0.05615, NMMSE: 0.068852, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:30:42] Epoch 171/240, Loss: 29.473026, Train_MMSE: 0.056143, NMMSE: 0.068866, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:33:04] Epoch 172/240, Loss: 29.666573, Train_MMSE: 0.05614, NMMSE: 0.068873, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:35:26] Epoch 173/240, Loss: 29.628828, Train_MMSE: 0.056133, NMMSE: 0.068869, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:37:47] Epoch 174/240, Loss: 29.602011, Train_MMSE: 0.056125, NMMSE: 0.068871, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:40:09] Epoch 175/240, Loss: 29.289843, Train_MMSE: 0.056116, NMMSE: 0.068909, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:42:30] Epoch 176/240, Loss: 29.748966, Train_MMSE: 0.056111, NMMSE: 0.06889, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:44:51] Epoch 177/240, Loss: 29.448782, Train_MMSE: 0.056102, NMMSE: 0.068888, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:47:13] Epoch 178/240, Loss: 29.715919, Train_MMSE: 0.056092, NMMSE: 0.068947, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:49:34] Epoch 179/240, Loss: 29.487732, Train_MMSE: 0.056091, NMMSE: 0.068983, LS_NMSE: 0.141043, Lr: 1e-05
[2025-02-24 22:51:56] Epoch 180/240, Loss: 29.597717, Train_MMSE: 0.056079, NMMSE: 0.068961, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 22:54:18] Epoch 181/240, Loss: 29.739235, Train_MMSE: 0.05594, NMMSE: 0.068953, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 22:56:38] Epoch 182/240, Loss: 29.423849, Train_MMSE: 0.055931, NMMSE: 0.068965, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 22:59:00] Epoch 183/240, Loss: 29.386410, Train_MMSE: 0.055926, NMMSE: 0.068946, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:01:21] Epoch 184/240, Loss: 29.592215, Train_MMSE: 0.055923, NMMSE: 0.068949, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:03:43] Epoch 185/240, Loss: 29.355251, Train_MMSE: 0.055923, NMMSE: 0.069004, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:06:04] Epoch 186/240, Loss: 29.508125, Train_MMSE: 0.055927, NMMSE: 0.0689, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:08:25] Epoch 187/240, Loss: 29.344477, Train_MMSE: 0.055924, NMMSE: 0.068965, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:48] Epoch 188/240, Loss: 29.375648, Train_MMSE: 0.055921, NMMSE: 0.068947, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:13:08] Epoch 189/240, Loss: 29.353540, Train_MMSE: 0.05592, NMMSE: 0.069009, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:15:29] Epoch 190/240, Loss: 29.258402, Train_MMSE: 0.055921, NMMSE: 0.06897, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:17:51] Epoch 191/240, Loss: 29.385431, Train_MMSE: 0.055919, NMMSE: 0.068987, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:20:13] Epoch 192/240, Loss: 29.297432, Train_MMSE: 0.055925, NMMSE: 0.068966, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:22:34] Epoch 193/240, Loss: 29.279898, Train_MMSE: 0.055917, NMMSE: 0.069025, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:24:55] Epoch 194/240, Loss: 29.450308, Train_MMSE: 0.055913, NMMSE: 0.069027, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:27:17] Epoch 195/240, Loss: 29.375174, Train_MMSE: 0.055921, NMMSE: 0.068987, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:29:39] Epoch 196/240, Loss: 29.334185, Train_MMSE: 0.055917, NMMSE: 0.069043, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:31:59] Epoch 197/240, Loss: 29.396936, Train_MMSE: 0.055914, NMMSE: 0.068998, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:34:22] Epoch 198/240, Loss: 29.637720, Train_MMSE: 0.055916, NMMSE: 0.068992, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:36:44] Epoch 199/240, Loss: 29.478559, Train_MMSE: 0.055914, NMMSE: 0.069046, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:39:04] Epoch 200/240, Loss: 29.283068, Train_MMSE: 0.055914, NMMSE: 0.069023, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:41:26] Epoch 201/240, Loss: 29.665266, Train_MMSE: 0.055912, NMMSE: 0.069012, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:43:48] Epoch 202/240, Loss: 29.436388, Train_MMSE: 0.055912, NMMSE: 0.068976, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:46:11] Epoch 203/240, Loss: 29.473721, Train_MMSE: 0.05591, NMMSE: 0.06902, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:48:32] Epoch 204/240, Loss: 29.274849, Train_MMSE: 0.055909, NMMSE: 0.068977, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:50:53] Epoch 205/240, Loss: 29.372232, Train_MMSE: 0.055905, NMMSE: 0.06898, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:53:16] Epoch 206/240, Loss: 29.126871, Train_MMSE: 0.055908, NMMSE: 0.069046, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:55:38] Epoch 207/240, Loss: 29.416142, Train_MMSE: 0.055907, NMMSE: 0.069012, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-24 23:57:59] Epoch 208/240, Loss: 29.410639, Train_MMSE: 0.055905, NMMSE: 0.068973, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:00:22] Epoch 209/240, Loss: 29.403172, Train_MMSE: 0.055904, NMMSE: 0.069067, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:02:44] Epoch 210/240, Loss: 29.556925, Train_MMSE: 0.055907, NMMSE: 0.069047, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:05:05] Epoch 211/240, Loss: 29.281298, Train_MMSE: 0.055906, NMMSE: 0.069017, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:07:27] Epoch 212/240, Loss: 29.327860, Train_MMSE: 0.055902, NMMSE: 0.069041, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:09:50] Epoch 213/240, Loss: 29.379879, Train_MMSE: 0.055903, NMMSE: 0.068991, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:12:12] Epoch 214/240, Loss: 29.408312, Train_MMSE: 0.055898, NMMSE: 0.069036, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:14:37] Epoch 215/240, Loss: 29.527731, Train_MMSE: 0.055902, NMMSE: 0.069044, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:16:58] Epoch 216/240, Loss: 29.576015, Train_MMSE: 0.055902, NMMSE: 0.068984, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:19:20] Epoch 217/240, Loss: 29.235186, Train_MMSE: 0.0559, NMMSE: 0.06903, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:21:41] Epoch 218/240, Loss: 29.250174, Train_MMSE: 0.055897, NMMSE: 0.069024, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:24:03] Epoch 219/240, Loss: 29.277182, Train_MMSE: 0.0559, NMMSE: 0.068945, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:26:26] Epoch 220/240, Loss: 29.324076, Train_MMSE: 0.055893, NMMSE: 0.069059, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:28:46] Epoch 221/240, Loss: 29.660488, Train_MMSE: 0.055897, NMMSE: 0.069024, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:31:09] Epoch 222/240, Loss: 29.556723, Train_MMSE: 0.055895, NMMSE: 0.069014, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:33:32] Epoch 223/240, Loss: 29.523895, Train_MMSE: 0.055893, NMMSE: 0.069012, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:54] Epoch 224/240, Loss: 29.419081, Train_MMSE: 0.055889, NMMSE: 0.069092, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:17] Epoch 225/240, Loss: 29.334890, Train_MMSE: 0.055895, NMMSE: 0.069044, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:40] Epoch 226/240, Loss: 29.535723, Train_MMSE: 0.055892, NMMSE: 0.069063, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:02] Epoch 227/240, Loss: 29.317200, Train_MMSE: 0.055892, NMMSE: 0.069043, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:26] Epoch 228/240, Loss: 29.230906, Train_MMSE: 0.055892, NMMSE: 0.06905, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:47] Epoch 229/240, Loss: 29.472033, Train_MMSE: 0.055894, NMMSE: 0.069129, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:50:11] Epoch 230/240, Loss: 29.351044, Train_MMSE: 0.055892, NMMSE: 0.069019, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:52:35] Epoch 231/240, Loss: 29.408264, Train_MMSE: 0.055883, NMMSE: 0.06905, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:54:55] Epoch 232/240, Loss: 29.424829, Train_MMSE: 0.055892, NMMSE: 0.06899, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:57:18] Epoch 233/240, Loss: 29.382944, Train_MMSE: 0.055884, NMMSE: 0.069011, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 00:59:39] Epoch 234/240, Loss: 29.473585, Train_MMSE: 0.055888, NMMSE: 0.069013, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:02:00] Epoch 235/240, Loss: 29.152510, Train_MMSE: 0.055885, NMMSE: 0.069022, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:04:23] Epoch 236/240, Loss: 29.582184, Train_MMSE: 0.05589, NMMSE: 0.069086, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:06:45] Epoch 237/240, Loss: 29.240187, Train_MMSE: 0.055882, NMMSE: 0.069122, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:09:05] Epoch 238/240, Loss: 29.312931, Train_MMSE: 0.055886, NMMSE: 0.06904, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:11:26] Epoch 239/240, Loss: 29.357574, Train_MMSE: 0.055882, NMMSE: 0.069024, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-06
[2025-02-25 01:13:39] Epoch 240/240, Loss: 29.166351, Train_MMSE: 0.055881, NMMSE: 0.068979, LS_NMSE: 0.141043, Lr: 1.0000000000000002e-07
