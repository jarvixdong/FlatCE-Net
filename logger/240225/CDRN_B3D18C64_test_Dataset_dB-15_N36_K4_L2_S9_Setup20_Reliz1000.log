Train.py PID: 1280

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.1072786865458829
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L2_S9_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L2_S9_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L2_S9_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7ff513751040>
loss function:: SmoothL1Loss()
[2025-02-24 15:43:49] Epoch 1/240, Loss: 65.285103, Train_MMSE: 0.435208, NMMSE: 0.281317, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:44:15] Epoch 2/240, Loss: 56.309048, Train_MMSE: 0.219636, NMMSE: 0.213277, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:44:47] Epoch 3/240, Loss: 54.268379, Train_MMSE: 0.188837, NMMSE: 0.194736, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:45:26] Epoch 4/240, Loss: 52.921402, Train_MMSE: 0.176286, NMMSE: 0.186891, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:46:24] Epoch 5/240, Loss: 52.149326, Train_MMSE: 0.16889, NMMSE: 0.178971, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:47:40] Epoch 6/240, Loss: 51.385784, Train_MMSE: 0.163599, NMMSE: 0.174624, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:49:01] Epoch 7/240, Loss: 50.870441, Train_MMSE: 0.159699, NMMSE: 0.172623, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:50:58] Epoch 8/240, Loss: 49.941578, Train_MMSE: 0.156251, NMMSE: 0.167021, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:53:05] Epoch 9/240, Loss: 49.832962, Train_MMSE: 0.153459, NMMSE: 0.165111, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:56:20] Epoch 10/240, Loss: 49.314568, Train_MMSE: 0.150897, NMMSE: 0.161791, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 15:59:06] Epoch 11/240, Loss: 49.506748, Train_MMSE: 0.148703, NMMSE: 0.161551, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:01:47] Epoch 12/240, Loss: 48.898678, Train_MMSE: 0.146866, NMMSE: 0.158332, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:04:18] Epoch 13/240, Loss: 47.980175, Train_MMSE: 0.145055, NMMSE: 0.1563, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:06:49] Epoch 14/240, Loss: 48.071968, Train_MMSE: 0.143649, NMMSE: 0.155797, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:09:19] Epoch 15/240, Loss: 47.912090, Train_MMSE: 0.142465, NMMSE: 0.15465, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:11:48] Epoch 16/240, Loss: 47.035145, Train_MMSE: 0.141215, NMMSE: 0.153024, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:14:23] Epoch 17/240, Loss: 47.364548, Train_MMSE: 0.140371, NMMSE: 0.151648, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:16:51] Epoch 18/240, Loss: 47.271294, Train_MMSE: 0.139436, NMMSE: 0.150341, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:19:20] Epoch 19/240, Loss: 46.874718, Train_MMSE: 0.138674, NMMSE: 0.15122, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:21:50] Epoch 20/240, Loss: 46.915596, Train_MMSE: 0.137995, NMMSE: 0.149414, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:24:19] Epoch 21/240, Loss: 46.917873, Train_MMSE: 0.137347, NMMSE: 0.149735, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:26:49] Epoch 22/240, Loss: 46.990871, Train_MMSE: 0.136634, NMMSE: 0.15042, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:29:20] Epoch 23/240, Loss: 47.325787, Train_MMSE: 0.136202, NMMSE: 0.147757, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:31:50] Epoch 24/240, Loss: 46.379768, Train_MMSE: 0.135707, NMMSE: 0.147704, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:34:20] Epoch 25/240, Loss: 46.105488, Train_MMSE: 0.135265, NMMSE: 0.147681, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:36:48] Epoch 26/240, Loss: 46.487720, Train_MMSE: 0.13474, NMMSE: 0.148482, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:39:16] Epoch 27/240, Loss: 46.952419, Train_MMSE: 0.134433, NMMSE: 0.146412, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:41:45] Epoch 28/240, Loss: 47.033649, Train_MMSE: 0.134047, NMMSE: 0.14773, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:44:13] Epoch 29/240, Loss: 45.689152, Train_MMSE: 0.133766, NMMSE: 0.145078, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:46:42] Epoch 30/240, Loss: 45.763012, Train_MMSE: 0.133451, NMMSE: 0.146138, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:49:11] Epoch 31/240, Loss: 46.264088, Train_MMSE: 0.133066, NMMSE: 0.144903, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:51:39] Epoch 32/240, Loss: 46.250584, Train_MMSE: 0.132918, NMMSE: 0.146159, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:54:07] Epoch 33/240, Loss: 46.220512, Train_MMSE: 0.132546, NMMSE: 0.146332, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:56:37] Epoch 34/240, Loss: 46.235352, Train_MMSE: 0.13233, NMMSE: 0.145516, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 16:59:05] Epoch 35/240, Loss: 45.693779, Train_MMSE: 0.132168, NMMSE: 0.14421, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:01:33] Epoch 36/240, Loss: 45.921772, Train_MMSE: 0.131857, NMMSE: 0.145844, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:04:02] Epoch 37/240, Loss: 46.241600, Train_MMSE: 0.131725, NMMSE: 0.144297, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:06:29] Epoch 38/240, Loss: 46.156471, Train_MMSE: 0.131537, NMMSE: 0.144273, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:08:57] Epoch 39/240, Loss: 46.315331, Train_MMSE: 0.131317, NMMSE: 0.144345, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:11:27] Epoch 40/240, Loss: 46.125591, Train_MMSE: 0.131149, NMMSE: 0.14293, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:13:56] Epoch 41/240, Loss: 45.833195, Train_MMSE: 0.13095, NMMSE: 0.144194, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:16:25] Epoch 42/240, Loss: 45.365654, Train_MMSE: 0.130784, NMMSE: 0.143064, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:18:53] Epoch 43/240, Loss: 46.059620, Train_MMSE: 0.130629, NMMSE: 0.14297, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:21:21] Epoch 44/240, Loss: 45.396381, Train_MMSE: 0.130471, NMMSE: 0.143601, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:23:50] Epoch 45/240, Loss: 45.852512, Train_MMSE: 0.130361, NMMSE: 0.142213, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:26:18] Epoch 46/240, Loss: 45.965733, Train_MMSE: 0.130228, NMMSE: 0.143735, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:28:47] Epoch 47/240, Loss: 46.218002, Train_MMSE: 0.130121, NMMSE: 0.14257, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:31:15] Epoch 48/240, Loss: 45.581886, Train_MMSE: 0.130013, NMMSE: 0.141538, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:33:46] Epoch 49/240, Loss: 46.175804, Train_MMSE: 0.129802, NMMSE: 0.142793, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:36:15] Epoch 50/240, Loss: 46.258549, Train_MMSE: 0.129695, NMMSE: 0.142262, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:38:44] Epoch 51/240, Loss: 46.052155, Train_MMSE: 0.129619, NMMSE: 0.142515, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:41:15] Epoch 52/240, Loss: 45.390327, Train_MMSE: 0.129461, NMMSE: 0.142435, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:43:46] Epoch 53/240, Loss: 45.536324, Train_MMSE: 0.129488, NMMSE: 0.141856, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:46:14] Epoch 54/240, Loss: 45.539032, Train_MMSE: 0.129289, NMMSE: 0.14164, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:48:42] Epoch 55/240, Loss: 45.621265, Train_MMSE: 0.129076, NMMSE: 0.142038, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:51:11] Epoch 56/240, Loss: 45.570732, Train_MMSE: 0.129129, NMMSE: 0.142843, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:53:39] Epoch 57/240, Loss: 45.825645, Train_MMSE: 0.128956, NMMSE: 0.141381, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:56:08] Epoch 58/240, Loss: 45.637547, Train_MMSE: 0.128911, NMMSE: 0.141695, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 17:58:35] Epoch 59/240, Loss: 45.707214, Train_MMSE: 0.128998, NMMSE: 0.142902, LS_NMSE: 0.799979, Lr: 0.001
[2025-02-24 18:01:04] Epoch 60/240, Loss: 45.845020, Train_MMSE: 0.12878, NMMSE: 0.143144, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:03:32] Epoch 61/240, Loss: 44.373070, Train_MMSE: 0.122974, NMMSE: 0.134446, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:06:10] Epoch 62/240, Loss: 44.045967, Train_MMSE: 0.122222, NMMSE: 0.134438, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:08:39] Epoch 63/240, Loss: 44.064411, Train_MMSE: 0.122018, NMMSE: 0.134552, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:11:07] Epoch 64/240, Loss: 44.204372, Train_MMSE: 0.121875, NMMSE: 0.134658, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:13:39] Epoch 65/240, Loss: 43.875431, Train_MMSE: 0.121782, NMMSE: 0.134587, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:16:08] Epoch 66/240, Loss: 44.586334, Train_MMSE: 0.121665, NMMSE: 0.134613, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:18:37] Epoch 67/240, Loss: 43.969357, Train_MMSE: 0.121551, NMMSE: 0.134713, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:21:06] Epoch 68/240, Loss: 43.823971, Train_MMSE: 0.12146, NMMSE: 0.13458, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:23:34] Epoch 69/240, Loss: 43.473377, Train_MMSE: 0.12135, NMMSE: 0.134801, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:26:03] Epoch 70/240, Loss: 43.619587, Train_MMSE: 0.121291, NMMSE: 0.134559, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:28:31] Epoch 71/240, Loss: 43.474869, Train_MMSE: 0.121213, NMMSE: 0.134641, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:31:00] Epoch 72/240, Loss: 43.710327, Train_MMSE: 0.121127, NMMSE: 0.134669, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:33:28] Epoch 73/240, Loss: 44.485584, Train_MMSE: 0.121048, NMMSE: 0.134882, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:35:56] Epoch 74/240, Loss: 43.850426, Train_MMSE: 0.120957, NMMSE: 0.134678, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:38:23] Epoch 75/240, Loss: 43.366478, Train_MMSE: 0.120898, NMMSE: 0.134725, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:40:51] Epoch 76/240, Loss: 44.156639, Train_MMSE: 0.120819, NMMSE: 0.135036, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:43:21] Epoch 77/240, Loss: 43.870876, Train_MMSE: 0.120759, NMMSE: 0.134953, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:45:50] Epoch 78/240, Loss: 43.251038, Train_MMSE: 0.120698, NMMSE: 0.135073, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:48:19] Epoch 79/240, Loss: 43.348732, Train_MMSE: 0.120637, NMMSE: 0.134954, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:50:49] Epoch 80/240, Loss: 43.552555, Train_MMSE: 0.120571, NMMSE: 0.135001, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:53:17] Epoch 81/240, Loss: 43.613224, Train_MMSE: 0.120518, NMMSE: 0.135056, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:55:44] Epoch 82/240, Loss: 44.264973, Train_MMSE: 0.120463, NMMSE: 0.134966, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 18:58:12] Epoch 83/240, Loss: 43.489338, Train_MMSE: 0.120407, NMMSE: 0.135093, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:00:41] Epoch 84/240, Loss: 43.449680, Train_MMSE: 0.120331, NMMSE: 0.135213, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:03:08] Epoch 85/240, Loss: 43.514320, Train_MMSE: 0.120289, NMMSE: 0.135429, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:05:36] Epoch 86/240, Loss: 43.569294, Train_MMSE: 0.120249, NMMSE: 0.135135, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:08:03] Epoch 87/240, Loss: 43.518417, Train_MMSE: 0.12018, NMMSE: 0.135371, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:10:31] Epoch 88/240, Loss: 43.876266, Train_MMSE: 0.120153, NMMSE: 0.135136, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:12:59] Epoch 89/240, Loss: 43.725784, Train_MMSE: 0.120105, NMMSE: 0.135456, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:15:25] Epoch 90/240, Loss: 43.639053, Train_MMSE: 0.120038, NMMSE: 0.135416, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:17:53] Epoch 91/240, Loss: 43.978828, Train_MMSE: 0.119991, NMMSE: 0.135588, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:20:21] Epoch 92/240, Loss: 43.896507, Train_MMSE: 0.119966, NMMSE: 0.135638, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:22:49] Epoch 93/240, Loss: 44.995022, Train_MMSE: 0.119895, NMMSE: 0.135346, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:25:18] Epoch 94/240, Loss: 43.287743, Train_MMSE: 0.119862, NMMSE: 0.135527, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:27:45] Epoch 95/240, Loss: 43.356495, Train_MMSE: 0.119796, NMMSE: 0.13554, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:30:13] Epoch 96/240, Loss: 43.397030, Train_MMSE: 0.119755, NMMSE: 0.13542, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:32:41] Epoch 97/240, Loss: 43.389065, Train_MMSE: 0.119722, NMMSE: 0.135572, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:35:08] Epoch 98/240, Loss: 43.701607, Train_MMSE: 0.11967, NMMSE: 0.13578, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:37:36] Epoch 99/240, Loss: 43.246014, Train_MMSE: 0.11964, NMMSE: 0.135659, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:40:04] Epoch 100/240, Loss: 43.179115, Train_MMSE: 0.119589, NMMSE: 0.135678, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:42:31] Epoch 101/240, Loss: 43.905540, Train_MMSE: 0.119562, NMMSE: 0.135932, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:44:59] Epoch 102/240, Loss: 43.500607, Train_MMSE: 0.119488, NMMSE: 0.135798, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:47:27] Epoch 103/240, Loss: 43.669510, Train_MMSE: 0.119459, NMMSE: 0.135832, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:49:53] Epoch 104/240, Loss: 43.670956, Train_MMSE: 0.119407, NMMSE: 0.135809, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:52:21] Epoch 105/240, Loss: 43.343010, Train_MMSE: 0.11939, NMMSE: 0.135974, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:54:49] Epoch 106/240, Loss: 43.710529, Train_MMSE: 0.119313, NMMSE: 0.13588, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:57:16] Epoch 107/240, Loss: 43.639091, Train_MMSE: 0.11929, NMMSE: 0.136089, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 19:59:45] Epoch 108/240, Loss: 42.876820, Train_MMSE: 0.119268, NMMSE: 0.136056, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:02:13] Epoch 109/240, Loss: 44.100220, Train_MMSE: 0.119237, NMMSE: 0.135974, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:04:41] Epoch 110/240, Loss: 43.796738, Train_MMSE: 0.119165, NMMSE: 0.136237, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:07:08] Epoch 111/240, Loss: 43.040184, Train_MMSE: 0.119132, NMMSE: 0.13619, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:09:37] Epoch 112/240, Loss: 43.174442, Train_MMSE: 0.119119, NMMSE: 0.136143, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:12:04] Epoch 113/240, Loss: 43.183662, Train_MMSE: 0.119069, NMMSE: 0.136264, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:14:29] Epoch 114/240, Loss: 43.689983, Train_MMSE: 0.11904, NMMSE: 0.136467, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:16:57] Epoch 115/240, Loss: 43.399216, Train_MMSE: 0.118991, NMMSE: 0.136304, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:19:24] Epoch 116/240, Loss: 42.945026, Train_MMSE: 0.118963, NMMSE: 0.136321, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:21:52] Epoch 117/240, Loss: 43.159073, Train_MMSE: 0.118887, NMMSE: 0.136267, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:24:20] Epoch 118/240, Loss: 43.710636, Train_MMSE: 0.118855, NMMSE: 0.136425, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:26:46] Epoch 119/240, Loss: 43.596333, Train_MMSE: 0.118824, NMMSE: 0.136245, LS_NMSE: 0.799979, Lr: 0.0001
[2025-02-24 20:29:13] Epoch 120/240, Loss: 43.246357, Train_MMSE: 0.118783, NMMSE: 0.136274, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:31:42] Epoch 121/240, Loss: 42.950603, Train_MMSE: 0.117516, NMMSE: 0.135495, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:34:09] Epoch 122/240, Loss: 43.003635, Train_MMSE: 0.117379, NMMSE: 0.135524, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:36:35] Epoch 123/240, Loss: 42.954720, Train_MMSE: 0.117351, NMMSE: 0.13555, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:39:01] Epoch 124/240, Loss: 42.723045, Train_MMSE: 0.117329, NMMSE: 0.135604, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:41:27] Epoch 125/240, Loss: 43.596119, Train_MMSE: 0.117319, NMMSE: 0.135617, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:43:54] Epoch 126/240, Loss: 42.841545, Train_MMSE: 0.11731, NMMSE: 0.135627, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:46:19] Epoch 127/240, Loss: 42.570263, Train_MMSE: 0.117301, NMMSE: 0.135701, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:48:45] Epoch 128/240, Loss: 43.382053, Train_MMSE: 0.11729, NMMSE: 0.135715, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:51:13] Epoch 129/240, Loss: 43.046211, Train_MMSE: 0.117282, NMMSE: 0.135721, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:53:39] Epoch 130/240, Loss: 43.108875, Train_MMSE: 0.117268, NMMSE: 0.135773, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:56:06] Epoch 131/240, Loss: 43.389061, Train_MMSE: 0.11726, NMMSE: 0.135759, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 20:58:31] Epoch 132/240, Loss: 42.460876, Train_MMSE: 0.117251, NMMSE: 0.135794, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:00:57] Epoch 133/240, Loss: 43.014824, Train_MMSE: 0.117249, NMMSE: 0.135812, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:03:22] Epoch 134/240, Loss: 42.695431, Train_MMSE: 0.11723, NMMSE: 0.13584, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:05:48] Epoch 135/240, Loss: 43.131702, Train_MMSE: 0.117231, NMMSE: 0.135817, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:08:14] Epoch 136/240, Loss: 42.857105, Train_MMSE: 0.117219, NMMSE: 0.135856, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:10:39] Epoch 137/240, Loss: 42.910690, Train_MMSE: 0.117209, NMMSE: 0.13589, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:13:05] Epoch 138/240, Loss: 42.516327, Train_MMSE: 0.117204, NMMSE: 0.135864, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:15:31] Epoch 139/240, Loss: 43.060261, Train_MMSE: 0.117199, NMMSE: 0.135874, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:17:58] Epoch 140/240, Loss: 43.598862, Train_MMSE: 0.117192, NMMSE: 0.135844, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:20:26] Epoch 141/240, Loss: 42.519032, Train_MMSE: 0.117179, NMMSE: 0.135847, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:22:52] Epoch 142/240, Loss: 43.200298, Train_MMSE: 0.117167, NMMSE: 0.135887, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:25:19] Epoch 143/240, Loss: 42.452698, Train_MMSE: 0.117161, NMMSE: 0.135922, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:27:46] Epoch 144/240, Loss: 43.167530, Train_MMSE: 0.117147, NMMSE: 0.13589, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:30:13] Epoch 145/240, Loss: 42.926167, Train_MMSE: 0.117149, NMMSE: 0.135921, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:32:39] Epoch 146/240, Loss: 42.838390, Train_MMSE: 0.117139, NMMSE: 0.135976, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:35:05] Epoch 147/240, Loss: 43.172035, Train_MMSE: 0.117134, NMMSE: 0.135916, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:37:31] Epoch 148/240, Loss: 42.892376, Train_MMSE: 0.117118, NMMSE: 0.135997, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:39:58] Epoch 149/240, Loss: 43.039017, Train_MMSE: 0.117117, NMMSE: 0.135935, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:42:24] Epoch 150/240, Loss: 42.776726, Train_MMSE: 0.117114, NMMSE: 0.135976, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:44:51] Epoch 151/240, Loss: 42.983833, Train_MMSE: 0.117098, NMMSE: 0.136025, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:47:16] Epoch 152/240, Loss: 42.603596, Train_MMSE: 0.117095, NMMSE: 0.136011, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:49:42] Epoch 153/240, Loss: 43.101398, Train_MMSE: 0.117081, NMMSE: 0.136058, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:52:09] Epoch 154/240, Loss: 42.649090, Train_MMSE: 0.11708, NMMSE: 0.13599, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:54:34] Epoch 155/240, Loss: 43.237221, Train_MMSE: 0.117071, NMMSE: 0.136046, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:57:01] Epoch 156/240, Loss: 42.518166, Train_MMSE: 0.117062, NMMSE: 0.136033, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 21:59:27] Epoch 157/240, Loss: 42.712746, Train_MMSE: 0.117057, NMMSE: 0.136088, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:01:54] Epoch 158/240, Loss: 43.256577, Train_MMSE: 0.117051, NMMSE: 0.136026, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:04:20] Epoch 159/240, Loss: 42.938782, Train_MMSE: 0.117045, NMMSE: 0.13609, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:06:47] Epoch 160/240, Loss: 43.049736, Train_MMSE: 0.117039, NMMSE: 0.136073, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:09:12] Epoch 161/240, Loss: 43.380329, Train_MMSE: 0.11703, NMMSE: 0.136055, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:11:39] Epoch 162/240, Loss: 42.699635, Train_MMSE: 0.117027, NMMSE: 0.136114, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:14:04] Epoch 163/240, Loss: 43.159363, Train_MMSE: 0.117015, NMMSE: 0.136118, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:16:30] Epoch 164/240, Loss: 42.809895, Train_MMSE: 0.117014, NMMSE: 0.136121, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:18:58] Epoch 165/240, Loss: 42.803501, Train_MMSE: 0.117004, NMMSE: 0.136144, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:21:23] Epoch 166/240, Loss: 42.510269, Train_MMSE: 0.116992, NMMSE: 0.13612, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:23:50] Epoch 167/240, Loss: 42.941162, Train_MMSE: 0.116992, NMMSE: 0.136139, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:26:17] Epoch 168/240, Loss: 42.755085, Train_MMSE: 0.116983, NMMSE: 0.136155, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:28:44] Epoch 169/240, Loss: 42.800079, Train_MMSE: 0.116971, NMMSE: 0.136136, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:31:10] Epoch 170/240, Loss: 43.041046, Train_MMSE: 0.116963, NMMSE: 0.136132, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:33:36] Epoch 171/240, Loss: 42.871010, Train_MMSE: 0.116969, NMMSE: 0.136203, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:36:03] Epoch 172/240, Loss: 43.246082, Train_MMSE: 0.116961, NMMSE: 0.136175, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:38:30] Epoch 173/240, Loss: 42.989128, Train_MMSE: 0.116946, NMMSE: 0.136166, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:40:58] Epoch 174/240, Loss: 42.837196, Train_MMSE: 0.116938, NMMSE: 0.13619, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:43:25] Epoch 175/240, Loss: 42.891617, Train_MMSE: 0.116935, NMMSE: 0.13623, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:45:50] Epoch 176/240, Loss: 42.780594, Train_MMSE: 0.11693, NMMSE: 0.136217, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:48:17] Epoch 177/240, Loss: 42.936165, Train_MMSE: 0.116917, NMMSE: 0.136253, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:50:42] Epoch 178/240, Loss: 42.784508, Train_MMSE: 0.11691, NMMSE: 0.136195, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:53:08] Epoch 179/240, Loss: 43.069210, Train_MMSE: 0.116909, NMMSE: 0.136304, LS_NMSE: 0.799979, Lr: 1e-05
[2025-02-24 22:55:36] Epoch 180/240, Loss: 42.581989, Train_MMSE: 0.116908, NMMSE: 0.136268, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 22:58:02] Epoch 181/240, Loss: 43.109711, Train_MMSE: 0.116727, NMMSE: 0.136163, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:00:28] Epoch 182/240, Loss: 43.009968, Train_MMSE: 0.11671, NMMSE: 0.136146, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:02:54] Epoch 183/240, Loss: 42.185261, Train_MMSE: 0.116704, NMMSE: 0.13617, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:05:21] Epoch 184/240, Loss: 42.562515, Train_MMSE: 0.11671, NMMSE: 0.136166, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:07:48] Epoch 185/240, Loss: 42.965439, Train_MMSE: 0.116702, NMMSE: 0.136186, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:15] Epoch 186/240, Loss: 42.818401, Train_MMSE: 0.1167, NMMSE: 0.13615, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:12:42] Epoch 187/240, Loss: 43.141319, Train_MMSE: 0.116699, NMMSE: 0.136188, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:15:08] Epoch 188/240, Loss: 43.197506, Train_MMSE: 0.116695, NMMSE: 0.136177, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:17:34] Epoch 189/240, Loss: 42.859856, Train_MMSE: 0.116698, NMMSE: 0.136146, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:20:00] Epoch 190/240, Loss: 43.064259, Train_MMSE: 0.116697, NMMSE: 0.136175, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:22:28] Epoch 191/240, Loss: 42.925381, Train_MMSE: 0.116693, NMMSE: 0.136205, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:24:54] Epoch 192/240, Loss: 42.373009, Train_MMSE: 0.116694, NMMSE: 0.136225, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:27:22] Epoch 193/240, Loss: 42.689396, Train_MMSE: 0.116691, NMMSE: 0.1362, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:29:48] Epoch 194/240, Loss: 42.602497, Train_MMSE: 0.116694, NMMSE: 0.13619, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:32:14] Epoch 195/240, Loss: 42.612389, Train_MMSE: 0.116699, NMMSE: 0.136189, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:34:42] Epoch 196/240, Loss: 42.654686, Train_MMSE: 0.116688, NMMSE: 0.136194, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:37:07] Epoch 197/240, Loss: 42.717793, Train_MMSE: 0.11669, NMMSE: 0.136152, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:39:34] Epoch 198/240, Loss: 43.004208, Train_MMSE: 0.116691, NMMSE: 0.13619, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:42:00] Epoch 199/240, Loss: 42.832905, Train_MMSE: 0.116687, NMMSE: 0.136194, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:44:27] Epoch 200/240, Loss: 43.311199, Train_MMSE: 0.116692, NMMSE: 0.136206, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:46:55] Epoch 201/240, Loss: 42.696732, Train_MMSE: 0.116692, NMMSE: 0.136215, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:49:21] Epoch 202/240, Loss: 43.156311, Train_MMSE: 0.116686, NMMSE: 0.136218, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:51:48] Epoch 203/240, Loss: 42.851288, Train_MMSE: 0.116695, NMMSE: 0.136198, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:54:17] Epoch 204/240, Loss: 42.623112, Train_MMSE: 0.116694, NMMSE: 0.136257, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:56:44] Epoch 205/240, Loss: 42.891518, Train_MMSE: 0.116685, NMMSE: 0.136196, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-24 23:59:11] Epoch 206/240, Loss: 42.842510, Train_MMSE: 0.116683, NMMSE: 0.13617, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:01:38] Epoch 207/240, Loss: 43.060719, Train_MMSE: 0.116689, NMMSE: 0.136192, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:04:05] Epoch 208/240, Loss: 42.910889, Train_MMSE: 0.116685, NMMSE: 0.136192, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:06:31] Epoch 209/240, Loss: 42.620724, Train_MMSE: 0.116673, NMMSE: 0.136203, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:08:58] Epoch 210/240, Loss: 42.763882, Train_MMSE: 0.116682, NMMSE: 0.136194, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:11:26] Epoch 211/240, Loss: 42.792625, Train_MMSE: 0.116686, NMMSE: 0.136198, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:13:53] Epoch 212/240, Loss: 43.121525, Train_MMSE: 0.11668, NMMSE: 0.136185, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:16:20] Epoch 213/240, Loss: 43.141579, Train_MMSE: 0.116676, NMMSE: 0.136203, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:18:47] Epoch 214/240, Loss: 43.319363, Train_MMSE: 0.116683, NMMSE: 0.136204, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:21:13] Epoch 215/240, Loss: 43.448689, Train_MMSE: 0.11668, NMMSE: 0.136197, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:23:41] Epoch 216/240, Loss: 42.406693, Train_MMSE: 0.116681, NMMSE: 0.136177, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:26:08] Epoch 217/240, Loss: 42.769905, Train_MMSE: 0.116678, NMMSE: 0.136222, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:28:34] Epoch 218/240, Loss: 42.560810, Train_MMSE: 0.116673, NMMSE: 0.136214, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:31:00] Epoch 219/240, Loss: 42.444721, Train_MMSE: 0.116674, NMMSE: 0.136225, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:33:27] Epoch 220/240, Loss: 43.261726, Train_MMSE: 0.116676, NMMSE: 0.136215, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:54] Epoch 221/240, Loss: 42.735630, Train_MMSE: 0.116671, NMMSE: 0.136259, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:22] Epoch 222/240, Loss: 43.362217, Train_MMSE: 0.116677, NMMSE: 0.136234, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:48] Epoch 223/240, Loss: 42.598465, Train_MMSE: 0.116675, NMMSE: 0.136227, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:16] Epoch 224/240, Loss: 43.046124, Train_MMSE: 0.116675, NMMSE: 0.13619, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:43] Epoch 225/240, Loss: 42.935726, Train_MMSE: 0.11667, NMMSE: 0.136235, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:11] Epoch 226/240, Loss: 42.884499, Train_MMSE: 0.116666, NMMSE: 0.136226, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:50:37] Epoch 227/240, Loss: 43.374977, Train_MMSE: 0.116669, NMMSE: 0.136207, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:53:04] Epoch 228/240, Loss: 42.764416, Train_MMSE: 0.116673, NMMSE: 0.136256, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:55:31] Epoch 229/240, Loss: 42.899082, Train_MMSE: 0.11667, NMMSE: 0.136237, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 00:58:00] Epoch 230/240, Loss: 42.747696, Train_MMSE: 0.116672, NMMSE: 0.136248, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:00:27] Epoch 231/240, Loss: 42.874489, Train_MMSE: 0.116668, NMMSE: 0.136203, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:02:53] Epoch 232/240, Loss: 42.725010, Train_MMSE: 0.116668, NMMSE: 0.136254, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:05:20] Epoch 233/240, Loss: 42.672634, Train_MMSE: 0.116665, NMMSE: 0.13621, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:07:45] Epoch 234/240, Loss: 42.911404, Train_MMSE: 0.11667, NMMSE: 0.136225, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:10:12] Epoch 235/240, Loss: 42.438278, Train_MMSE: 0.116662, NMMSE: 0.136255, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:12:38] Epoch 236/240, Loss: 42.828236, Train_MMSE: 0.116664, NMMSE: 0.136242, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:14:37] Epoch 237/240, Loss: 42.811146, Train_MMSE: 0.116662, NMMSE: 0.136234, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:16:25] Epoch 238/240, Loss: 42.845604, Train_MMSE: 0.116663, NMMSE: 0.136225, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:18:13] Epoch 239/240, Loss: 42.906151, Train_MMSE: 0.116657, NMMSE: 0.13626, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-06
[2025-02-25 01:20:01] Epoch 240/240, Loss: 42.843357, Train_MMSE: 0.116663, NMMSE: 0.136273, LS_NMSE: 0.799979, Lr: 1.0000000000000002e-07
