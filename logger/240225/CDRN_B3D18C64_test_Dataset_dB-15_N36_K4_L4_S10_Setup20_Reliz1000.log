Train.py PID: 15651

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.027357171434330044
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S10_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S10_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S10_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fb27faa4260>
loss function:: SmoothL1Loss()
[2025-02-24 15:06:08] Epoch 1/240, Loss: 26.881292, Train_MMSE: 0.045152, NMMSE: 0.035551, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:08:29] Epoch 2/240, Loss: 26.066923, Train_MMSE: 0.041743, NMMSE: 0.033754, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:10:50] Epoch 3/240, Loss: 25.404646, Train_MMSE: 0.03991, NMMSE: 0.032803, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:13:09] Epoch 4/240, Loss: 25.350080, Train_MMSE: 0.039011, NMMSE: 0.032633, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:15:31] Epoch 5/240, Loss: 25.379831, Train_MMSE: 0.038597, NMMSE: 0.032208, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:17:51] Epoch 6/240, Loss: 25.193201, Train_MMSE: 0.038314, NMMSE: 0.03204, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:20:12] Epoch 7/240, Loss: 25.007586, Train_MMSE: 0.038128, NMMSE: 0.031981, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:22:33] Epoch 8/240, Loss: 24.972958, Train_MMSE: 0.037971, NMMSE: 0.031883, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:24:52] Epoch 9/240, Loss: 25.035545, Train_MMSE: 0.037842, NMMSE: 0.031832, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:27:11] Epoch 10/240, Loss: 25.017572, Train_MMSE: 0.037724, NMMSE: 0.031835, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:29:34] Epoch 11/240, Loss: 24.895618, Train_MMSE: 0.037632, NMMSE: 0.031635, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:31:53] Epoch 12/240, Loss: 25.029560, Train_MMSE: 0.037552, NMMSE: 0.031702, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:34:13] Epoch 13/240, Loss: 24.880157, Train_MMSE: 0.037479, NMMSE: 0.031701, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:36:32] Epoch 14/240, Loss: 24.995852, Train_MMSE: 0.037422, NMMSE: 0.031556, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:38:52] Epoch 15/240, Loss: 24.866808, Train_MMSE: 0.037357, NMMSE: 0.031454, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:41:13] Epoch 16/240, Loss: 24.766497, Train_MMSE: 0.037297, NMMSE: 0.031432, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:43:32] Epoch 17/240, Loss: 24.683912, Train_MMSE: 0.037251, NMMSE: 0.031429, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:45:50] Epoch 18/240, Loss: 24.790812, Train_MMSE: 0.037194, NMMSE: 0.031507, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:48:10] Epoch 19/240, Loss: 24.891153, Train_MMSE: 0.037154, NMMSE: 0.031296, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:50:27] Epoch 20/240, Loss: 24.488712, Train_MMSE: 0.037116, NMMSE: 0.031266, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:52:46] Epoch 21/240, Loss: 24.771437, Train_MMSE: 0.037078, NMMSE: 0.031346, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:55:04] Epoch 22/240, Loss: 24.741488, Train_MMSE: 0.037038, NMMSE: 0.031282, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:57:25] Epoch 23/240, Loss: 24.711134, Train_MMSE: 0.037, NMMSE: 0.031222, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 15:59:46] Epoch 24/240, Loss: 24.715153, Train_MMSE: 0.036966, NMMSE: 0.031182, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:02:06] Epoch 25/240, Loss: 24.669838, Train_MMSE: 0.036933, NMMSE: 0.031296, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:04:25] Epoch 26/240, Loss: 24.688772, Train_MMSE: 0.036906, NMMSE: 0.031231, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:06:45] Epoch 27/240, Loss: 24.619562, Train_MMSE: 0.036871, NMMSE: 0.031264, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:09:05] Epoch 28/240, Loss: 24.682125, Train_MMSE: 0.036835, NMMSE: 0.031101, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:11:24] Epoch 29/240, Loss: 24.609125, Train_MMSE: 0.036811, NMMSE: 0.031052, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:13:42] Epoch 30/240, Loss: 24.508575, Train_MMSE: 0.036769, NMMSE: 0.031061, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:15:59] Epoch 31/240, Loss: 24.675106, Train_MMSE: 0.036743, NMMSE: 0.031046, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:18:20] Epoch 32/240, Loss: 24.776873, Train_MMSE: 0.036722, NMMSE: 0.031041, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:20:40] Epoch 33/240, Loss: 24.627296, Train_MMSE: 0.036703, NMMSE: 0.031012, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:23:00] Epoch 34/240, Loss: 24.578594, Train_MMSE: 0.03667, NMMSE: 0.030965, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:25:21] Epoch 35/240, Loss: 24.399353, Train_MMSE: 0.036648, NMMSE: 0.030961, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:27:39] Epoch 36/240, Loss: 24.529230, Train_MMSE: 0.036627, NMMSE: 0.03088, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:30:00] Epoch 37/240, Loss: 24.549950, Train_MMSE: 0.036603, NMMSE: 0.030935, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:32:22] Epoch 38/240, Loss: 24.550671, Train_MMSE: 0.036588, NMMSE: 0.030922, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:34:43] Epoch 39/240, Loss: 24.880556, Train_MMSE: 0.036558, NMMSE: 0.030938, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:37:04] Epoch 40/240, Loss: 24.699032, Train_MMSE: 0.036541, NMMSE: 0.030882, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:39:24] Epoch 41/240, Loss: 24.471397, Train_MMSE: 0.036521, NMMSE: 0.030879, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:41:41] Epoch 42/240, Loss: 24.549505, Train_MMSE: 0.036495, NMMSE: 0.030949, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:44:02] Epoch 43/240, Loss: 24.539221, Train_MMSE: 0.036467, NMMSE: 0.03094, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:46:21] Epoch 44/240, Loss: 24.615271, Train_MMSE: 0.036453, NMMSE: 0.030875, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:48:41] Epoch 45/240, Loss: 24.748787, Train_MMSE: 0.036435, NMMSE: 0.030796, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:51:01] Epoch 46/240, Loss: 24.379694, Train_MMSE: 0.036411, NMMSE: 0.030734, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:53:22] Epoch 47/240, Loss: 24.348846, Train_MMSE: 0.036394, NMMSE: 0.030736, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:55:44] Epoch 48/240, Loss: 24.498102, Train_MMSE: 0.036369, NMMSE: 0.030818, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 16:58:02] Epoch 49/240, Loss: 24.589376, Train_MMSE: 0.036345, NMMSE: 0.030872, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:00:21] Epoch 50/240, Loss: 24.538744, Train_MMSE: 0.036323, NMMSE: 0.030892, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:02:40] Epoch 51/240, Loss: 24.569166, Train_MMSE: 0.036305, NMMSE: 0.030691, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:04:56] Epoch 52/240, Loss: 24.469795, Train_MMSE: 0.036268, NMMSE: 0.030755, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:07:15] Epoch 53/240, Loss: 24.468033, Train_MMSE: 0.036242, NMMSE: 0.030654, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:09:35] Epoch 54/240, Loss: 24.535782, Train_MMSE: 0.036213, NMMSE: 0.030795, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:11:56] Epoch 55/240, Loss: 24.447262, Train_MMSE: 0.036199, NMMSE: 0.030569, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:14:16] Epoch 56/240, Loss: 24.364044, Train_MMSE: 0.036164, NMMSE: 0.030626, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:16:37] Epoch 57/240, Loss: 24.257494, Train_MMSE: 0.036146, NMMSE: 0.030576, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:18:58] Epoch 58/240, Loss: 24.445356, Train_MMSE: 0.036122, NMMSE: 0.030579, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:21:17] Epoch 59/240, Loss: 24.392010, Train_MMSE: 0.036088, NMMSE: 0.030628, LS_NMSE: 0.040528, Lr: 0.001
[2025-02-24 17:23:36] Epoch 60/240, Loss: 24.558617, Train_MMSE: 0.036078, NMMSE: 0.030561, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:25:56] Epoch 61/240, Loss: 23.996847, Train_MMSE: 0.035393, NMMSE: 0.029982, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:28:15] Epoch 62/240, Loss: 23.931940, Train_MMSE: 0.035245, NMMSE: 0.029956, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:30:33] Epoch 63/240, Loss: 23.985834, Train_MMSE: 0.035185, NMMSE: 0.029956, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:32:53] Epoch 64/240, Loss: 23.698421, Train_MMSE: 0.03514, NMMSE: 0.029926, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:35:13] Epoch 65/240, Loss: 24.125244, Train_MMSE: 0.0351, NMMSE: 0.029935, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:37:33] Epoch 66/240, Loss: 24.034628, Train_MMSE: 0.03507, NMMSE: 0.029914, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:39:51] Epoch 67/240, Loss: 24.006130, Train_MMSE: 0.035036, NMMSE: 0.029916, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:42:14] Epoch 68/240, Loss: 23.976238, Train_MMSE: 0.035006, NMMSE: 0.029922, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:44:36] Epoch 69/240, Loss: 24.083693, Train_MMSE: 0.034983, NMMSE: 0.029912, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:47:14] Epoch 70/240, Loss: 23.834625, Train_MMSE: 0.034959, NMMSE: 0.029921, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:49:47] Epoch 71/240, Loss: 24.010338, Train_MMSE: 0.034935, NMMSE: 0.029915, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:52:20] Epoch 72/240, Loss: 23.862085, Train_MMSE: 0.034916, NMMSE: 0.029919, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:54:57] Epoch 73/240, Loss: 23.959225, Train_MMSE: 0.034894, NMMSE: 0.029928, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 17:57:34] Epoch 74/240, Loss: 23.834795, Train_MMSE: 0.034871, NMMSE: 0.029943, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:00:04] Epoch 75/240, Loss: 24.065622, Train_MMSE: 0.034855, NMMSE: 0.02992, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:02:28] Epoch 76/240, Loss: 24.021326, Train_MMSE: 0.034836, NMMSE: 0.029927, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:04:57] Epoch 77/240, Loss: 24.110544, Train_MMSE: 0.034821, NMMSE: 0.029941, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:07:27] Epoch 78/240, Loss: 23.797802, Train_MMSE: 0.034808, NMMSE: 0.029938, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:10:00] Epoch 79/240, Loss: 23.651377, Train_MMSE: 0.034791, NMMSE: 0.029946, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:12:29] Epoch 80/240, Loss: 23.934553, Train_MMSE: 0.034781, NMMSE: 0.029946, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:15:01] Epoch 81/240, Loss: 23.991920, Train_MMSE: 0.034763, NMMSE: 0.029947, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:17:31] Epoch 82/240, Loss: 24.027079, Train_MMSE: 0.034748, NMMSE: 0.029948, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:19:54] Epoch 83/240, Loss: 23.633806, Train_MMSE: 0.034739, NMMSE: 0.029963, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:22:17] Epoch 84/240, Loss: 23.810575, Train_MMSE: 0.034724, NMMSE: 0.029982, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:24:38] Epoch 85/240, Loss: 23.893713, Train_MMSE: 0.034712, NMMSE: 0.029967, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:27:01] Epoch 86/240, Loss: 23.823055, Train_MMSE: 0.034701, NMMSE: 0.029962, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:29:18] Epoch 87/240, Loss: 23.849995, Train_MMSE: 0.034688, NMMSE: 0.030007, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:31:22] Epoch 88/240, Loss: 23.793430, Train_MMSE: 0.034681, NMMSE: 0.029986, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:33:26] Epoch 89/240, Loss: 23.766958, Train_MMSE: 0.034665, NMMSE: 0.030004, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:35:29] Epoch 90/240, Loss: 24.051226, Train_MMSE: 0.034652, NMMSE: 0.029988, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:37:33] Epoch 91/240, Loss: 23.692457, Train_MMSE: 0.034647, NMMSE: 0.029998, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:39:39] Epoch 92/240, Loss: 23.928516, Train_MMSE: 0.034636, NMMSE: 0.03003, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:41:42] Epoch 93/240, Loss: 23.763727, Train_MMSE: 0.034629, NMMSE: 0.030018, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:43:47] Epoch 94/240, Loss: 23.829819, Train_MMSE: 0.034618, NMMSE: 0.030029, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:45:51] Epoch 95/240, Loss: 23.942856, Train_MMSE: 0.034607, NMMSE: 0.030019, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:47:55] Epoch 96/240, Loss: 23.926249, Train_MMSE: 0.034598, NMMSE: 0.030021, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:49:58] Epoch 97/240, Loss: 23.648153, Train_MMSE: 0.034592, NMMSE: 0.030019, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:52:03] Epoch 98/240, Loss: 23.873199, Train_MMSE: 0.034582, NMMSE: 0.030051, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:54:10] Epoch 99/240, Loss: 23.847891, Train_MMSE: 0.034571, NMMSE: 0.030041, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:56:12] Epoch 100/240, Loss: 23.809784, Train_MMSE: 0.034564, NMMSE: 0.030047, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 18:58:15] Epoch 101/240, Loss: 23.585701, Train_MMSE: 0.034554, NMMSE: 0.03004, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:00:20] Epoch 102/240, Loss: 23.631897, Train_MMSE: 0.03455, NMMSE: 0.030061, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:02:26] Epoch 103/240, Loss: 23.885370, Train_MMSE: 0.034541, NMMSE: 0.030052, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:04:29] Epoch 104/240, Loss: 23.767847, Train_MMSE: 0.034532, NMMSE: 0.030061, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:06:34] Epoch 105/240, Loss: 23.819407, Train_MMSE: 0.034525, NMMSE: 0.030058, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:08:35] Epoch 106/240, Loss: 23.780424, Train_MMSE: 0.034518, NMMSE: 0.030072, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:10:38] Epoch 107/240, Loss: 23.737862, Train_MMSE: 0.03451, NMMSE: 0.030117, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:12:43] Epoch 108/240, Loss: 23.713291, Train_MMSE: 0.034504, NMMSE: 0.030075, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:14:45] Epoch 109/240, Loss: 23.836716, Train_MMSE: 0.034496, NMMSE: 0.030092, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:16:49] Epoch 110/240, Loss: 23.532202, Train_MMSE: 0.034489, NMMSE: 0.030112, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:18:54] Epoch 111/240, Loss: 23.592501, Train_MMSE: 0.034486, NMMSE: 0.030111, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:21:00] Epoch 112/240, Loss: 23.684008, Train_MMSE: 0.034475, NMMSE: 0.03013, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:23:03] Epoch 113/240, Loss: 23.690212, Train_MMSE: 0.034472, NMMSE: 0.030123, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:25:10] Epoch 114/240, Loss: 23.732111, Train_MMSE: 0.034463, NMMSE: 0.030115, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:27:15] Epoch 115/240, Loss: 23.925133, Train_MMSE: 0.034454, NMMSE: 0.03011, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:29:18] Epoch 116/240, Loss: 23.734629, Train_MMSE: 0.034449, NMMSE: 0.030124, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:31:22] Epoch 117/240, Loss: 23.975000, Train_MMSE: 0.034445, NMMSE: 0.030133, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:33:27] Epoch 118/240, Loss: 23.782591, Train_MMSE: 0.034439, NMMSE: 0.030152, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:35:32] Epoch 119/240, Loss: 23.609715, Train_MMSE: 0.034433, NMMSE: 0.030119, LS_NMSE: 0.040528, Lr: 0.0001
[2025-02-24 19:37:36] Epoch 120/240, Loss: 23.727777, Train_MMSE: 0.034427, NMMSE: 0.030162, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:39:43] Epoch 121/240, Loss: 23.579630, Train_MMSE: 0.03421, NMMSE: 0.030055, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:41:46] Epoch 122/240, Loss: 23.644770, Train_MMSE: 0.034183, NMMSE: 0.030063, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:43:48] Epoch 123/240, Loss: 23.486372, Train_MMSE: 0.034176, NMMSE: 0.030076, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:45:53] Epoch 124/240, Loss: 23.332920, Train_MMSE: 0.034171, NMMSE: 0.030078, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:47:54] Epoch 125/240, Loss: 23.562920, Train_MMSE: 0.03417, NMMSE: 0.030089, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:49:56] Epoch 126/240, Loss: 23.559595, Train_MMSE: 0.034165, NMMSE: 0.030091, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:52:00] Epoch 127/240, Loss: 23.644159, Train_MMSE: 0.034162, NMMSE: 0.030096, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:54:08] Epoch 128/240, Loss: 23.640680, Train_MMSE: 0.03416, NMMSE: 0.030092, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:56:13] Epoch 129/240, Loss: 23.658974, Train_MMSE: 0.034157, NMMSE: 0.030105, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 19:58:18] Epoch 130/240, Loss: 23.673969, Train_MMSE: 0.034158, NMMSE: 0.030101, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:00:26] Epoch 131/240, Loss: 23.511047, Train_MMSE: 0.034155, NMMSE: 0.030109, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:02:29] Epoch 132/240, Loss: 23.669241, Train_MMSE: 0.034152, NMMSE: 0.030111, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:04:31] Epoch 133/240, Loss: 23.605761, Train_MMSE: 0.034153, NMMSE: 0.030113, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:06:37] Epoch 134/240, Loss: 23.578560, Train_MMSE: 0.03415, NMMSE: 0.030117, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:08:39] Epoch 135/240, Loss: 23.506697, Train_MMSE: 0.034148, NMMSE: 0.030119, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:10:45] Epoch 136/240, Loss: 23.578869, Train_MMSE: 0.034147, NMMSE: 0.030119, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:12:52] Epoch 137/240, Loss: 23.703617, Train_MMSE: 0.034148, NMMSE: 0.030114, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:15:02] Epoch 138/240, Loss: 23.725109, Train_MMSE: 0.034144, NMMSE: 0.030123, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:17:05] Epoch 139/240, Loss: 23.441906, Train_MMSE: 0.034143, NMMSE: 0.030125, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:19:07] Epoch 140/240, Loss: 23.440866, Train_MMSE: 0.034139, NMMSE: 0.030126, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:21:13] Epoch 141/240, Loss: 23.492741, Train_MMSE: 0.034139, NMMSE: 0.030131, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:23:19] Epoch 142/240, Loss: 23.625244, Train_MMSE: 0.034137, NMMSE: 0.03013, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:25:22] Epoch 143/240, Loss: 23.611963, Train_MMSE: 0.034136, NMMSE: 0.030131, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:27:26] Epoch 144/240, Loss: 23.547020, Train_MMSE: 0.034134, NMMSE: 0.030127, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:29:30] Epoch 145/240, Loss: 23.503956, Train_MMSE: 0.034135, NMMSE: 0.030134, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:31:36] Epoch 146/240, Loss: 23.585728, Train_MMSE: 0.034132, NMMSE: 0.030139, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:33:40] Epoch 147/240, Loss: 23.459814, Train_MMSE: 0.034134, NMMSE: 0.030138, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:35:45] Epoch 148/240, Loss: 23.564362, Train_MMSE: 0.03413, NMMSE: 0.03014, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:37:49] Epoch 149/240, Loss: 23.674904, Train_MMSE: 0.034127, NMMSE: 0.030141, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:39:52] Epoch 150/240, Loss: 23.589836, Train_MMSE: 0.034128, NMMSE: 0.030138, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:41:58] Epoch 151/240, Loss: 23.495199, Train_MMSE: 0.034126, NMMSE: 0.030146, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:44:00] Epoch 152/240, Loss: 23.389479, Train_MMSE: 0.034126, NMMSE: 0.030148, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:46:07] Epoch 153/240, Loss: 23.490290, Train_MMSE: 0.034127, NMMSE: 0.030143, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:48:13] Epoch 154/240, Loss: 23.634760, Train_MMSE: 0.034124, NMMSE: 0.030146, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:50:19] Epoch 155/240, Loss: 23.454203, Train_MMSE: 0.034122, NMMSE: 0.030154, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:52:26] Epoch 156/240, Loss: 23.481710, Train_MMSE: 0.034122, NMMSE: 0.03015, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:54:32] Epoch 157/240, Loss: 23.503624, Train_MMSE: 0.034119, NMMSE: 0.030155, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:56:38] Epoch 158/240, Loss: 23.486345, Train_MMSE: 0.034119, NMMSE: 0.030152, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 20:58:46] Epoch 159/240, Loss: 23.639544, Train_MMSE: 0.034118, NMMSE: 0.030148, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:00:50] Epoch 160/240, Loss: 23.518894, Train_MMSE: 0.034119, NMMSE: 0.030155, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:02:57] Epoch 161/240, Loss: 23.555119, Train_MMSE: 0.034116, NMMSE: 0.030157, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:05:01] Epoch 162/240, Loss: 23.554884, Train_MMSE: 0.034115, NMMSE: 0.030157, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:07:02] Epoch 163/240, Loss: 23.622555, Train_MMSE: 0.034113, NMMSE: 0.030159, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:08:47] Epoch 164/240, Loss: 23.439596, Train_MMSE: 0.034113, NMMSE: 0.030157, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:10:32] Epoch 165/240, Loss: 23.560160, Train_MMSE: 0.034112, NMMSE: 0.030165, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:12:19] Epoch 166/240, Loss: 23.478933, Train_MMSE: 0.034111, NMMSE: 0.030161, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:14:04] Epoch 167/240, Loss: 23.595921, Train_MMSE: 0.03411, NMMSE: 0.030172, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:15:50] Epoch 168/240, Loss: 23.440542, Train_MMSE: 0.03411, NMMSE: 0.03016, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:17:36] Epoch 169/240, Loss: 23.556288, Train_MMSE: 0.034105, NMMSE: 0.030159, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:19:23] Epoch 170/240, Loss: 23.462868, Train_MMSE: 0.034107, NMMSE: 0.030164, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:21:10] Epoch 171/240, Loss: 23.590925, Train_MMSE: 0.034107, NMMSE: 0.030167, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:22:51] Epoch 172/240, Loss: 23.680140, Train_MMSE: 0.034105, NMMSE: 0.030171, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:24:25] Epoch 173/240, Loss: 23.418266, Train_MMSE: 0.034104, NMMSE: 0.030176, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:26:02] Epoch 174/240, Loss: 23.564219, Train_MMSE: 0.034103, NMMSE: 0.030169, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:27:40] Epoch 175/240, Loss: 23.522408, Train_MMSE: 0.034103, NMMSE: 0.030171, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:29:21] Epoch 176/240, Loss: 23.517420, Train_MMSE: 0.034102, NMMSE: 0.030177, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:31:00] Epoch 177/240, Loss: 23.667240, Train_MMSE: 0.034099, NMMSE: 0.030176, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:32:37] Epoch 178/240, Loss: 23.656452, Train_MMSE: 0.034099, NMMSE: 0.030171, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:34:17] Epoch 179/240, Loss: 23.382856, Train_MMSE: 0.034096, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1e-05
[2025-02-24 21:35:55] Epoch 180/240, Loss: 23.539047, Train_MMSE: 0.034096, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:37:34] Epoch 181/240, Loss: 23.520746, Train_MMSE: 0.034067, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:39:14] Epoch 182/240, Loss: 23.397177, Train_MMSE: 0.034066, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:40:54] Epoch 183/240, Loss: 23.656708, Train_MMSE: 0.034062, NMMSE: 0.030172, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:42:34] Epoch 184/240, Loss: 23.344940, Train_MMSE: 0.034061, NMMSE: 0.030172, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:44:12] Epoch 185/240, Loss: 23.529442, Train_MMSE: 0.034064, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:45:47] Epoch 186/240, Loss: 23.423674, Train_MMSE: 0.034062, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:47:26] Epoch 187/240, Loss: 23.508478, Train_MMSE: 0.034063, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:49:05] Epoch 188/240, Loss: 23.537563, Train_MMSE: 0.034063, NMMSE: 0.030172, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:50:43] Epoch 189/240, Loss: 23.598011, Train_MMSE: 0.034064, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:52:23] Epoch 190/240, Loss: 23.375027, Train_MMSE: 0.034063, NMMSE: 0.030175, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:54:01] Epoch 191/240, Loss: 23.589916, Train_MMSE: 0.034065, NMMSE: 0.030174, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:55:44] Epoch 192/240, Loss: 23.482744, Train_MMSE: 0.03406, NMMSE: 0.030174, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:57:21] Epoch 193/240, Loss: 23.494396, Train_MMSE: 0.034063, NMMSE: 0.030173, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 21:58:59] Epoch 194/240, Loss: 23.745399, Train_MMSE: 0.034061, NMMSE: 0.030174, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:00:40] Epoch 195/240, Loss: 23.482748, Train_MMSE: 0.034062, NMMSE: 0.030175, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:02:16] Epoch 196/240, Loss: 23.390108, Train_MMSE: 0.034062, NMMSE: 0.030172, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:03:54] Epoch 197/240, Loss: 23.576982, Train_MMSE: 0.034062, NMMSE: 0.030175, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:05:34] Epoch 198/240, Loss: 23.552380, Train_MMSE: 0.03406, NMMSE: 0.030177, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:07:16] Epoch 199/240, Loss: 23.559931, Train_MMSE: 0.034062, NMMSE: 0.030175, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:08:54] Epoch 200/240, Loss: 23.305510, Train_MMSE: 0.034063, NMMSE: 0.030177, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:10:33] Epoch 201/240, Loss: 23.461426, Train_MMSE: 0.034063, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:12:16] Epoch 202/240, Loss: 23.520267, Train_MMSE: 0.034062, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:13:53] Epoch 203/240, Loss: 23.581461, Train_MMSE: 0.034062, NMMSE: 0.030177, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:15:32] Epoch 204/240, Loss: 23.574162, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:17:09] Epoch 205/240, Loss: 23.330982, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:18:46] Epoch 206/240, Loss: 23.334047, Train_MMSE: 0.03406, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:20:28] Epoch 207/240, Loss: 23.628113, Train_MMSE: 0.034059, NMMSE: 0.030176, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:22:08] Epoch 208/240, Loss: 23.500223, Train_MMSE: 0.034063, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:23:47] Epoch 209/240, Loss: 23.409307, Train_MMSE: 0.034061, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:25:23] Epoch 210/240, Loss: 23.469208, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:27:03] Epoch 211/240, Loss: 23.540770, Train_MMSE: 0.034062, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:28:43] Epoch 212/240, Loss: 23.563181, Train_MMSE: 0.03406, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:30:22] Epoch 213/240, Loss: 23.513947, Train_MMSE: 0.03406, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:31:57] Epoch 214/240, Loss: 23.594839, Train_MMSE: 0.03406, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:33:36] Epoch 215/240, Loss: 23.398378, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:35:14] Epoch 216/240, Loss: 23.378267, Train_MMSE: 0.03406, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:36:47] Epoch 217/240, Loss: 23.572309, Train_MMSE: 0.03406, NMMSE: 0.030182, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:38:24] Epoch 218/240, Loss: 23.485020, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:40:07] Epoch 219/240, Loss: 23.572514, Train_MMSE: 0.03406, NMMSE: 0.030182, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:41:44] Epoch 220/240, Loss: 23.506638, Train_MMSE: 0.034061, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:43:28] Epoch 221/240, Loss: 23.317282, Train_MMSE: 0.034059, NMMSE: 0.030179, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:45:08] Epoch 222/240, Loss: 23.701851, Train_MMSE: 0.03406, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:46:45] Epoch 223/240, Loss: 23.418121, Train_MMSE: 0.034059, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:48:18] Epoch 224/240, Loss: 23.438425, Train_MMSE: 0.03406, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:49:57] Epoch 225/240, Loss: 23.631569, Train_MMSE: 0.034058, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:51:36] Epoch 226/240, Loss: 23.404673, Train_MMSE: 0.03406, NMMSE: 0.030184, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:53:12] Epoch 227/240, Loss: 23.697172, Train_MMSE: 0.03406, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:54:44] Epoch 228/240, Loss: 23.277004, Train_MMSE: 0.03406, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:56:08] Epoch 229/240, Loss: 23.540798, Train_MMSE: 0.034058, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:57:23] Epoch 230/240, Loss: 23.668343, Train_MMSE: 0.034059, NMMSE: 0.030178, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 22:58:43] Epoch 231/240, Loss: 23.499252, Train_MMSE: 0.034062, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:00:03] Epoch 232/240, Loss: 23.479200, Train_MMSE: 0.034058, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:01:23] Epoch 233/240, Loss: 23.435324, Train_MMSE: 0.034059, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:02:43] Epoch 234/240, Loss: 23.570118, Train_MMSE: 0.034057, NMMSE: 0.030182, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:04:04] Epoch 235/240, Loss: 23.577717, Train_MMSE: 0.034059, NMMSE: 0.03018, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:05:24] Epoch 236/240, Loss: 23.649515, Train_MMSE: 0.034056, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:06:47] Epoch 237/240, Loss: 23.552376, Train_MMSE: 0.034057, NMMSE: 0.030184, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:08:07] Epoch 238/240, Loss: 23.541378, Train_MMSE: 0.034059, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:09:24] Epoch 239/240, Loss: 23.552713, Train_MMSE: 0.034059, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:41] Epoch 240/240, Loss: 23.491821, Train_MMSE: 0.034056, NMMSE: 0.030183, LS_NMSE: 0.040528, Lr: 1.0000000000000002e-07
