Train.py PID: 24964

H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.05240398605031261
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-15_N36_K4_L4_S9_Setup100_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-15_N36_K4_L4_S9_Setup10_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S9_Setup10_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f3091c9da30>
loss function:: SmoothL1Loss()
[2025-02-24 23:28:01] Epoch 1/240, Loss: 43.621887, Train_MMSE: 0.143293, NMMSE: 0.111345, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:28:25] Epoch 2/240, Loss: 39.246449, Train_MMSE: 0.103672, NMMSE: 0.092996, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:28:49] Epoch 3/240, Loss: 38.155720, Train_MMSE: 0.092386, NMMSE: 0.086917, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:29:13] Epoch 4/240, Loss: 37.542725, Train_MMSE: 0.087998, NMMSE: 0.085296, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:29:37] Epoch 5/240, Loss: 36.490925, Train_MMSE: 0.085314, NMMSE: 0.083327, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:30:02] Epoch 6/240, Loss: 36.445362, Train_MMSE: 0.083548, NMMSE: 0.081988, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:30:25] Epoch 7/240, Loss: 36.403305, Train_MMSE: 0.082184, NMMSE: 0.08228, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:30:50] Epoch 8/240, Loss: 35.791004, Train_MMSE: 0.081136, NMMSE: 0.080635, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:31:14] Epoch 9/240, Loss: 35.640984, Train_MMSE: 0.080258, NMMSE: 0.079688, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:31:39] Epoch 10/240, Loss: 35.487080, Train_MMSE: 0.079517, NMMSE: 0.079589, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:32:02] Epoch 11/240, Loss: 35.574360, Train_MMSE: 0.07887, NMMSE: 0.078838, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:32:26] Epoch 12/240, Loss: 35.365234, Train_MMSE: 0.078254, NMMSE: 0.079097, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:32:50] Epoch 13/240, Loss: 35.695286, Train_MMSE: 0.077678, NMMSE: 0.078904, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:33:14] Epoch 14/240, Loss: 35.007744, Train_MMSE: 0.077078, NMMSE: 0.07832, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:33:39] Epoch 15/240, Loss: 34.959904, Train_MMSE: 0.076622, NMMSE: 0.078023, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:34:03] Epoch 16/240, Loss: 35.014866, Train_MMSE: 0.076029, NMMSE: 0.077726, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:34:28] Epoch 17/240, Loss: 34.939827, Train_MMSE: 0.075645, NMMSE: 0.077717, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:34:51] Epoch 18/240, Loss: 34.705818, Train_MMSE: 0.075206, NMMSE: 0.077771, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:35:15] Epoch 19/240, Loss: 34.692364, Train_MMSE: 0.074785, NMMSE: 0.077516, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:35:39] Epoch 20/240, Loss: 34.522121, Train_MMSE: 0.074335, NMMSE: 0.07718, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:36:02] Epoch 21/240, Loss: 34.503563, Train_MMSE: 0.073991, NMMSE: 0.077592, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:36:26] Epoch 22/240, Loss: 34.271664, Train_MMSE: 0.073604, NMMSE: 0.077342, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:36:50] Epoch 23/240, Loss: 34.600430, Train_MMSE: 0.073221, NMMSE: 0.077297, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:37:14] Epoch 24/240, Loss: 34.260662, Train_MMSE: 0.072938, NMMSE: 0.077397, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:37:37] Epoch 25/240, Loss: 34.058048, Train_MMSE: 0.072543, NMMSE: 0.07714, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:38:01] Epoch 26/240, Loss: 33.589359, Train_MMSE: 0.072272, NMMSE: 0.077017, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:38:26] Epoch 27/240, Loss: 34.127563, Train_MMSE: 0.07186, NMMSE: 0.077082, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:38:49] Epoch 28/240, Loss: 33.942001, Train_MMSE: 0.071625, NMMSE: 0.077798, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:39:12] Epoch 29/240, Loss: 33.716236, Train_MMSE: 0.071421, NMMSE: 0.07751, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:39:37] Epoch 30/240, Loss: 33.624744, Train_MMSE: 0.071047, NMMSE: 0.077343, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:40:01] Epoch 31/240, Loss: 33.544384, Train_MMSE: 0.070795, NMMSE: 0.077704, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:40:25] Epoch 32/240, Loss: 33.407139, Train_MMSE: 0.07053, NMMSE: 0.077289, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:40:49] Epoch 33/240, Loss: 33.423725, Train_MMSE: 0.070277, NMMSE: 0.077333, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:41:14] Epoch 34/240, Loss: 33.576069, Train_MMSE: 0.069992, NMMSE: 0.07774, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:41:39] Epoch 35/240, Loss: 33.578823, Train_MMSE: 0.069841, NMMSE: 0.077845, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:42:03] Epoch 36/240, Loss: 33.001945, Train_MMSE: 0.069573, NMMSE: 0.078013, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:42:27] Epoch 37/240, Loss: 33.596691, Train_MMSE: 0.069322, NMMSE: 0.077686, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:42:51] Epoch 38/240, Loss: 33.327435, Train_MMSE: 0.069088, NMMSE: 0.077567, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:43:15] Epoch 39/240, Loss: 33.311806, Train_MMSE: 0.068965, NMMSE: 0.078158, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:43:39] Epoch 40/240, Loss: 33.156628, Train_MMSE: 0.068656, NMMSE: 0.077299, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:44:04] Epoch 41/240, Loss: 33.223835, Train_MMSE: 0.068455, NMMSE: 0.078122, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:44:28] Epoch 42/240, Loss: 33.276222, Train_MMSE: 0.068331, NMMSE: 0.077486, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:44:52] Epoch 43/240, Loss: 33.043327, Train_MMSE: 0.06811, NMMSE: 0.078345, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:45:16] Epoch 44/240, Loss: 33.050438, Train_MMSE: 0.067916, NMMSE: 0.078462, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:45:40] Epoch 45/240, Loss: 32.857544, Train_MMSE: 0.067788, NMMSE: 0.07843, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:46:04] Epoch 46/240, Loss: 32.814392, Train_MMSE: 0.067635, NMMSE: 0.078431, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:46:29] Epoch 47/240, Loss: 32.845356, Train_MMSE: 0.067448, NMMSE: 0.078211, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:46:52] Epoch 48/240, Loss: 32.761986, Train_MMSE: 0.067227, NMMSE: 0.078815, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:47:17] Epoch 49/240, Loss: 32.681374, Train_MMSE: 0.067103, NMMSE: 0.079081, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:47:41] Epoch 50/240, Loss: 32.565380, Train_MMSE: 0.066898, NMMSE: 0.079719, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:48:05] Epoch 51/240, Loss: 32.522213, Train_MMSE: 0.066759, NMMSE: 0.078827, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:48:28] Epoch 52/240, Loss: 32.457020, Train_MMSE: 0.066676, NMMSE: 0.07863, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:48:53] Epoch 53/240, Loss: 32.667149, Train_MMSE: 0.066557, NMMSE: 0.079172, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:49:15] Epoch 54/240, Loss: 32.408428, Train_MMSE: 0.066345, NMMSE: 0.078404, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:49:36] Epoch 55/240, Loss: 32.834034, Train_MMSE: 0.066199, NMMSE: 0.078769, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:49:58] Epoch 56/240, Loss: 32.767963, Train_MMSE: 0.066103, NMMSE: 0.078863, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:50:20] Epoch 57/240, Loss: 32.758373, Train_MMSE: 0.065958, NMMSE: 0.079119, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:50:42] Epoch 58/240, Loss: 32.499237, Train_MMSE: 0.065801, NMMSE: 0.079317, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:51:04] Epoch 59/240, Loss: 32.567451, Train_MMSE: 0.06576, NMMSE: 0.079206, LS_NMSE: 0.170189, Lr: 0.001
[2025-02-24 23:51:26] Epoch 60/240, Loss: 32.374134, Train_MMSE: 0.065594, NMMSE: 0.079292, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:51:48] Epoch 61/240, Loss: 30.752550, Train_MMSE: 0.06091, NMMSE: 0.077752, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:52:10] Epoch 62/240, Loss: 30.460659, Train_MMSE: 0.059575, NMMSE: 0.078197, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:52:32] Epoch 63/240, Loss: 30.321049, Train_MMSE: 0.059094, NMMSE: 0.078597, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:52:54] Epoch 64/240, Loss: 30.441833, Train_MMSE: 0.058798, NMMSE: 0.079073, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:53:16] Epoch 65/240, Loss: 30.129299, Train_MMSE: 0.058555, NMMSE: 0.079158, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:53:38] Epoch 66/240, Loss: 30.108755, Train_MMSE: 0.058354, NMMSE: 0.079403, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:54:00] Epoch 67/240, Loss: 30.104879, Train_MMSE: 0.058182, NMMSE: 0.079435, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:54:22] Epoch 68/240, Loss: 30.102871, Train_MMSE: 0.058028, NMMSE: 0.079831, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:54:44] Epoch 69/240, Loss: 30.073338, Train_MMSE: 0.057882, NMMSE: 0.079772, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:55:05] Epoch 70/240, Loss: 29.935146, Train_MMSE: 0.057755, NMMSE: 0.079979, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:55:27] Epoch 71/240, Loss: 29.998501, Train_MMSE: 0.057621, NMMSE: 0.080241, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:55:48] Epoch 72/240, Loss: 29.907457, Train_MMSE: 0.057496, NMMSE: 0.080377, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:56:10] Epoch 73/240, Loss: 29.640467, Train_MMSE: 0.057399, NMMSE: 0.080533, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:56:31] Epoch 74/240, Loss: 29.814291, Train_MMSE: 0.057284, NMMSE: 0.080492, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:56:53] Epoch 75/240, Loss: 29.616337, Train_MMSE: 0.057188, NMMSE: 0.080696, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:57:14] Epoch 76/240, Loss: 29.815681, Train_MMSE: 0.057087, NMMSE: 0.080816, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:57:36] Epoch 77/240, Loss: 29.541616, Train_MMSE: 0.057, NMMSE: 0.080925, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:57:58] Epoch 78/240, Loss: 30.004473, Train_MMSE: 0.056913, NMMSE: 0.081016, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:58:19] Epoch 79/240, Loss: 29.799486, Train_MMSE: 0.056821, NMMSE: 0.081296, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:58:41] Epoch 80/240, Loss: 29.752665, Train_MMSE: 0.056739, NMMSE: 0.081316, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:59:02] Epoch 81/240, Loss: 29.587664, Train_MMSE: 0.056672, NMMSE: 0.081371, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:59:24] Epoch 82/240, Loss: 29.659695, Train_MMSE: 0.056591, NMMSE: 0.081519, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-24 23:59:46] Epoch 83/240, Loss: 29.536722, Train_MMSE: 0.056513, NMMSE: 0.081606, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:00:08] Epoch 84/240, Loss: 29.630287, Train_MMSE: 0.056444, NMMSE: 0.081708, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:00:29] Epoch 85/240, Loss: 29.830605, Train_MMSE: 0.056362, NMMSE: 0.081685, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:00:51] Epoch 86/240, Loss: 29.474722, Train_MMSE: 0.056299, NMMSE: 0.081789, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:01:13] Epoch 87/240, Loss: 29.444544, Train_MMSE: 0.056239, NMMSE: 0.081911, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:01:35] Epoch 88/240, Loss: 29.545139, Train_MMSE: 0.056173, NMMSE: 0.081859, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:01:57] Epoch 89/240, Loss: 29.375235, Train_MMSE: 0.056107, NMMSE: 0.082116, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:02:19] Epoch 90/240, Loss: 29.589603, Train_MMSE: 0.056054, NMMSE: 0.082317, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:02:40] Epoch 91/240, Loss: 29.453783, Train_MMSE: 0.055988, NMMSE: 0.082031, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:03:01] Epoch 92/240, Loss: 29.444239, Train_MMSE: 0.055928, NMMSE: 0.082344, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:03:22] Epoch 93/240, Loss: 29.380501, Train_MMSE: 0.055865, NMMSE: 0.082468, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:03:44] Epoch 94/240, Loss: 29.127380, Train_MMSE: 0.055822, NMMSE: 0.082399, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:04:05] Epoch 95/240, Loss: 29.163988, Train_MMSE: 0.055765, NMMSE: 0.082421, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:04:27] Epoch 96/240, Loss: 29.284815, Train_MMSE: 0.055726, NMMSE: 0.082737, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:04:48] Epoch 97/240, Loss: 29.214735, Train_MMSE: 0.055666, NMMSE: 0.082707, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:05:10] Epoch 98/240, Loss: 29.427233, Train_MMSE: 0.055616, NMMSE: 0.082657, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:05:32] Epoch 99/240, Loss: 29.267954, Train_MMSE: 0.055579, NMMSE: 0.082938, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:05:54] Epoch 100/240, Loss: 29.199900, Train_MMSE: 0.055512, NMMSE: 0.082814, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:06:15] Epoch 101/240, Loss: 29.230686, Train_MMSE: 0.055471, NMMSE: 0.08279, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:06:38] Epoch 102/240, Loss: 29.135498, Train_MMSE: 0.055417, NMMSE: 0.082924, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:07:00] Epoch 103/240, Loss: 29.339899, Train_MMSE: 0.055372, NMMSE: 0.08325, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:07:21] Epoch 104/240, Loss: 29.053663, Train_MMSE: 0.055335, NMMSE: 0.083062, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:07:43] Epoch 105/240, Loss: 28.915859, Train_MMSE: 0.055304, NMMSE: 0.083233, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:08:04] Epoch 106/240, Loss: 29.229113, Train_MMSE: 0.055244, NMMSE: 0.083366, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:08:27] Epoch 107/240, Loss: 29.084774, Train_MMSE: 0.055203, NMMSE: 0.08333, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:08:48] Epoch 108/240, Loss: 29.164724, Train_MMSE: 0.055164, NMMSE: 0.083358, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:09:09] Epoch 109/240, Loss: 29.194485, Train_MMSE: 0.055116, NMMSE: 0.083485, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:09:30] Epoch 110/240, Loss: 29.262659, Train_MMSE: 0.055079, NMMSE: 0.083426, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:09:51] Epoch 111/240, Loss: 29.009029, Train_MMSE: 0.055038, NMMSE: 0.083441, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:10:13] Epoch 112/240, Loss: 29.064362, Train_MMSE: 0.055008, NMMSE: 0.083522, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:10:34] Epoch 113/240, Loss: 29.123003, Train_MMSE: 0.05496, NMMSE: 0.083798, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:10:57] Epoch 114/240, Loss: 29.093044, Train_MMSE: 0.054927, NMMSE: 0.083713, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:11:19] Epoch 115/240, Loss: 28.901659, Train_MMSE: 0.054887, NMMSE: 0.083561, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:11:40] Epoch 116/240, Loss: 29.098627, Train_MMSE: 0.054851, NMMSE: 0.08383, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:12:02] Epoch 117/240, Loss: 29.067183, Train_MMSE: 0.05482, NMMSE: 0.08382, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:12:24] Epoch 118/240, Loss: 29.016026, Train_MMSE: 0.054789, NMMSE: 0.083846, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:12:46] Epoch 119/240, Loss: 29.083269, Train_MMSE: 0.05474, NMMSE: 0.083898, LS_NMSE: 0.170189, Lr: 0.0001
[2025-02-25 00:13:08] Epoch 120/240, Loss: 28.901239, Train_MMSE: 0.054701, NMMSE: 0.084101, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:13:29] Epoch 121/240, Loss: 28.423357, Train_MMSE: 0.053788, NMMSE: 0.083988, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:13:51] Epoch 122/240, Loss: 28.420155, Train_MMSE: 0.053643, NMMSE: 0.084124, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:14:13] Epoch 123/240, Loss: 28.452475, Train_MMSE: 0.053602, NMMSE: 0.084185, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:14:35] Epoch 124/240, Loss: 28.402842, Train_MMSE: 0.053583, NMMSE: 0.084163, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:14:56] Epoch 125/240, Loss: 28.484304, Train_MMSE: 0.053572, NMMSE: 0.08425, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:15:17] Epoch 126/240, Loss: 28.399897, Train_MMSE: 0.053556, NMMSE: 0.084369, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:15:39] Epoch 127/240, Loss: 28.389019, Train_MMSE: 0.05355, NMMSE: 0.084332, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:16:01] Epoch 128/240, Loss: 28.492697, Train_MMSE: 0.053542, NMMSE: 0.084376, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:16:22] Epoch 129/240, Loss: 28.449270, Train_MMSE: 0.053537, NMMSE: 0.084414, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:16:44] Epoch 130/240, Loss: 28.394756, Train_MMSE: 0.053533, NMMSE: 0.084446, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:17:05] Epoch 131/240, Loss: 28.509216, Train_MMSE: 0.053517, NMMSE: 0.084383, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:17:27] Epoch 132/240, Loss: 28.544289, Train_MMSE: 0.053511, NMMSE: 0.084497, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:17:49] Epoch 133/240, Loss: 28.399982, Train_MMSE: 0.053508, NMMSE: 0.084475, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:18:10] Epoch 134/240, Loss: 28.486599, Train_MMSE: 0.0535, NMMSE: 0.084495, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:18:33] Epoch 135/240, Loss: 28.524536, Train_MMSE: 0.053496, NMMSE: 0.084544, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:18:54] Epoch 136/240, Loss: 28.359768, Train_MMSE: 0.053486, NMMSE: 0.084506, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:19:16] Epoch 137/240, Loss: 28.487474, Train_MMSE: 0.053481, NMMSE: 0.084512, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:19:38] Epoch 138/240, Loss: 28.434864, Train_MMSE: 0.053479, NMMSE: 0.084574, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:20:00] Epoch 139/240, Loss: 28.531338, Train_MMSE: 0.053472, NMMSE: 0.084543, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:20:21] Epoch 140/240, Loss: 28.527021, Train_MMSE: 0.053466, NMMSE: 0.084567, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:20:44] Epoch 141/240, Loss: 28.524637, Train_MMSE: 0.053457, NMMSE: 0.084567, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:21:05] Epoch 142/240, Loss: 28.708052, Train_MMSE: 0.053453, NMMSE: 0.084649, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:21:27] Epoch 143/240, Loss: 28.368887, Train_MMSE: 0.053448, NMMSE: 0.084594, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:21:48] Epoch 144/240, Loss: 28.746826, Train_MMSE: 0.053441, NMMSE: 0.084641, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:22:10] Epoch 145/240, Loss: 28.610081, Train_MMSE: 0.053429, NMMSE: 0.084661, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:22:31] Epoch 146/240, Loss: 28.286669, Train_MMSE: 0.053441, NMMSE: 0.084696, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:22:53] Epoch 147/240, Loss: 28.497467, Train_MMSE: 0.053427, NMMSE: 0.084691, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:23:14] Epoch 148/240, Loss: 28.568655, Train_MMSE: 0.053414, NMMSE: 0.084655, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:23:35] Epoch 149/240, Loss: 28.263487, Train_MMSE: 0.053421, NMMSE: 0.084712, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:23:56] Epoch 150/240, Loss: 28.408655, Train_MMSE: 0.053417, NMMSE: 0.084695, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:24:17] Epoch 151/240, Loss: 28.530470, Train_MMSE: 0.053407, NMMSE: 0.084668, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:24:39] Epoch 152/240, Loss: 28.324068, Train_MMSE: 0.053405, NMMSE: 0.084713, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:25:01] Epoch 153/240, Loss: 28.086365, Train_MMSE: 0.053401, NMMSE: 0.084743, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:25:22] Epoch 154/240, Loss: 28.530186, Train_MMSE: 0.053397, NMMSE: 0.084741, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:25:44] Epoch 155/240, Loss: 28.571419, Train_MMSE: 0.053388, NMMSE: 0.084762, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:26:06] Epoch 156/240, Loss: 28.555593, Train_MMSE: 0.053386, NMMSE: 0.084772, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:26:28] Epoch 157/240, Loss: 28.529938, Train_MMSE: 0.053378, NMMSE: 0.084802, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:26:49] Epoch 158/240, Loss: 28.536608, Train_MMSE: 0.053371, NMMSE: 0.084756, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:27:11] Epoch 159/240, Loss: 28.296595, Train_MMSE: 0.053367, NMMSE: 0.084798, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:27:32] Epoch 160/240, Loss: 28.224298, Train_MMSE: 0.053364, NMMSE: 0.084795, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:27:53] Epoch 161/240, Loss: 28.347178, Train_MMSE: 0.053358, NMMSE: 0.084833, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:28:14] Epoch 162/240, Loss: 28.499834, Train_MMSE: 0.053353, NMMSE: 0.084775, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:28:37] Epoch 163/240, Loss: 28.220360, Train_MMSE: 0.053352, NMMSE: 0.084875, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:28:58] Epoch 164/240, Loss: 28.408674, Train_MMSE: 0.053347, NMMSE: 0.084846, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:29:20] Epoch 165/240, Loss: 28.506199, Train_MMSE: 0.05334, NMMSE: 0.084841, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:29:41] Epoch 166/240, Loss: 28.265738, Train_MMSE: 0.053337, NMMSE: 0.084869, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:30:03] Epoch 167/240, Loss: 28.223326, Train_MMSE: 0.053335, NMMSE: 0.084912, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:30:25] Epoch 168/240, Loss: 28.600807, Train_MMSE: 0.053329, NMMSE: 0.084829, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:30:47] Epoch 169/240, Loss: 28.414528, Train_MMSE: 0.05333, NMMSE: 0.084875, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:31:09] Epoch 170/240, Loss: 28.321390, Train_MMSE: 0.053319, NMMSE: 0.084906, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:31:31] Epoch 171/240, Loss: 28.327938, Train_MMSE: 0.053315, NMMSE: 0.084923, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:31:53] Epoch 172/240, Loss: 28.351404, Train_MMSE: 0.05331, NMMSE: 0.084905, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:32:15] Epoch 173/240, Loss: 28.493769, Train_MMSE: 0.053308, NMMSE: 0.084913, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:32:37] Epoch 174/240, Loss: 28.501949, Train_MMSE: 0.053298, NMMSE: 0.084915, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:32:59] Epoch 175/240, Loss: 28.404057, Train_MMSE: 0.053297, NMMSE: 0.084946, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:33:20] Epoch 176/240, Loss: 28.423929, Train_MMSE: 0.053296, NMMSE: 0.085008, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:33:42] Epoch 177/240, Loss: 28.518646, Train_MMSE: 0.053294, NMMSE: 0.084974, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:34:03] Epoch 178/240, Loss: 28.121511, Train_MMSE: 0.053285, NMMSE: 0.085005, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:34:25] Epoch 179/240, Loss: 28.343977, Train_MMSE: 0.053286, NMMSE: 0.084956, LS_NMSE: 0.170189, Lr: 1e-05
[2025-02-25 00:34:46] Epoch 180/240, Loss: 28.359064, Train_MMSE: 0.053277, NMMSE: 0.085003, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:08] Epoch 181/240, Loss: 28.211580, Train_MMSE: 0.053158, NMMSE: 0.084949, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:29] Epoch 182/240, Loss: 28.220127, Train_MMSE: 0.053156, NMMSE: 0.085009, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:35:51] Epoch 183/240, Loss: 28.167101, Train_MMSE: 0.053146, NMMSE: 0.085031, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:36:13] Epoch 184/240, Loss: 28.078735, Train_MMSE: 0.053154, NMMSE: 0.084962, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:36:35] Epoch 185/240, Loss: 28.299959, Train_MMSE: 0.053152, NMMSE: 0.08496, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:36:56] Epoch 186/240, Loss: 28.179073, Train_MMSE: 0.053148, NMMSE: 0.085036, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:37:18] Epoch 187/240, Loss: 28.370506, Train_MMSE: 0.053145, NMMSE: 0.085043, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:37:39] Epoch 188/240, Loss: 28.117968, Train_MMSE: 0.053152, NMMSE: 0.08499, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:37:58] Epoch 189/240, Loss: 28.222965, Train_MMSE: 0.053144, NMMSE: 0.085046, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:11] Epoch 190/240, Loss: 28.181797, Train_MMSE: 0.05315, NMMSE: 0.085014, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:24] Epoch 191/240, Loss: 28.120575, Train_MMSE: 0.053149, NMMSE: 0.085058, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:38] Epoch 192/240, Loss: 28.325062, Train_MMSE: 0.053139, NMMSE: 0.085067, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:38:51] Epoch 193/240, Loss: 28.159618, Train_MMSE: 0.053146, NMMSE: 0.085, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:04] Epoch 194/240, Loss: 28.361406, Train_MMSE: 0.053143, NMMSE: 0.084987, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:17] Epoch 195/240, Loss: 28.315729, Train_MMSE: 0.053142, NMMSE: 0.084986, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:30] Epoch 196/240, Loss: 28.122183, Train_MMSE: 0.053138, NMMSE: 0.085033, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:43] Epoch 197/240, Loss: 28.227091, Train_MMSE: 0.053148, NMMSE: 0.08503, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:39:56] Epoch 198/240, Loss: 28.263548, Train_MMSE: 0.053138, NMMSE: 0.085064, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:09] Epoch 199/240, Loss: 28.259983, Train_MMSE: 0.053142, NMMSE: 0.085043, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:22] Epoch 200/240, Loss: 28.235773, Train_MMSE: 0.053139, NMMSE: 0.085, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:35] Epoch 201/240, Loss: 28.474007, Train_MMSE: 0.053137, NMMSE: 0.084986, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:40:48] Epoch 202/240, Loss: 28.116693, Train_MMSE: 0.053141, NMMSE: 0.085043, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:41:01] Epoch 203/240, Loss: 28.273626, Train_MMSE: 0.05314, NMMSE: 0.08503, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:41:14] Epoch 204/240, Loss: 28.231964, Train_MMSE: 0.053141, NMMSE: 0.085026, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:41:27] Epoch 205/240, Loss: 28.198191, Train_MMSE: 0.053138, NMMSE: 0.08509, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:41:40] Epoch 206/240, Loss: 28.250458, Train_MMSE: 0.053135, NMMSE: 0.084995, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:41:53] Epoch 207/240, Loss: 28.235004, Train_MMSE: 0.053137, NMMSE: 0.084997, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:06] Epoch 208/240, Loss: 28.283943, Train_MMSE: 0.053135, NMMSE: 0.085072, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:19] Epoch 209/240, Loss: 28.125240, Train_MMSE: 0.053134, NMMSE: 0.085026, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:32] Epoch 210/240, Loss: 28.330193, Train_MMSE: 0.053135, NMMSE: 0.085127, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:45] Epoch 211/240, Loss: 28.069448, Train_MMSE: 0.05313, NMMSE: 0.085157, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:42:58] Epoch 212/240, Loss: 28.331112, Train_MMSE: 0.053135, NMMSE: 0.085072, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:11] Epoch 213/240, Loss: 28.197771, Train_MMSE: 0.053138, NMMSE: 0.085029, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:24] Epoch 214/240, Loss: 28.190945, Train_MMSE: 0.053136, NMMSE: 0.085093, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:37] Epoch 215/240, Loss: 28.445930, Train_MMSE: 0.053132, NMMSE: 0.085076, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:43:50] Epoch 216/240, Loss: 28.322975, Train_MMSE: 0.053128, NMMSE: 0.085004, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:03] Epoch 217/240, Loss: 28.222080, Train_MMSE: 0.053129, NMMSE: 0.085052, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:17] Epoch 218/240, Loss: 28.452799, Train_MMSE: 0.053132, NMMSE: 0.08506, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:30] Epoch 219/240, Loss: 28.031961, Train_MMSE: 0.053129, NMMSE: 0.085108, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:43] Epoch 220/240, Loss: 28.547337, Train_MMSE: 0.053139, NMMSE: 0.085051, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:44:56] Epoch 221/240, Loss: 28.352726, Train_MMSE: 0.053131, NMMSE: 0.085066, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:09] Epoch 222/240, Loss: 28.330446, Train_MMSE: 0.053133, NMMSE: 0.085012, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:22] Epoch 223/240, Loss: 28.206095, Train_MMSE: 0.053129, NMMSE: 0.085082, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:35] Epoch 224/240, Loss: 28.246721, Train_MMSE: 0.053129, NMMSE: 0.085107, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:45:48] Epoch 225/240, Loss: 28.159035, Train_MMSE: 0.053129, NMMSE: 0.085052, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:46:01] Epoch 226/240, Loss: 28.306053, Train_MMSE: 0.05313, NMMSE: 0.085041, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:46:14] Epoch 227/240, Loss: 28.337645, Train_MMSE: 0.053126, NMMSE: 0.085046, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:46:26] Epoch 228/240, Loss: 28.366600, Train_MMSE: 0.053117, NMMSE: 0.085021, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:46:39] Epoch 229/240, Loss: 28.075460, Train_MMSE: 0.053131, NMMSE: 0.084964, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:46:52] Epoch 230/240, Loss: 28.098389, Train_MMSE: 0.053125, NMMSE: 0.085044, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:05] Epoch 231/240, Loss: 28.273813, Train_MMSE: 0.05312, NMMSE: 0.085063, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:18] Epoch 232/240, Loss: 28.522804, Train_MMSE: 0.053129, NMMSE: 0.08506, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:31] Epoch 233/240, Loss: 28.244104, Train_MMSE: 0.053128, NMMSE: 0.085006, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:44] Epoch 234/240, Loss: 28.487772, Train_MMSE: 0.053123, NMMSE: 0.085062, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:47:57] Epoch 235/240, Loss: 28.338745, Train_MMSE: 0.053128, NMMSE: 0.085094, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:10] Epoch 236/240, Loss: 28.152712, Train_MMSE: 0.053123, NMMSE: 0.085143, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:23] Epoch 237/240, Loss: 28.290112, Train_MMSE: 0.053132, NMMSE: 0.085111, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:36] Epoch 238/240, Loss: 28.327118, Train_MMSE: 0.053122, NMMSE: 0.085081, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:48:49] Epoch 239/240, Loss: 28.157572, Train_MMSE: 0.053126, NMMSE: 0.085116, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-06
[2025-02-25 00:49:02] Epoch 240/240, Loss: 28.197035, Train_MMSE: 0.053123, NMMSE: 0.085076, LS_NMSE: 0.170189, Lr: 1.0000000000000002e-07
