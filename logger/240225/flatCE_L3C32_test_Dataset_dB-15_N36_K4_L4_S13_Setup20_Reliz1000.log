Train.py PID: 28037

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.01241999814868016
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S13_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S13_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/flatCE_L3C32_test_Dataset_dB-15_N36_K4_L4_S13_Setup20_Reliz1000.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fa2ff135d30>
loss function:: SmoothL1Loss()
[2025-02-24 14:50:34] Epoch 1/240, Loss: 63.017212, Train_MMSE: 0.719756, NMMSE: 0.436369, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 14:52:15] Epoch 2/240, Loss: 15.088893, Train_MMSE: 0.116835, NMMSE: 0.014374, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 14:53:53] Epoch 3/240, Loss: 15.019753, Train_MMSE: 0.013924, NMMSE: 0.014162, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 14:55:29] Epoch 4/240, Loss: 15.012772, Train_MMSE: 0.013604, NMMSE: 0.013801, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 14:57:06] Epoch 5/240, Loss: 14.551485, Train_MMSE: 0.013238, NMMSE: 0.013375, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 14:58:43] Epoch 6/240, Loss: 14.527019, Train_MMSE: 0.013002, NMMSE: 0.013218, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:00:19] Epoch 7/240, Loss: 14.410413, Train_MMSE: 0.012841, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:01:55] Epoch 8/240, Loss: 14.569987, Train_MMSE: 0.012749, NMMSE: 0.01297, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:03:32] Epoch 9/240, Loss: 14.453035, Train_MMSE: 0.012705, NMMSE: 0.012908, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:05:09] Epoch 10/240, Loss: 14.344951, Train_MMSE: 0.012683, NMMSE: 0.013014, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:06:44] Epoch 11/240, Loss: 14.323691, Train_MMSE: 0.012653, NMMSE: 0.012899, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:08:20] Epoch 12/240, Loss: 14.359882, Train_MMSE: 0.012604, NMMSE: 0.012947, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:09:56] Epoch 13/240, Loss: 14.581594, Train_MMSE: 0.0126, NMMSE: 0.012869, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:11:32] Epoch 14/240, Loss: 14.245875, Train_MMSE: 0.012583, NMMSE: 0.01285, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:13:10] Epoch 15/240, Loss: 14.319619, Train_MMSE: 0.012551, NMMSE: 0.012977, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:14:46] Epoch 16/240, Loss: 14.279102, Train_MMSE: 0.012576, NMMSE: 0.012799, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:16:24] Epoch 17/240, Loss: 14.390369, Train_MMSE: 0.012548, NMMSE: 0.012866, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:18:01] Epoch 18/240, Loss: 14.314603, Train_MMSE: 0.012522, NMMSE: 0.012855, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:19:38] Epoch 19/240, Loss: 14.459427, Train_MMSE: 0.012512, NMMSE: 0.012949, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:21:15] Epoch 20/240, Loss: 14.263643, Train_MMSE: 0.012512, NMMSE: 0.012873, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:22:51] Epoch 21/240, Loss: 14.275652, Train_MMSE: 0.012519, NMMSE: 0.012813, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:24:29] Epoch 22/240, Loss: 14.443412, Train_MMSE: 0.012506, NMMSE: 0.012954, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:26:06] Epoch 23/240, Loss: 14.230550, Train_MMSE: 0.012525, NMMSE: 0.01283, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:27:42] Epoch 24/240, Loss: 14.300805, Train_MMSE: 0.012522, NMMSE: 0.012824, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:29:20] Epoch 25/240, Loss: 14.254822, Train_MMSE: 0.012501, NMMSE: 0.012997, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:30:56] Epoch 26/240, Loss: 14.473531, Train_MMSE: 0.012514, NMMSE: 0.012779, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:32:33] Epoch 27/240, Loss: 14.227300, Train_MMSE: 0.012511, NMMSE: 0.012729, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:34:10] Epoch 28/240, Loss: 14.425093, Train_MMSE: 0.012507, NMMSE: 0.012791, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:35:47] Epoch 29/240, Loss: 14.350803, Train_MMSE: 0.012494, NMMSE: 0.012808, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:37:24] Epoch 30/240, Loss: 14.188474, Train_MMSE: 0.012499, NMMSE: 0.012771, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:39:01] Epoch 31/240, Loss: 14.457011, Train_MMSE: 0.012491, NMMSE: 0.012826, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:40:39] Epoch 32/240, Loss: 14.279570, Train_MMSE: 0.012483, NMMSE: 0.012909, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:42:16] Epoch 33/240, Loss: 14.242905, Train_MMSE: 0.012494, NMMSE: 0.012802, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:43:53] Epoch 34/240, Loss: 14.259988, Train_MMSE: 0.012491, NMMSE: 0.012819, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:45:32] Epoch 35/240, Loss: 14.313775, Train_MMSE: 0.012474, NMMSE: 0.012772, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:47:10] Epoch 36/240, Loss: 14.240097, Train_MMSE: 0.012489, NMMSE: 0.012859, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:48:45] Epoch 37/240, Loss: 14.235707, Train_MMSE: 0.012471, NMMSE: 0.012854, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:49:58] Epoch 38/240, Loss: 14.198729, Train_MMSE: 0.012484, NMMSE: 0.012757, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:50:52] Epoch 39/240, Loss: 14.292362, Train_MMSE: 0.012459, NMMSE: 0.01273, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:51:26] Epoch 40/240, Loss: 14.312114, Train_MMSE: 0.01247, NMMSE: 0.012781, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:52:17] Epoch 41/240, Loss: 14.270720, Train_MMSE: 0.012482, NMMSE: 0.012747, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:57:40] Epoch 42/240, Loss: 14.467424, Train_MMSE: 0.012471, NMMSE: 0.012819, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:59:33] Epoch 43/240, Loss: 14.273043, Train_MMSE: 0.012479, NMMSE: 0.012809, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:01:20] Epoch 44/240, Loss: 14.201596, Train_MMSE: 0.012464, NMMSE: 0.012725, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:03:05] Epoch 45/240, Loss: 14.326303, Train_MMSE: 0.012473, NMMSE: 0.013866, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:04:52] Epoch 46/240, Loss: 14.220254, Train_MMSE: 0.012477, NMMSE: 0.012751, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:06:39] Epoch 47/240, Loss: 14.195306, Train_MMSE: 0.012478, NMMSE: 0.012738, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:08:25] Epoch 48/240, Loss: 14.327769, Train_MMSE: 0.012453, NMMSE: 0.012725, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:10:11] Epoch 49/240, Loss: 14.234752, Train_MMSE: 0.01245, NMMSE: 0.012772, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:11:57] Epoch 50/240, Loss: 14.345842, Train_MMSE: 0.012455, NMMSE: 0.01278, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:13:43] Epoch 51/240, Loss: 14.196055, Train_MMSE: 0.012446, NMMSE: 0.012805, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:15:29] Epoch 52/240, Loss: 14.298396, Train_MMSE: 0.012453, NMMSE: 0.012806, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:17:15] Epoch 53/240, Loss: 14.278985, Train_MMSE: 0.012464, NMMSE: 0.012809, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:19:02] Epoch 54/240, Loss: 14.247087, Train_MMSE: 0.012458, NMMSE: 0.012808, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:20:50] Epoch 55/240, Loss: 14.160353, Train_MMSE: 0.012448, NMMSE: 0.012757, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:22:37] Epoch 56/240, Loss: 14.246553, Train_MMSE: 0.012463, NMMSE: 0.012822, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:24:24] Epoch 57/240, Loss: 14.336824, Train_MMSE: 0.01246, NMMSE: 0.012711, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:26:09] Epoch 58/240, Loss: 14.417784, Train_MMSE: 0.012436, NMMSE: 0.012794, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:27:55] Epoch 59/240, Loss: 14.250385, Train_MMSE: 0.012447, NMMSE: 0.012738, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:29:41] Epoch 60/240, Loss: 14.338742, Train_MMSE: 0.012473, NMMSE: 0.012804, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:31:27] Epoch 61/240, Loss: 14.144756, Train_MMSE: 0.012318, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:33:13] Epoch 62/240, Loss: 14.124863, Train_MMSE: 0.012306, NMMSE: 0.012573, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:34:57] Epoch 63/240, Loss: 14.209876, Train_MMSE: 0.01231, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:36:41] Epoch 64/240, Loss: 14.305200, Train_MMSE: 0.012296, NMMSE: 0.012566, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:38:25] Epoch 65/240, Loss: 14.097756, Train_MMSE: 0.012305, NMMSE: 0.012582, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:40:10] Epoch 66/240, Loss: 14.168241, Train_MMSE: 0.012306, NMMSE: 0.012575, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:41:55] Epoch 67/240, Loss: 14.207307, Train_MMSE: 0.012311, NMMSE: 0.012582, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:43:40] Epoch 68/240, Loss: 14.072398, Train_MMSE: 0.012301, NMMSE: 0.012572, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:45:26] Epoch 69/240, Loss: 14.172786, Train_MMSE: 0.012297, NMMSE: 0.012573, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:47:11] Epoch 70/240, Loss: 14.149907, Train_MMSE: 0.012306, NMMSE: 0.012575, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:48:58] Epoch 71/240, Loss: 14.234116, Train_MMSE: 0.012305, NMMSE: 0.012565, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:50:45] Epoch 72/240, Loss: 14.258528, Train_MMSE: 0.012307, NMMSE: 0.012577, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:52:29] Epoch 73/240, Loss: 14.237522, Train_MMSE: 0.012302, NMMSE: 0.012573, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:54:14] Epoch 74/240, Loss: 14.314579, Train_MMSE: 0.012303, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:55:58] Epoch 75/240, Loss: 14.284749, Train_MMSE: 0.01232, NMMSE: 0.012573, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:57:43] Epoch 76/240, Loss: 14.133311, Train_MMSE: 0.012326, NMMSE: 0.012565, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 16:59:28] Epoch 77/240, Loss: 14.153045, Train_MMSE: 0.012297, NMMSE: 0.012586, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:01:13] Epoch 78/240, Loss: 14.257030, Train_MMSE: 0.0123, NMMSE: 0.012571, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:02:57] Epoch 79/240, Loss: 14.497432, Train_MMSE: 0.01231, NMMSE: 0.012571, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:04:44] Epoch 80/240, Loss: 14.319462, Train_MMSE: 0.012307, NMMSE: 0.012589, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:06:30] Epoch 81/240, Loss: 14.194690, Train_MMSE: 0.012308, NMMSE: 0.012562, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:08:17] Epoch 82/240, Loss: 14.227256, Train_MMSE: 0.012309, NMMSE: 0.012567, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:10:03] Epoch 83/240, Loss: 14.118938, Train_MMSE: 0.012299, NMMSE: 0.012572, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:11:51] Epoch 84/240, Loss: 14.089002, Train_MMSE: 0.012309, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:13:37] Epoch 85/240, Loss: 14.273511, Train_MMSE: 0.012288, NMMSE: 0.012569, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:15:24] Epoch 86/240, Loss: 14.305559, Train_MMSE: 0.012309, NMMSE: 0.012566, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:17:11] Epoch 87/240, Loss: 14.084935, Train_MMSE: 0.012286, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:19:00] Epoch 88/240, Loss: 14.154526, Train_MMSE: 0.0123, NMMSE: 0.012564, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:20:47] Epoch 89/240, Loss: 14.327484, Train_MMSE: 0.012296, NMMSE: 0.012568, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:22:36] Epoch 90/240, Loss: 14.089564, Train_MMSE: 0.012295, NMMSE: 0.012568, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:24:23] Epoch 91/240, Loss: 14.167873, Train_MMSE: 0.012299, NMMSE: 0.012577, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:26:11] Epoch 92/240, Loss: 14.178771, Train_MMSE: 0.012296, NMMSE: 0.012567, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:27:59] Epoch 93/240, Loss: 14.243484, Train_MMSE: 0.012296, NMMSE: 0.012566, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:29:43] Epoch 94/240, Loss: 14.199528, Train_MMSE: 0.012301, NMMSE: 0.01258, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:31:29] Epoch 95/240, Loss: 14.267887, Train_MMSE: 0.012297, NMMSE: 0.01259, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:33:14] Epoch 96/240, Loss: 14.244531, Train_MMSE: 0.012294, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:35:00] Epoch 97/240, Loss: 14.106673, Train_MMSE: 0.012288, NMMSE: 0.012569, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:36:47] Epoch 98/240, Loss: 14.117744, Train_MMSE: 0.012292, NMMSE: 0.01259, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:38:34] Epoch 99/240, Loss: 14.102480, Train_MMSE: 0.01229, NMMSE: 0.012574, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:40:20] Epoch 100/240, Loss: 14.183820, Train_MMSE: 0.012303, NMMSE: 0.012568, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:42:06] Epoch 101/240, Loss: 14.157011, Train_MMSE: 0.012294, NMMSE: 0.012559, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:43:51] Epoch 102/240, Loss: 14.204533, Train_MMSE: 0.012288, NMMSE: 0.012576, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:45:36] Epoch 103/240, Loss: 14.195455, Train_MMSE: 0.012293, NMMSE: 0.01261, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:47:21] Epoch 104/240, Loss: 14.162414, Train_MMSE: 0.01228, NMMSE: 0.012593, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:49:06] Epoch 105/240, Loss: 14.205803, Train_MMSE: 0.012297, NMMSE: 0.012563, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:50:50] Epoch 106/240, Loss: 14.163505, Train_MMSE: 0.012304, NMMSE: 0.012568, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:52:36] Epoch 107/240, Loss: 14.143473, Train_MMSE: 0.01231, NMMSE: 0.012603, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:54:22] Epoch 108/240, Loss: 14.110211, Train_MMSE: 0.01227, NMMSE: 0.012576, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:56:08] Epoch 109/240, Loss: 14.117414, Train_MMSE: 0.012283, NMMSE: 0.01256, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:57:53] Epoch 110/240, Loss: 14.452825, Train_MMSE: 0.012291, NMMSE: 0.012582, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:59:42] Epoch 111/240, Loss: 14.195760, Train_MMSE: 0.012301, NMMSE: 0.012583, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:01:31] Epoch 112/240, Loss: 14.288230, Train_MMSE: 0.012296, NMMSE: 0.012583, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:03:19] Epoch 113/240, Loss: 14.170002, Train_MMSE: 0.012292, NMMSE: 0.012567, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:05:12] Epoch 114/240, Loss: 14.075109, Train_MMSE: 0.012288, NMMSE: 0.012582, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:06:56] Epoch 115/240, Loss: 14.082487, Train_MMSE: 0.012285, NMMSE: 0.012581, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:08:40] Epoch 116/240, Loss: 14.171195, Train_MMSE: 0.012302, NMMSE: 0.012568, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:10:25] Epoch 117/240, Loss: 14.253557, Train_MMSE: 0.012297, NMMSE: 0.012565, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:12:14] Epoch 118/240, Loss: 14.186627, Train_MMSE: 0.012293, NMMSE: 0.012563, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:13:59] Epoch 119/240, Loss: 14.121540, Train_MMSE: 0.012285, NMMSE: 0.01259, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:15:45] Epoch 120/240, Loss: 14.128580, Train_MMSE: 0.012291, NMMSE: 0.012582, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:17:30] Epoch 121/240, Loss: 14.163949, Train_MMSE: 0.012271, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:19:15] Epoch 122/240, Loss: 14.277139, Train_MMSE: 0.012275, NMMSE: 0.012557, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:21:00] Epoch 123/240, Loss: 14.075063, Train_MMSE: 0.01227, NMMSE: 0.01255, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:22:45] Epoch 124/240, Loss: 14.054799, Train_MMSE: 0.012281, NMMSE: 0.012552, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:24:30] Epoch 125/240, Loss: 14.155916, Train_MMSE: 0.012263, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:26:15] Epoch 126/240, Loss: 14.171236, Train_MMSE: 0.012266, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:27:59] Epoch 127/240, Loss: 14.241422, Train_MMSE: 0.012276, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:29:45] Epoch 128/240, Loss: 14.267720, Train_MMSE: 0.012264, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:31:31] Epoch 129/240, Loss: 14.301433, Train_MMSE: 0.012259, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:33:16] Epoch 130/240, Loss: 14.239049, Train_MMSE: 0.012269, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:35:00] Epoch 131/240, Loss: 14.226783, Train_MMSE: 0.012276, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:36:45] Epoch 132/240, Loss: 14.027561, Train_MMSE: 0.012267, NMMSE: 0.012558, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:38:32] Epoch 133/240, Loss: 14.179107, Train_MMSE: 0.012267, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:40:18] Epoch 134/240, Loss: 14.097427, Train_MMSE: 0.012279, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:42:04] Epoch 135/240, Loss: 14.164679, Train_MMSE: 0.012271, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:43:49] Epoch 136/240, Loss: 14.103493, Train_MMSE: 0.012269, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:45:34] Epoch 137/240, Loss: 14.186739, Train_MMSE: 0.012264, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:47:19] Epoch 138/240, Loss: 14.075612, Train_MMSE: 0.012261, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:49:04] Epoch 139/240, Loss: 14.244816, Train_MMSE: 0.012272, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:50:49] Epoch 140/240, Loss: 14.227634, Train_MMSE: 0.012263, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:52:17] Epoch 141/240, Loss: 14.083771, Train_MMSE: 0.012286, NMMSE: 0.012574, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:53:39] Epoch 142/240, Loss: 14.255142, Train_MMSE: 0.012266, NMMSE: 0.012553, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:55:01] Epoch 143/240, Loss: 14.151158, Train_MMSE: 0.012258, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:56:25] Epoch 144/240, Loss: 14.148664, Train_MMSE: 0.01226, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:57:48] Epoch 145/240, Loss: 14.196126, Train_MMSE: 0.012263, NMMSE: 0.01256, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 18:59:12] Epoch 146/240, Loss: 14.154469, Train_MMSE: 0.012264, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:00:35] Epoch 147/240, Loss: 14.160151, Train_MMSE: 0.012267, NMMSE: 0.012585, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:01:59] Epoch 148/240, Loss: 14.235953, Train_MMSE: 0.012266, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:03:23] Epoch 149/240, Loss: 14.131597, Train_MMSE: 0.012252, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:04:45] Epoch 150/240, Loss: 14.114882, Train_MMSE: 0.012285, NMMSE: 0.012569, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:06:08] Epoch 151/240, Loss: 14.300914, Train_MMSE: 0.012264, NMMSE: 0.012558, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:07:30] Epoch 152/240, Loss: 14.233562, Train_MMSE: 0.01227, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:08:53] Epoch 153/240, Loss: 14.276923, Train_MMSE: 0.01227, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:10:15] Epoch 154/240, Loss: 14.124173, Train_MMSE: 0.012275, NMMSE: 0.012561, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:11:39] Epoch 155/240, Loss: 14.142819, Train_MMSE: 0.012265, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:13:02] Epoch 156/240, Loss: 14.086635, Train_MMSE: 0.012266, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:14:26] Epoch 157/240, Loss: 14.237510, Train_MMSE: 0.012273, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:15:50] Epoch 158/240, Loss: 14.160172, Train_MMSE: 0.012265, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:17:13] Epoch 159/240, Loss: 14.106205, Train_MMSE: 0.012277, NMMSE: 0.012593, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:18:37] Epoch 160/240, Loss: 14.056124, Train_MMSE: 0.012274, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:20:01] Epoch 161/240, Loss: 14.108525, Train_MMSE: 0.012268, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:21:27] Epoch 162/240, Loss: 14.105802, Train_MMSE: 0.012264, NMMSE: 0.01257, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:22:52] Epoch 163/240, Loss: 14.223090, Train_MMSE: 0.012277, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:24:17] Epoch 164/240, Loss: 14.212645, Train_MMSE: 0.012257, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:25:41] Epoch 165/240, Loss: 14.103571, Train_MMSE: 0.012264, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:27:03] Epoch 166/240, Loss: 14.145886, Train_MMSE: 0.012259, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:28:26] Epoch 167/240, Loss: 14.142249, Train_MMSE: 0.012275, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:29:48] Epoch 168/240, Loss: 14.380048, Train_MMSE: 0.012262, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:31:11] Epoch 169/240, Loss: 14.114594, Train_MMSE: 0.012268, NMMSE: 0.012563, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:32:35] Epoch 170/240, Loss: 14.026992, Train_MMSE: 0.012256, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:33:59] Epoch 171/240, Loss: 14.197467, Train_MMSE: 0.012259, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:35:23] Epoch 172/240, Loss: 14.308685, Train_MMSE: 0.012279, NMMSE: 0.012573, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:36:46] Epoch 173/240, Loss: 14.266636, Train_MMSE: 0.012249, NMMSE: 0.012576, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:38:10] Epoch 174/240, Loss: 14.225074, Train_MMSE: 0.012277, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:39:34] Epoch 175/240, Loss: 14.130561, Train_MMSE: 0.012273, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:40:59] Epoch 176/240, Loss: 14.015100, Train_MMSE: 0.012267, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:42:24] Epoch 177/240, Loss: 14.137095, Train_MMSE: 0.012267, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:43:49] Epoch 178/240, Loss: 14.480085, Train_MMSE: 0.012276, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:45:14] Epoch 179/240, Loss: 14.154815, Train_MMSE: 0.012272, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 19:46:39] Epoch 180/240, Loss: 14.334211, Train_MMSE: 0.012271, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:48:04] Epoch 181/240, Loss: 14.252518, Train_MMSE: 0.012269, NMMSE: 0.012555, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:49:29] Epoch 182/240, Loss: 14.191783, Train_MMSE: 0.012256, NMMSE: 0.012558, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:50:54] Epoch 183/240, Loss: 14.051400, Train_MMSE: 0.012264, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:52:20] Epoch 184/240, Loss: 14.087750, Train_MMSE: 0.012268, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:53:47] Epoch 185/240, Loss: 14.151072, Train_MMSE: 0.012289, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:55:13] Epoch 186/240, Loss: 14.155408, Train_MMSE: 0.012271, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:56:40] Epoch 187/240, Loss: 14.094606, Train_MMSE: 0.012271, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:58:05] Epoch 188/240, Loss: 14.294150, Train_MMSE: 0.012262, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 19:59:31] Epoch 189/240, Loss: 14.138501, Train_MMSE: 0.012262, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:00:57] Epoch 190/240, Loss: 14.087154, Train_MMSE: 0.012268, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:02:23] Epoch 191/240, Loss: 14.171644, Train_MMSE: 0.012269, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:03:50] Epoch 192/240, Loss: 14.155796, Train_MMSE: 0.012267, NMMSE: 0.012565, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:05:16] Epoch 193/240, Loss: 14.090817, Train_MMSE: 0.012263, NMMSE: 0.012563, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:06:43] Epoch 194/240, Loss: 14.087977, Train_MMSE: 0.012271, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:08:09] Epoch 195/240, Loss: 14.159029, Train_MMSE: 0.012264, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:09:18] Epoch 196/240, Loss: 14.169558, Train_MMSE: 0.01226, NMMSE: 0.012552, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:10:20] Epoch 197/240, Loss: 14.123507, Train_MMSE: 0.012261, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:11:21] Epoch 198/240, Loss: 14.317173, Train_MMSE: 0.012255, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:12:24] Epoch 199/240, Loss: 14.157865, Train_MMSE: 0.012268, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:13:27] Epoch 200/240, Loss: 14.128165, Train_MMSE: 0.012264, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:14:29] Epoch 201/240, Loss: 14.117933, Train_MMSE: 0.012268, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:15:31] Epoch 202/240, Loss: 14.131137, Train_MMSE: 0.012279, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:16:32] Epoch 203/240, Loss: 14.195832, Train_MMSE: 0.012264, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:17:33] Epoch 204/240, Loss: 14.194346, Train_MMSE: 0.01226, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:18:26] Epoch 205/240, Loss: 14.170874, Train_MMSE: 0.012264, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:19:06] Epoch 206/240, Loss: 14.213368, Train_MMSE: 0.01225, NMMSE: 0.012556, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:19:46] Epoch 207/240, Loss: 14.126101, Train_MMSE: 0.012268, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:20:26] Epoch 208/240, Loss: 14.344074, Train_MMSE: 0.012259, NMMSE: 0.012559, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:21:06] Epoch 209/240, Loss: 14.113407, Train_MMSE: 0.012254, NMMSE: 0.012565, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:21:46] Epoch 210/240, Loss: 14.246368, Train_MMSE: 0.012269, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:22:14] Epoch 211/240, Loss: 14.136190, Train_MMSE: 0.012258, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:22:35] Epoch 212/240, Loss: 14.321971, Train_MMSE: 0.012276, NMMSE: 0.01255, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:22:55] Epoch 213/240, Loss: 14.151914, Train_MMSE: 0.01227, NMMSE: 0.01255, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:23:15] Epoch 214/240, Loss: 14.098892, Train_MMSE: 0.012263, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:23:35] Epoch 215/240, Loss: 14.048358, Train_MMSE: 0.012271, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:23:55] Epoch 216/240, Loss: 14.131498, Train_MMSE: 0.012265, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:24:16] Epoch 217/240, Loss: 14.323985, Train_MMSE: 0.01226, NMMSE: 0.012557, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:24:36] Epoch 218/240, Loss: 14.185640, Train_MMSE: 0.012271, NMMSE: 0.012553, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:24:56] Epoch 219/240, Loss: 14.144979, Train_MMSE: 0.01227, NMMSE: 0.012551, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:25:16] Epoch 220/240, Loss: 14.482960, Train_MMSE: 0.012252, NMMSE: 0.012552, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:25:36] Epoch 221/240, Loss: 13.964317, Train_MMSE: 0.012261, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:25:56] Epoch 222/240, Loss: 14.155797, Train_MMSE: 0.012275, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:26:17] Epoch 223/240, Loss: 14.218314, Train_MMSE: 0.012265, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:26:37] Epoch 224/240, Loss: 14.210630, Train_MMSE: 0.012278, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:26:57] Epoch 225/240, Loss: 14.160597, Train_MMSE: 0.012251, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:27:17] Epoch 226/240, Loss: 14.143985, Train_MMSE: 0.012261, NMMSE: 0.012545, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:27:37] Epoch 227/240, Loss: 14.232267, Train_MMSE: 0.012268, NMMSE: 0.012549, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:27:57] Epoch 228/240, Loss: 14.123106, Train_MMSE: 0.012262, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:28:17] Epoch 229/240, Loss: 14.151117, Train_MMSE: 0.01226, NMMSE: 0.012556, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:28:38] Epoch 230/240, Loss: 14.101944, Train_MMSE: 0.012265, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:28:58] Epoch 231/240, Loss: 14.092470, Train_MMSE: 0.012259, NMMSE: 0.012544, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:29:19] Epoch 232/240, Loss: 14.302172, Train_MMSE: 0.01227, NMMSE: 0.012555, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:29:39] Epoch 233/240, Loss: 14.403148, Train_MMSE: 0.012275, NMMSE: 0.012546, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:29:59] Epoch 234/240, Loss: 14.063966, Train_MMSE: 0.012245, NMMSE: 0.012555, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:30:19] Epoch 235/240, Loss: 14.091423, Train_MMSE: 0.012261, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:30:40] Epoch 236/240, Loss: 14.064075, Train_MMSE: 0.012256, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:31:00] Epoch 237/240, Loss: 14.146222, Train_MMSE: 0.012276, NMMSE: 0.012554, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:31:20] Epoch 238/240, Loss: 14.198962, Train_MMSE: 0.012275, NMMSE: 0.01255, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:31:40] Epoch 239/240, Loss: 14.342402, Train_MMSE: 0.01226, NMMSE: 0.012548, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 20:32:00] Epoch 240/240, Loss: 14.095065, Train_MMSE: 0.012265, NMMSE: 0.012547, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-07
