Train.py PID: 21837

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.018118952760023826
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S12_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S12_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S12_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 200,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fcb542c0e30>
loss function:: SmoothL1Loss()
[2025-02-24 14:24:14] Epoch 1/200, Loss: 17.062599, Train_MMSE: 0.017465, NMMSE: 0.021184, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:24:39] Epoch 2/200, Loss: 17.019876, Train_MMSE: 0.017231, NMMSE: 0.020939, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:25:04] Epoch 3/200, Loss: 16.817446, Train_MMSE: 0.017023, NMMSE: 0.020603, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:25:29] Epoch 4/200, Loss: 16.704807, Train_MMSE: 0.016795, NMMSE: 0.02041, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:25:54] Epoch 5/200, Loss: 16.705303, Train_MMSE: 0.016641, NMMSE: 0.020168, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:26:19] Epoch 6/200, Loss: 16.659292, Train_MMSE: 0.016536, NMMSE: 0.0201, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:26:45] Epoch 7/200, Loss: 16.577658, Train_MMSE: 0.016446, NMMSE: 0.019982, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:27:10] Epoch 8/200, Loss: 16.515911, Train_MMSE: 0.016373, NMMSE: 0.019914, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:27:35] Epoch 9/200, Loss: 16.514994, Train_MMSE: 0.016325, NMMSE: 0.019804, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:28:00] Epoch 10/200, Loss: 16.505663, Train_MMSE: 0.016288, NMMSE: 0.019766, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:28:26] Epoch 11/200, Loss: 16.541277, Train_MMSE: 0.016251, NMMSE: 0.019742, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:28:51] Epoch 12/200, Loss: 16.418480, Train_MMSE: 0.016217, NMMSE: 0.019707, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:29:16] Epoch 13/200, Loss: 16.336983, Train_MMSE: 0.016193, NMMSE: 0.019751, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:29:41] Epoch 14/200, Loss: 16.372307, Train_MMSE: 0.016177, NMMSE: 0.019602, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:30:06] Epoch 15/200, Loss: 16.375116, Train_MMSE: 0.016163, NMMSE: 0.019605, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:30:32] Epoch 16/200, Loss: 16.480331, Train_MMSE: 0.016155, NMMSE: 0.019635, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:30:57] Epoch 17/200, Loss: 16.413919, Train_MMSE: 0.016142, NMMSE: 0.019725, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:31:22] Epoch 18/200, Loss: 16.492979, Train_MMSE: 0.016134, NMMSE: 0.019619, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:31:48] Epoch 19/200, Loss: 16.370722, Train_MMSE: 0.01613, NMMSE: 0.019629, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:32:13] Epoch 20/200, Loss: 16.456079, Train_MMSE: 0.016124, NMMSE: 0.019652, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:32:38] Epoch 21/200, Loss: 16.248522, Train_MMSE: 0.016122, NMMSE: 0.019604, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:33:03] Epoch 22/200, Loss: 16.364784, Train_MMSE: 0.016119, NMMSE: 0.019573, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:33:29] Epoch 23/200, Loss: 16.378643, Train_MMSE: 0.016114, NMMSE: 0.019532, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:33:54] Epoch 24/200, Loss: 16.318935, Train_MMSE: 0.016108, NMMSE: 0.019496, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:34:19] Epoch 25/200, Loss: 16.360147, Train_MMSE: 0.016105, NMMSE: 0.019555, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:34:45] Epoch 26/200, Loss: 16.396669, Train_MMSE: 0.016104, NMMSE: 0.019554, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:35:10] Epoch 27/200, Loss: 16.501225, Train_MMSE: 0.016102, NMMSE: 0.019561, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:35:35] Epoch 28/200, Loss: 16.524462, Train_MMSE: 0.016102, NMMSE: 0.019616, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:36:03] Epoch 29/200, Loss: 16.426378, Train_MMSE: 0.016099, NMMSE: 0.01955, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:36:34] Epoch 30/200, Loss: 16.373505, Train_MMSE: 0.016094, NMMSE: 0.019558, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:37:07] Epoch 31/200, Loss: 16.307198, Train_MMSE: 0.016093, NMMSE: 0.019548, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:37:49] Epoch 32/200, Loss: 16.474222, Train_MMSE: 0.016093, NMMSE: 0.0195, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:38:19] Epoch 33/200, Loss: 16.403095, Train_MMSE: 0.01609, NMMSE: 0.019528, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:38:49] Epoch 34/200, Loss: 16.409161, Train_MMSE: 0.016091, NMMSE: 0.019562, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:39:27] Epoch 35/200, Loss: 16.360382, Train_MMSE: 0.016088, NMMSE: 0.01951, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:40:14] Epoch 36/200, Loss: 16.388647, Train_MMSE: 0.016088, NMMSE: 0.019524, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:41:05] Epoch 37/200, Loss: 16.436386, Train_MMSE: 0.016084, NMMSE: 0.019558, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:42:05] Epoch 38/200, Loss: 16.410379, Train_MMSE: 0.016084, NMMSE: 0.01956, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:43:06] Epoch 39/200, Loss: 16.372032, Train_MMSE: 0.016081, NMMSE: 0.019478, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:44:16] Epoch 40/200, Loss: 16.434496, Train_MMSE: 0.016083, NMMSE: 0.019566, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:45:32] Epoch 41/200, Loss: 16.514511, Train_MMSE: 0.01608, NMMSE: 0.019543, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:46:51] Epoch 42/200, Loss: 16.408674, Train_MMSE: 0.016079, NMMSE: 0.019582, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:48:11] Epoch 43/200, Loss: 16.420803, Train_MMSE: 0.016078, NMMSE: 0.019502, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:49:32] Epoch 44/200, Loss: 16.419338, Train_MMSE: 0.016078, NMMSE: 0.01953, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:51:00] Epoch 45/200, Loss: 16.447474, Train_MMSE: 0.016077, NMMSE: 0.019504, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:52:27] Epoch 46/200, Loss: 16.400852, Train_MMSE: 0.016072, NMMSE: 0.019464, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:53:53] Epoch 47/200, Loss: 16.359615, Train_MMSE: 0.016077, NMMSE: 0.01959, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:55:19] Epoch 48/200, Loss: 16.362486, Train_MMSE: 0.016075, NMMSE: 0.019534, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:56:43] Epoch 49/200, Loss: 16.438972, Train_MMSE: 0.016072, NMMSE: 0.019545, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:58:10] Epoch 50/200, Loss: 16.366522, Train_MMSE: 0.016072, NMMSE: 0.019506, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 14:59:36] Epoch 51/200, Loss: 16.428307, Train_MMSE: 0.016072, NMMSE: 0.01955, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:01:02] Epoch 52/200, Loss: 16.419558, Train_MMSE: 0.01607, NMMSE: 0.019519, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:02:28] Epoch 53/200, Loss: 16.432898, Train_MMSE: 0.016071, NMMSE: 0.019476, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:03:53] Epoch 54/200, Loss: 16.382217, Train_MMSE: 0.016068, NMMSE: 0.019509, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:05:18] Epoch 55/200, Loss: 16.345766, Train_MMSE: 0.016069, NMMSE: 0.01953, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:06:43] Epoch 56/200, Loss: 16.445524, Train_MMSE: 0.016068, NMMSE: 0.019542, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:08:08] Epoch 57/200, Loss: 16.431244, Train_MMSE: 0.01607, NMMSE: 0.019525, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:09:32] Epoch 58/200, Loss: 16.460079, Train_MMSE: 0.016066, NMMSE: 0.019581, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:10:58] Epoch 59/200, Loss: 16.460178, Train_MMSE: 0.016065, NMMSE: 0.019464, LS_NMSE: 0.021982, Lr: 0.001
[2025-02-24 15:12:23] Epoch 60/200, Loss: 16.467213, Train_MMSE: 0.016066, NMMSE: 0.019481, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:13:49] Epoch 61/200, Loss: 16.290684, Train_MMSE: 0.015948, NMMSE: 0.019279, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:15:15] Epoch 62/200, Loss: 16.342115, Train_MMSE: 0.015928, NMMSE: 0.019283, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:16:41] Epoch 63/200, Loss: 16.373428, Train_MMSE: 0.015922, NMMSE: 0.019275, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:18:06] Epoch 64/200, Loss: 16.322433, Train_MMSE: 0.015918, NMMSE: 0.019277, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:19:32] Epoch 65/200, Loss: 16.281837, Train_MMSE: 0.015915, NMMSE: 0.019278, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:20:58] Epoch 66/200, Loss: 16.289515, Train_MMSE: 0.015913, NMMSE: 0.019286, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:22:23] Epoch 67/200, Loss: 16.358755, Train_MMSE: 0.01591, NMMSE: 0.019278, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:23:48] Epoch 68/200, Loss: 16.291950, Train_MMSE: 0.015909, NMMSE: 0.019291, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:25:14] Epoch 69/200, Loss: 16.316965, Train_MMSE: 0.015907, NMMSE: 0.01928, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:26:39] Epoch 70/200, Loss: 16.290262, Train_MMSE: 0.015906, NMMSE: 0.019292, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:28:06] Epoch 71/200, Loss: 16.186859, Train_MMSE: 0.015903, NMMSE: 0.019298, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:29:30] Epoch 72/200, Loss: 16.292707, Train_MMSE: 0.015902, NMMSE: 0.019284, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:30:56] Epoch 73/200, Loss: 16.333384, Train_MMSE: 0.0159, NMMSE: 0.019289, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:32:23] Epoch 74/200, Loss: 16.220058, Train_MMSE: 0.015899, NMMSE: 0.019292, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:33:48] Epoch 75/200, Loss: 16.315718, Train_MMSE: 0.015899, NMMSE: 0.019292, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:35:14] Epoch 76/200, Loss: 16.242022, Train_MMSE: 0.015897, NMMSE: 0.019294, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:36:41] Epoch 77/200, Loss: 16.374580, Train_MMSE: 0.015896, NMMSE: 0.019311, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:38:06] Epoch 78/200, Loss: 16.381548, Train_MMSE: 0.015895, NMMSE: 0.019292, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:39:32] Epoch 79/200, Loss: 16.309221, Train_MMSE: 0.015895, NMMSE: 0.019307, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:40:57] Epoch 80/200, Loss: 16.301464, Train_MMSE: 0.015894, NMMSE: 0.0193, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:42:23] Epoch 81/200, Loss: 16.355038, Train_MMSE: 0.015892, NMMSE: 0.019301, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:43:51] Epoch 82/200, Loss: 16.304146, Train_MMSE: 0.015892, NMMSE: 0.019302, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:45:19] Epoch 83/200, Loss: 16.325802, Train_MMSE: 0.015891, NMMSE: 0.01931, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:46:48] Epoch 84/200, Loss: 16.160446, Train_MMSE: 0.015892, NMMSE: 0.019312, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:49:12] Epoch 85/200, Loss: 16.288866, Train_MMSE: 0.015889, NMMSE: 0.019316, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:52:01] Epoch 86/200, Loss: 16.211779, Train_MMSE: 0.015888, NMMSE: 0.019304, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:52:59] Epoch 87/200, Loss: 16.312584, Train_MMSE: 0.015887, NMMSE: 0.019338, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:53:45] Epoch 88/200, Loss: 16.215904, Train_MMSE: 0.015888, NMMSE: 0.019313, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:54:24] Epoch 89/200, Loss: 16.258854, Train_MMSE: 0.015887, NMMSE: 0.019304, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:55:06] Epoch 90/200, Loss: 16.302179, Train_MMSE: 0.015886, NMMSE: 0.019314, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:56:16] Epoch 91/200, Loss: 16.271980, Train_MMSE: 0.015886, NMMSE: 0.019313, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:57:52] Epoch 92/200, Loss: 16.243479, Train_MMSE: 0.015884, NMMSE: 0.019308, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 15:59:27] Epoch 93/200, Loss: 16.331758, Train_MMSE: 0.015885, NMMSE: 0.019313, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:01:04] Epoch 94/200, Loss: 16.257002, Train_MMSE: 0.015884, NMMSE: 0.019315, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:02:41] Epoch 95/200, Loss: 16.284851, Train_MMSE: 0.015884, NMMSE: 0.019321, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:04:14] Epoch 96/200, Loss: 16.191452, Train_MMSE: 0.015882, NMMSE: 0.019328, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:05:47] Epoch 97/200, Loss: 16.271189, Train_MMSE: 0.015883, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:07:21] Epoch 98/200, Loss: 16.271027, Train_MMSE: 0.015881, NMMSE: 0.01935, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:08:55] Epoch 99/200, Loss: 16.270733, Train_MMSE: 0.015882, NMMSE: 0.019319, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:10:28] Epoch 100/200, Loss: 16.376293, Train_MMSE: 0.015883, NMMSE: 0.019343, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:12:04] Epoch 101/200, Loss: 16.212305, Train_MMSE: 0.015881, NMMSE: 0.019326, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:13:40] Epoch 102/200, Loss: 16.337729, Train_MMSE: 0.01588, NMMSE: 0.01932, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:15:14] Epoch 103/200, Loss: 16.281864, Train_MMSE: 0.015879, NMMSE: 0.019324, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:16:49] Epoch 104/200, Loss: 16.291893, Train_MMSE: 0.01588, NMMSE: 0.019324, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:18:24] Epoch 105/200, Loss: 16.248804, Train_MMSE: 0.015879, NMMSE: 0.019341, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:19:59] Epoch 106/200, Loss: 16.265934, Train_MMSE: 0.015878, NMMSE: 0.019335, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:21:38] Epoch 107/200, Loss: 16.282482, Train_MMSE: 0.015878, NMMSE: 0.019323, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:23:13] Epoch 108/200, Loss: 16.325645, Train_MMSE: 0.015876, NMMSE: 0.019333, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:24:49] Epoch 109/200, Loss: 16.172352, Train_MMSE: 0.015877, NMMSE: 0.01934, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:26:25] Epoch 110/200, Loss: 16.261784, Train_MMSE: 0.015877, NMMSE: 0.019354, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:28:02] Epoch 111/200, Loss: 16.328972, Train_MMSE: 0.015878, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:29:38] Epoch 112/200, Loss: 16.291636, Train_MMSE: 0.015877, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:31:14] Epoch 113/200, Loss: 16.246889, Train_MMSE: 0.015875, NMMSE: 0.019347, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:32:49] Epoch 114/200, Loss: 16.309443, Train_MMSE: 0.015876, NMMSE: 0.01934, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:34:26] Epoch 115/200, Loss: 16.336222, Train_MMSE: 0.015876, NMMSE: 0.01934, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:36:02] Epoch 116/200, Loss: 16.206318, Train_MMSE: 0.015875, NMMSE: 0.019336, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:37:37] Epoch 117/200, Loss: 16.321980, Train_MMSE: 0.015874, NMMSE: 0.019342, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:39:12] Epoch 118/200, Loss: 16.335028, Train_MMSE: 0.015875, NMMSE: 0.019343, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:40:48] Epoch 119/200, Loss: 16.249063, Train_MMSE: 0.015873, NMMSE: 0.019346, LS_NMSE: 0.021982, Lr: 0.0001
[2025-02-24 16:42:24] Epoch 120/200, Loss: 16.231375, Train_MMSE: 0.015875, NMMSE: 0.019352, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:43:59] Epoch 121/200, Loss: 16.303177, Train_MMSE: 0.015842, NMMSE: 0.019308, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:45:35] Epoch 122/200, Loss: 16.344797, Train_MMSE: 0.015837, NMMSE: 0.019311, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:47:10] Epoch 123/200, Loss: 16.258209, Train_MMSE: 0.015836, NMMSE: 0.019315, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:48:47] Epoch 124/200, Loss: 16.173243, Train_MMSE: 0.015836, NMMSE: 0.019313, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:50:22] Epoch 125/200, Loss: 16.259636, Train_MMSE: 0.015835, NMMSE: 0.019313, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:51:58] Epoch 126/200, Loss: 16.228823, Train_MMSE: 0.015835, NMMSE: 0.019316, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:53:34] Epoch 127/200, Loss: 16.311239, Train_MMSE: 0.015835, NMMSE: 0.019316, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:55:09] Epoch 128/200, Loss: 16.256205, Train_MMSE: 0.015834, NMMSE: 0.019316, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:56:44] Epoch 129/200, Loss: 16.189753, Train_MMSE: 0.015835, NMMSE: 0.019317, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:58:20] Epoch 130/200, Loss: 16.269495, Train_MMSE: 0.015834, NMMSE: 0.019316, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 16:59:56] Epoch 131/200, Loss: 16.197187, Train_MMSE: 0.015834, NMMSE: 0.019319, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:01:31] Epoch 132/200, Loss: 16.119011, Train_MMSE: 0.015834, NMMSE: 0.019318, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:03:06] Epoch 133/200, Loss: 16.213566, Train_MMSE: 0.015834, NMMSE: 0.019319, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:04:42] Epoch 134/200, Loss: 16.209589, Train_MMSE: 0.015833, NMMSE: 0.01932, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:06:18] Epoch 135/200, Loss: 16.258907, Train_MMSE: 0.015833, NMMSE: 0.01932, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:07:53] Epoch 136/200, Loss: 16.193607, Train_MMSE: 0.015833, NMMSE: 0.019321, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:09:29] Epoch 137/200, Loss: 16.214619, Train_MMSE: 0.015833, NMMSE: 0.019321, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:11:05] Epoch 138/200, Loss: 16.136360, Train_MMSE: 0.015834, NMMSE: 0.019321, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:12:41] Epoch 139/200, Loss: 16.192823, Train_MMSE: 0.015834, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:14:17] Epoch 140/200, Loss: 16.275152, Train_MMSE: 0.015834, NMMSE: 0.019321, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:15:53] Epoch 141/200, Loss: 16.209215, Train_MMSE: 0.015833, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:17:28] Epoch 142/200, Loss: 16.254114, Train_MMSE: 0.015833, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:19:04] Epoch 143/200, Loss: 16.279085, Train_MMSE: 0.015833, NMMSE: 0.019323, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:20:39] Epoch 144/200, Loss: 16.243876, Train_MMSE: 0.015832, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:22:15] Epoch 145/200, Loss: 16.210978, Train_MMSE: 0.015831, NMMSE: 0.019323, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:23:51] Epoch 146/200, Loss: 16.217789, Train_MMSE: 0.015833, NMMSE: 0.019322, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:25:28] Epoch 147/200, Loss: 16.252155, Train_MMSE: 0.015833, NMMSE: 0.019323, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:27:08] Epoch 148/200, Loss: 16.280245, Train_MMSE: 0.015833, NMMSE: 0.019323, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:28:45] Epoch 149/200, Loss: 16.154888, Train_MMSE: 0.015831, NMMSE: 0.019325, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:30:22] Epoch 150/200, Loss: 16.200367, Train_MMSE: 0.015832, NMMSE: 0.019326, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:31:57] Epoch 151/200, Loss: 16.277100, Train_MMSE: 0.015832, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:33:34] Epoch 152/200, Loss: 16.400839, Train_MMSE: 0.015831, NMMSE: 0.019325, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:35:11] Epoch 153/200, Loss: 16.283743, Train_MMSE: 0.015831, NMMSE: 0.019325, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:36:48] Epoch 154/200, Loss: 16.258780, Train_MMSE: 0.015832, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:38:26] Epoch 155/200, Loss: 16.296402, Train_MMSE: 0.015832, NMMSE: 0.019324, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:40:02] Epoch 156/200, Loss: 16.283522, Train_MMSE: 0.015831, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:41:38] Epoch 157/200, Loss: 16.275312, Train_MMSE: 0.015831, NMMSE: 0.019326, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:43:17] Epoch 158/200, Loss: 16.218018, Train_MMSE: 0.01583, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:44:53] Epoch 159/200, Loss: 16.196802, Train_MMSE: 0.015832, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:46:30] Epoch 160/200, Loss: 16.266510, Train_MMSE: 0.015832, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:48:07] Epoch 161/200, Loss: 16.206289, Train_MMSE: 0.015831, NMMSE: 0.019329, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:49:43] Epoch 162/200, Loss: 16.184792, Train_MMSE: 0.015833, NMMSE: 0.019327, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:51:20] Epoch 163/200, Loss: 16.232431, Train_MMSE: 0.015831, NMMSE: 0.019328, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:52:57] Epoch 164/200, Loss: 16.137705, Train_MMSE: 0.015831, NMMSE: 0.019328, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:54:34] Epoch 165/200, Loss: 16.223509, Train_MMSE: 0.01583, NMMSE: 0.019329, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:56:11] Epoch 166/200, Loss: 16.167645, Train_MMSE: 0.01583, NMMSE: 0.019328, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:57:48] Epoch 167/200, Loss: 16.040396, Train_MMSE: 0.015831, NMMSE: 0.019328, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 17:59:27] Epoch 168/200, Loss: 16.216509, Train_MMSE: 0.015831, NMMSE: 0.019329, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:01:04] Epoch 169/200, Loss: 16.210487, Train_MMSE: 0.01583, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:02:41] Epoch 170/200, Loss: 16.292269, Train_MMSE: 0.01583, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:04:18] Epoch 171/200, Loss: 16.215204, Train_MMSE: 0.015831, NMMSE: 0.019329, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:05:54] Epoch 172/200, Loss: 16.196630, Train_MMSE: 0.015831, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:07:31] Epoch 173/200, Loss: 16.162903, Train_MMSE: 0.015829, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:09:08] Epoch 174/200, Loss: 16.229822, Train_MMSE: 0.01583, NMMSE: 0.019332, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:10:45] Epoch 175/200, Loss: 16.205513, Train_MMSE: 0.015831, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:12:22] Epoch 176/200, Loss: 16.297373, Train_MMSE: 0.01583, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:13:58] Epoch 177/200, Loss: 16.341133, Train_MMSE: 0.01583, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:15:34] Epoch 178/200, Loss: 16.237240, Train_MMSE: 0.015829, NMMSE: 0.019333, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:17:12] Epoch 179/200, Loss: 16.235992, Train_MMSE: 0.01583, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1e-05
[2025-02-24 18:18:48] Epoch 180/200, Loss: 16.154867, Train_MMSE: 0.01583, NMMSE: 0.019332, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:20:25] Epoch 181/200, Loss: 16.109720, Train_MMSE: 0.015825, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:22:02] Epoch 182/200, Loss: 16.191824, Train_MMSE: 0.015824, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:23:39] Epoch 183/200, Loss: 16.222818, Train_MMSE: 0.015824, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:25:16] Epoch 184/200, Loss: 16.233194, Train_MMSE: 0.015826, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:26:53] Epoch 185/200, Loss: 16.240883, Train_MMSE: 0.015825, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:28:30] Epoch 186/200, Loss: 16.153961, Train_MMSE: 0.015826, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:30:07] Epoch 187/200, Loss: 16.319815, Train_MMSE: 0.015825, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:31:45] Epoch 188/200, Loss: 16.229359, Train_MMSE: 0.015823, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:33:21] Epoch 189/200, Loss: 16.182665, Train_MMSE: 0.015824, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:34:57] Epoch 190/200, Loss: 16.261789, Train_MMSE: 0.015823, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:36:34] Epoch 191/200, Loss: 16.070286, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:38:11] Epoch 192/200, Loss: 16.162533, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:39:47] Epoch 193/200, Loss: 16.198462, Train_MMSE: 0.015825, NMMSE: 0.01933, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:41:23] Epoch 194/200, Loss: 16.144121, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:43:00] Epoch 195/200, Loss: 16.200907, Train_MMSE: 0.015824, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:44:38] Epoch 196/200, Loss: 16.262217, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:46:14] Epoch 197/200, Loss: 16.268715, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:47:51] Epoch 198/200, Loss: 16.226105, Train_MMSE: 0.015824, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:49:28] Epoch 199/200, Loss: 16.202990, Train_MMSE: 0.015824, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
[2025-02-24 18:51:05] Epoch 200/200, Loss: 16.290405, Train_MMSE: 0.015825, NMMSE: 0.019331, LS_NMSE: 0.021982, Lr: 1.0000000000000002e-06
