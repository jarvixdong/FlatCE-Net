Train.py PID: 15261

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.01241999814868016
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S13_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S13_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/CDRN_B3D18C64_test_Dataset_dB-15_N36_K4_L4_S13_Setup20_Reliz1000.log',
 'model': {'name': 'DnCNN_MultiBlock_ds',
           'params': {'block': 3,
                      'depth': 18,
                      'filters': 64,
                      'image_channels': 2,
                      'use_bnorm': True}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (46): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (47): ReLU(inplace=True)
      (48): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (49): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (50): ReLU(inplace=True)
      (51): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 6.80 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fd8ac5f3f50>
loss function:: SmoothL1Loss()
[2025-02-24 15:05:10] Epoch 1/240, Loss: 14.827589, Train_MMSE: 0.013585, NMMSE: 0.013965, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:07:43] Epoch 2/240, Loss: 14.903994, Train_MMSE: 0.013465, NMMSE: 0.01387, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:10:22] Epoch 3/240, Loss: 14.772461, Train_MMSE: 0.013366, NMMSE: 0.01377, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:12:57] Epoch 4/240, Loss: 14.752477, Train_MMSE: 0.013265, NMMSE: 0.013636, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:15:36] Epoch 5/240, Loss: 14.702632, Train_MMSE: 0.013174, NMMSE: 0.013599, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:18:18] Epoch 6/240, Loss: 14.567801, Train_MMSE: 0.01309, NMMSE: 0.013521, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:20:54] Epoch 7/240, Loss: 14.652237, Train_MMSE: 0.013006, NMMSE: 0.013479, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:23:31] Epoch 8/240, Loss: 14.538640, Train_MMSE: 0.012936, NMMSE: 0.013438, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:26:09] Epoch 9/240, Loss: 14.507058, Train_MMSE: 0.012899, NMMSE: 0.013328, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:28:48] Epoch 10/240, Loss: 14.502263, Train_MMSE: 0.012874, NMMSE: 0.013392, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:31:26] Epoch 11/240, Loss: 14.543247, Train_MMSE: 0.012854, NMMSE: 0.013311, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:34:05] Epoch 12/240, Loss: 14.599836, Train_MMSE: 0.012836, NMMSE: 0.013274, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:36:43] Epoch 13/240, Loss: 14.520238, Train_MMSE: 0.012823, NMMSE: 0.013291, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:39:21] Epoch 14/240, Loss: 14.488881, Train_MMSE: 0.012814, NMMSE: 0.013263, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:41:59] Epoch 15/240, Loss: 14.391544, Train_MMSE: 0.012804, NMMSE: 0.013251, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:44:37] Epoch 16/240, Loss: 14.525031, Train_MMSE: 0.012798, NMMSE: 0.013256, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:47:20] Epoch 17/240, Loss: 14.520259, Train_MMSE: 0.01279, NMMSE: 0.013255, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:49:58] Epoch 18/240, Loss: 14.453435, Train_MMSE: 0.012784, NMMSE: 0.013262, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:52:34] Epoch 19/240, Loss: 14.463394, Train_MMSE: 0.012767, NMMSE: 0.013191, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:55:12] Epoch 20/240, Loss: 14.407213, Train_MMSE: 0.012752, NMMSE: 0.013212, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 15:57:49] Epoch 21/240, Loss: 14.503481, Train_MMSE: 0.012744, NMMSE: 0.013189, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:00:27] Epoch 22/240, Loss: 14.447437, Train_MMSE: 0.012738, NMMSE: 0.013191, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:03:05] Epoch 23/240, Loss: 14.475695, Train_MMSE: 0.012731, NMMSE: 0.013182, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:05:42] Epoch 24/240, Loss: 14.418978, Train_MMSE: 0.012726, NMMSE: 0.013198, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:08:21] Epoch 25/240, Loss: 14.510646, Train_MMSE: 0.012725, NMMSE: 0.013189, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:10:59] Epoch 26/240, Loss: 14.408837, Train_MMSE: 0.012718, NMMSE: 0.01321, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:13:36] Epoch 27/240, Loss: 14.375273, Train_MMSE: 0.012716, NMMSE: 0.013166, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:16:13] Epoch 28/240, Loss: 14.512266, Train_MMSE: 0.012707, NMMSE: 0.013172, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:18:51] Epoch 29/240, Loss: 14.437468, Train_MMSE: 0.012702, NMMSE: 0.013165, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:21:29] Epoch 30/240, Loss: 14.441689, Train_MMSE: 0.012699, NMMSE: 0.013159, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:24:09] Epoch 31/240, Loss: 14.453997, Train_MMSE: 0.012696, NMMSE: 0.013135, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:26:49] Epoch 32/240, Loss: 14.393040, Train_MMSE: 0.012693, NMMSE: 0.013172, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:29:25] Epoch 33/240, Loss: 14.396820, Train_MMSE: 0.012692, NMMSE: 0.013141, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:32:04] Epoch 34/240, Loss: 14.442426, Train_MMSE: 0.01269, NMMSE: 0.013171, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:34:43] Epoch 35/240, Loss: 14.422758, Train_MMSE: 0.012691, NMMSE: 0.013152, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:37:20] Epoch 36/240, Loss: 14.412282, Train_MMSE: 0.012688, NMMSE: 0.013155, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:39:58] Epoch 37/240, Loss: 14.399770, Train_MMSE: 0.012688, NMMSE: 0.013119, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:42:41] Epoch 38/240, Loss: 14.392987, Train_MMSE: 0.012684, NMMSE: 0.013125, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:45:20] Epoch 39/240, Loss: 14.506948, Train_MMSE: 0.012684, NMMSE: 0.013151, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:47:57] Epoch 40/240, Loss: 14.350263, Train_MMSE: 0.012684, NMMSE: 0.013146, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:50:36] Epoch 41/240, Loss: 14.417543, Train_MMSE: 0.012683, NMMSE: 0.013197, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:53:16] Epoch 42/240, Loss: 14.438923, Train_MMSE: 0.01268, NMMSE: 0.013136, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:55:54] Epoch 43/240, Loss: 14.473253, Train_MMSE: 0.012681, NMMSE: 0.013125, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 16:58:34] Epoch 44/240, Loss: 14.360953, Train_MMSE: 0.01268, NMMSE: 0.013116, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:01:14] Epoch 45/240, Loss: 14.371672, Train_MMSE: 0.012679, NMMSE: 0.013118, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:03:55] Epoch 46/240, Loss: 14.372179, Train_MMSE: 0.01268, NMMSE: 0.013156, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:06:34] Epoch 47/240, Loss: 14.345442, Train_MMSE: 0.012677, NMMSE: 0.013152, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:09:10] Epoch 48/240, Loss: 14.515584, Train_MMSE: 0.012677, NMMSE: 0.013134, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:11:48] Epoch 49/240, Loss: 14.373854, Train_MMSE: 0.012676, NMMSE: 0.013115, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:14:27] Epoch 50/240, Loss: 14.382989, Train_MMSE: 0.012673, NMMSE: 0.013134, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:17:04] Epoch 51/240, Loss: 14.407102, Train_MMSE: 0.012674, NMMSE: 0.013139, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:19:43] Epoch 52/240, Loss: 14.438738, Train_MMSE: 0.012673, NMMSE: 0.013134, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:22:20] Epoch 53/240, Loss: 14.377698, Train_MMSE: 0.012673, NMMSE: 0.013131, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:24:58] Epoch 54/240, Loss: 14.327177, Train_MMSE: 0.012672, NMMSE: 0.013158, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:27:34] Epoch 55/240, Loss: 14.438784, Train_MMSE: 0.012672, NMMSE: 0.013112, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:30:20] Epoch 56/240, Loss: 14.467311, Train_MMSE: 0.012672, NMMSE: 0.013124, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:32:58] Epoch 57/240, Loss: 14.372530, Train_MMSE: 0.012671, NMMSE: 0.013141, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:35:38] Epoch 58/240, Loss: 14.383906, Train_MMSE: 0.01267, NMMSE: 0.013119, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:38:18] Epoch 59/240, Loss: 14.385319, Train_MMSE: 0.012667, NMMSE: 0.013139, LS_NMSE: 0.014302, Lr: 0.001
[2025-02-24 17:40:56] Epoch 60/240, Loss: 14.360476, Train_MMSE: 0.012668, NMMSE: 0.013101, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:44:09] Epoch 61/240, Loss: 14.328153, Train_MMSE: 0.012581, NMMSE: 0.012992, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:47:41] Epoch 62/240, Loss: 14.354152, Train_MMSE: 0.012563, NMMSE: 0.012991, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:50:51] Epoch 63/240, Loss: 14.367771, Train_MMSE: 0.012558, NMMSE: 0.012984, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:53:51] Epoch 64/240, Loss: 14.305379, Train_MMSE: 0.012554, NMMSE: 0.012986, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:56:51] Epoch 65/240, Loss: 14.375892, Train_MMSE: 0.012551, NMMSE: 0.01299, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 17:59:50] Epoch 66/240, Loss: 14.364866, Train_MMSE: 0.012549, NMMSE: 0.012991, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:02:36] Epoch 67/240, Loss: 14.316833, Train_MMSE: 0.012547, NMMSE: 0.012987, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:05:28] Epoch 68/240, Loss: 14.327839, Train_MMSE: 0.012544, NMMSE: 0.012988, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:08:17] Epoch 69/240, Loss: 14.280177, Train_MMSE: 0.012543, NMMSE: 0.012989, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:11:08] Epoch 70/240, Loss: 14.376051, Train_MMSE: 0.012542, NMMSE: 0.012991, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:14:01] Epoch 71/240, Loss: 14.320316, Train_MMSE: 0.012541, NMMSE: 0.012991, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:16:53] Epoch 72/240, Loss: 14.307004, Train_MMSE: 0.012541, NMMSE: 0.012997, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:19:36] Epoch 73/240, Loss: 14.247796, Train_MMSE: 0.012539, NMMSE: 0.012995, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:22:21] Epoch 74/240, Loss: 14.290936, Train_MMSE: 0.012538, NMMSE: 0.012992, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:25:08] Epoch 75/240, Loss: 14.274457, Train_MMSE: 0.012537, NMMSE: 0.012996, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:27:56] Epoch 76/240, Loss: 14.356391, Train_MMSE: 0.012536, NMMSE: 0.012996, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:30:25] Epoch 77/240, Loss: 14.288220, Train_MMSE: 0.012536, NMMSE: 0.012991, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:32:47] Epoch 78/240, Loss: 14.370619, Train_MMSE: 0.012535, NMMSE: 0.012992, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:35:16] Epoch 79/240, Loss: 14.311964, Train_MMSE: 0.012534, NMMSE: 0.012994, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:37:38] Epoch 80/240, Loss: 14.315037, Train_MMSE: 0.012533, NMMSE: 0.012998, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:40:01] Epoch 81/240, Loss: 14.409263, Train_MMSE: 0.012532, NMMSE: 0.012999, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:42:24] Epoch 82/240, Loss: 14.354099, Train_MMSE: 0.012531, NMMSE: 0.012999, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:44:48] Epoch 83/240, Loss: 14.268620, Train_MMSE: 0.012531, NMMSE: 0.012998, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:47:11] Epoch 84/240, Loss: 14.299048, Train_MMSE: 0.012531, NMMSE: 0.012997, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:49:35] Epoch 85/240, Loss: 14.268201, Train_MMSE: 0.01253, NMMSE: 0.012999, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:51:57] Epoch 86/240, Loss: 14.373315, Train_MMSE: 0.012528, NMMSE: 0.013001, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:54:21] Epoch 87/240, Loss: 14.264306, Train_MMSE: 0.012528, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:56:43] Epoch 88/240, Loss: 14.346324, Train_MMSE: 0.012529, NMMSE: 0.013, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 18:59:05] Epoch 89/240, Loss: 14.299968, Train_MMSE: 0.012527, NMMSE: 0.012997, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:01:28] Epoch 90/240, Loss: 14.283155, Train_MMSE: 0.012527, NMMSE: 0.013001, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:03:49] Epoch 91/240, Loss: 14.261909, Train_MMSE: 0.012526, NMMSE: 0.013002, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:06:12] Epoch 92/240, Loss: 14.284101, Train_MMSE: 0.012526, NMMSE: 0.013002, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:08:32] Epoch 93/240, Loss: 14.335216, Train_MMSE: 0.012527, NMMSE: 0.013004, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:10:55] Epoch 94/240, Loss: 14.258372, Train_MMSE: 0.012526, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:13:18] Epoch 95/240, Loss: 14.324633, Train_MMSE: 0.012525, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:15:41] Epoch 96/240, Loss: 14.345737, Train_MMSE: 0.012525, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:18:05] Epoch 97/240, Loss: 14.271227, Train_MMSE: 0.012524, NMMSE: 0.013003, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:20:28] Epoch 98/240, Loss: 14.376645, Train_MMSE: 0.012524, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:22:51] Epoch 99/240, Loss: 14.340641, Train_MMSE: 0.012524, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:25:10] Epoch 100/240, Loss: 14.259759, Train_MMSE: 0.012522, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:27:32] Epoch 101/240, Loss: 14.319471, Train_MMSE: 0.012523, NMMSE: 0.013022, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:29:56] Epoch 102/240, Loss: 14.321873, Train_MMSE: 0.012522, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:32:22] Epoch 103/240, Loss: 14.303704, Train_MMSE: 0.012522, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:34:46] Epoch 104/240, Loss: 14.273904, Train_MMSE: 0.012522, NMMSE: 0.013014, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:37:09] Epoch 105/240, Loss: 14.306378, Train_MMSE: 0.012522, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:39:31] Epoch 106/240, Loss: 14.326625, Train_MMSE: 0.012521, NMMSE: 0.013015, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:41:53] Epoch 107/240, Loss: 14.353765, Train_MMSE: 0.01252, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:44:14] Epoch 108/240, Loss: 14.365524, Train_MMSE: 0.01252, NMMSE: 0.013015, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:46:37] Epoch 109/240, Loss: 14.227275, Train_MMSE: 0.01252, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:48:58] Epoch 110/240, Loss: 14.355547, Train_MMSE: 0.01252, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:51:22] Epoch 111/240, Loss: 14.356297, Train_MMSE: 0.01252, NMMSE: 0.013016, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:53:46] Epoch 112/240, Loss: 14.358773, Train_MMSE: 0.012519, NMMSE: 0.013017, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:56:10] Epoch 113/240, Loss: 14.243873, Train_MMSE: 0.012519, NMMSE: 0.013017, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 19:58:32] Epoch 114/240, Loss: 14.303257, Train_MMSE: 0.012519, NMMSE: 0.013015, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:00:54] Epoch 115/240, Loss: 14.328883, Train_MMSE: 0.012519, NMMSE: 0.013015, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:03:18] Epoch 116/240, Loss: 14.293252, Train_MMSE: 0.012519, NMMSE: 0.013014, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:05:42] Epoch 117/240, Loss: 14.268775, Train_MMSE: 0.012518, NMMSE: 0.013015, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:08:04] Epoch 118/240, Loss: 14.251726, Train_MMSE: 0.012518, NMMSE: 0.013014, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:10:26] Epoch 119/240, Loss: 14.363231, Train_MMSE: 0.012516, NMMSE: 0.013014, LS_NMSE: 0.014302, Lr: 0.0001
[2025-02-24 20:12:49] Epoch 120/240, Loss: 14.282033, Train_MMSE: 0.012517, NMMSE: 0.01302, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:15:14] Epoch 121/240, Loss: 14.266351, Train_MMSE: 0.012494, NMMSE: 0.013, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:17:36] Epoch 122/240, Loss: 14.280446, Train_MMSE: 0.01249, NMMSE: 0.013, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:20:00] Epoch 123/240, Loss: 14.271055, Train_MMSE: 0.012489, NMMSE: 0.013001, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:22:24] Epoch 124/240, Loss: 14.288798, Train_MMSE: 0.012488, NMMSE: 0.013001, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:24:47] Epoch 125/240, Loss: 14.282708, Train_MMSE: 0.012488, NMMSE: 0.013002, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:27:11] Epoch 126/240, Loss: 14.231116, Train_MMSE: 0.012488, NMMSE: 0.013002, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:29:34] Epoch 127/240, Loss: 14.294298, Train_MMSE: 0.012487, NMMSE: 0.013003, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:32:01] Epoch 128/240, Loss: 14.281243, Train_MMSE: 0.012487, NMMSE: 0.013003, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:34:23] Epoch 129/240, Loss: 14.223160, Train_MMSE: 0.012487, NMMSE: 0.013003, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:36:48] Epoch 130/240, Loss: 14.239268, Train_MMSE: 0.012486, NMMSE: 0.013004, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:39:11] Epoch 131/240, Loss: 14.322926, Train_MMSE: 0.012487, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:41:36] Epoch 132/240, Loss: 14.315083, Train_MMSE: 0.012486, NMMSE: 0.013004, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:43:58] Epoch 133/240, Loss: 14.244799, Train_MMSE: 0.012487, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:46:22] Epoch 134/240, Loss: 14.195361, Train_MMSE: 0.012486, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:48:48] Epoch 135/240, Loss: 14.314261, Train_MMSE: 0.012487, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:51:14] Epoch 136/240, Loss: 14.281981, Train_MMSE: 0.012487, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:53:36] Epoch 137/240, Loss: 14.300256, Train_MMSE: 0.012487, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:55:58] Epoch 138/240, Loss: 14.276971, Train_MMSE: 0.012486, NMMSE: 0.013005, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 20:58:22] Epoch 139/240, Loss: 14.207153, Train_MMSE: 0.012486, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:00:43] Epoch 140/240, Loss: 14.224267, Train_MMSE: 0.012485, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:03:07] Epoch 141/240, Loss: 14.290261, Train_MMSE: 0.012486, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:05:29] Epoch 142/240, Loss: 14.302017, Train_MMSE: 0.012485, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:07:41] Epoch 143/240, Loss: 14.291181, Train_MMSE: 0.012486, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:09:44] Epoch 144/240, Loss: 14.315564, Train_MMSE: 0.012485, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:11:49] Epoch 145/240, Loss: 14.303148, Train_MMSE: 0.012486, NMMSE: 0.013006, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:13:51] Epoch 146/240, Loss: 14.295362, Train_MMSE: 0.012486, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:15:50] Epoch 147/240, Loss: 14.235539, Train_MMSE: 0.012485, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:17:57] Epoch 148/240, Loss: 14.243110, Train_MMSE: 0.012485, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:19:58] Epoch 149/240, Loss: 14.286562, Train_MMSE: 0.012484, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:22:00] Epoch 150/240, Loss: 14.263133, Train_MMSE: 0.012485, NMMSE: 0.013007, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:23:52] Epoch 151/240, Loss: 14.230336, Train_MMSE: 0.012485, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:25:35] Epoch 152/240, Loss: 14.297575, Train_MMSE: 0.012485, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:27:19] Epoch 153/240, Loss: 14.342431, Train_MMSE: 0.012484, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:29:03] Epoch 154/240, Loss: 14.211625, Train_MMSE: 0.012485, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:30:52] Epoch 155/240, Loss: 14.285751, Train_MMSE: 0.012485, NMMSE: 0.013008, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:32:39] Epoch 156/240, Loss: 14.329936, Train_MMSE: 0.012483, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:34:24] Epoch 157/240, Loss: 14.355723, Train_MMSE: 0.012484, NMMSE: 0.013009, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:36:08] Epoch 158/240, Loss: 14.267615, Train_MMSE: 0.012485, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:37:53] Epoch 159/240, Loss: 14.260328, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:39:38] Epoch 160/240, Loss: 14.240106, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:41:21] Epoch 161/240, Loss: 14.255055, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:43:06] Epoch 162/240, Loss: 14.331220, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:44:49] Epoch 163/240, Loss: 14.264055, Train_MMSE: 0.012485, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:46:35] Epoch 164/240, Loss: 14.228657, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:48:20] Epoch 165/240, Loss: 14.246648, Train_MMSE: 0.012484, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:50:03] Epoch 166/240, Loss: 14.323661, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:51:49] Epoch 167/240, Loss: 14.244267, Train_MMSE: 0.012484, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:53:30] Epoch 168/240, Loss: 14.288757, Train_MMSE: 0.012483, NMMSE: 0.01301, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:55:11] Epoch 169/240, Loss: 14.253842, Train_MMSE: 0.012483, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:56:54] Epoch 170/240, Loss: 14.182441, Train_MMSE: 0.012483, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 21:58:39] Epoch 171/240, Loss: 14.343824, Train_MMSE: 0.012483, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:00:21] Epoch 172/240, Loss: 14.242013, Train_MMSE: 0.012483, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:02:08] Epoch 173/240, Loss: 14.236284, Train_MMSE: 0.012483, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:03:50] Epoch 174/240, Loss: 14.214576, Train_MMSE: 0.012483, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:05:33] Epoch 175/240, Loss: 14.246552, Train_MMSE: 0.012483, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:07:17] Epoch 176/240, Loss: 14.196389, Train_MMSE: 0.012483, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:09:02] Epoch 177/240, Loss: 14.298355, Train_MMSE: 0.012484, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:10:47] Epoch 178/240, Loss: 14.244267, Train_MMSE: 0.012484, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:12:36] Epoch 179/240, Loss: 14.254026, Train_MMSE: 0.012483, NMMSE: 0.013013, LS_NMSE: 0.014302, Lr: 1e-05
[2025-02-24 22:14:17] Epoch 180/240, Loss: 14.252370, Train_MMSE: 0.012483, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:15:59] Epoch 181/240, Loss: 14.250783, Train_MMSE: 0.01248, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:17:42] Epoch 182/240, Loss: 14.293435, Train_MMSE: 0.01248, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:19:24] Epoch 183/240, Loss: 14.290290, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:21:07] Epoch 184/240, Loss: 14.290358, Train_MMSE: 0.01248, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:22:52] Epoch 185/240, Loss: 14.181805, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:24:37] Epoch 186/240, Loss: 14.195706, Train_MMSE: 0.01248, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:26:20] Epoch 187/240, Loss: 14.291753, Train_MMSE: 0.01248, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:28:04] Epoch 188/240, Loss: 14.220273, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:29:46] Epoch 189/240, Loss: 14.302671, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:31:28] Epoch 190/240, Loss: 14.267205, Train_MMSE: 0.012478, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:33:08] Epoch 191/240, Loss: 14.260077, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:34:52] Epoch 192/240, Loss: 14.261185, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:36:35] Epoch 193/240, Loss: 14.231462, Train_MMSE: 0.012479, NMMSE: 0.013011, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:38:18] Epoch 194/240, Loss: 14.202149, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:40:05] Epoch 195/240, Loss: 14.214708, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:41:46] Epoch 196/240, Loss: 14.277557, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:43:28] Epoch 197/240, Loss: 14.277128, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:45:12] Epoch 198/240, Loss: 14.256677, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:46:52] Epoch 199/240, Loss: 14.360042, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:48:35] Epoch 200/240, Loss: 14.314549, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:50:17] Epoch 201/240, Loss: 14.265630, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:51:59] Epoch 202/240, Loss: 14.206865, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:53:42] Epoch 203/240, Loss: 14.224461, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:55:23] Epoch 204/240, Loss: 14.230213, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:56:49] Epoch 205/240, Loss: 14.301812, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:58:20] Epoch 206/240, Loss: 14.230893, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 22:59:53] Epoch 207/240, Loss: 14.351405, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:01:25] Epoch 208/240, Loss: 14.296407, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:02:56] Epoch 209/240, Loss: 14.219303, Train_MMSE: 0.012481, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:04:27] Epoch 210/240, Loss: 14.207863, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:06:01] Epoch 211/240, Loss: 14.224813, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:07:34] Epoch 212/240, Loss: 14.259166, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:09:04] Epoch 213/240, Loss: 14.208973, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:10:33] Epoch 214/240, Loss: 14.203214, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:11:45] Epoch 215/240, Loss: 14.243896, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:12:50] Epoch 216/240, Loss: 14.292036, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:13:56] Epoch 217/240, Loss: 14.282369, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:15:03] Epoch 218/240, Loss: 14.264494, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:16:08] Epoch 219/240, Loss: 14.277422, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:17:15] Epoch 220/240, Loss: 14.287639, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:18:22] Epoch 221/240, Loss: 14.301697, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:19:13] Epoch 222/240, Loss: 14.249867, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:19:56] Epoch 223/240, Loss: 14.285553, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:20:40] Epoch 224/240, Loss: 14.148655, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:21:23] Epoch 225/240, Loss: 14.297818, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:22:06] Epoch 226/240, Loss: 14.218669, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:22:49] Epoch 227/240, Loss: 14.360600, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:23:34] Epoch 228/240, Loss: 14.304045, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:24:19] Epoch 229/240, Loss: 14.249719, Train_MMSE: 0.012478, NMMSE: 0.013013, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:25:03] Epoch 230/240, Loss: 14.275153, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:25:47] Epoch 231/240, Loss: 14.339161, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:26:31] Epoch 232/240, Loss: 14.239430, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:27:16] Epoch 233/240, Loss: 14.237944, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:27:59] Epoch 234/240, Loss: 14.255507, Train_MMSE: 0.012479, NMMSE: 0.013013, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:28:44] Epoch 235/240, Loss: 14.329670, Train_MMSE: 0.012478, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:29:28] Epoch 236/240, Loss: 14.289808, Train_MMSE: 0.01248, NMMSE: 0.013013, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:30:13] Epoch 237/240, Loss: 14.288130, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:30:56] Epoch 238/240, Loss: 14.278420, Train_MMSE: 0.01248, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:31:39] Epoch 239/240, Loss: 14.173450, Train_MMSE: 0.012478, NMMSE: 0.013013, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-06
[2025-02-24 23:32:23] Epoch 240/240, Loss: 14.285509, Train_MMSE: 0.012479, NMMSE: 0.013012, LS_NMSE: 0.014302, Lr: 1.0000000000000002e-07
