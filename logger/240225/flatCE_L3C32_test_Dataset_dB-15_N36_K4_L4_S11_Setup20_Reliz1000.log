Train.py PID: 25803

H shape: (20000, 4, 36) (20000, 4, 36)
NMMSE of valid dataset:: 0.023510711085663393
num samples :: 200000
num valid: 20000
config_path: conf/config_multisetup.yml
{'config': 'conf/config_multisetup.yml',
 'dataloader': {'batch_size': 512, 'num_workers': 1, 'shuffle': True},
 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/train_Dataset_dB-15_N36_K4_L4_S11_Setup200_Reliz1000.mat',
             'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset13_N36K4M4/test_Dataset_dB-15_N36_K4_L4_S11_Setup20_Reliz1000.mat',
             'with_Vpinv': True},
 'log_path': 'logger/240225/flatCE_L3C32_test_Dataset_dB-15_N36_K4_L4_S11_Setup20_Reliz1000.log',
 'model': {'name': 'DiaUNet1D',
           'params': {'base_channels': 32,
                      'in_channels': 2,
                      'num_layers': 3,
                      'out_channels': 2}},
 'seed': 10,
 'trainer': {'epoch_num': 240,
             'loss': 'SmoothL1Loss',
             'lr_scheduler': {'name': 'StepLR',
                              'params': {'gamma': 0.1, 'step_size': 60}},
             'optimizer': {'name': 'Adam',
                           'params': {'lr': 0.001, 'weight_decay': 0.001}}}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(64, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 4, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(32, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 2.58 MB
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0.001
)
scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fb96a03a240>
loss function:: SmoothL1Loss()
[2025-02-24 14:40:07] Epoch 1/240, Loss: 31.626923, Train_MMSE: 0.640277, NMMSE: 0.080013, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:40:53] Epoch 2/240, Loss: 20.041428, Train_MMSE: 0.028112, NMMSE: 0.02617, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:41:51] Epoch 3/240, Loss: 19.999968, Train_MMSE: 0.024133, NMMSE: 0.025454, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:42:51] Epoch 4/240, Loss: 19.610115, Train_MMSE: 0.023739, NMMSE: 0.025268, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:43:57] Epoch 5/240, Loss: 19.668509, Train_MMSE: 0.02357, NMMSE: 0.025014, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:45:08] Epoch 6/240, Loss: 19.609921, Train_MMSE: 0.02344, NMMSE: 0.024952, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:46:26] Epoch 7/240, Loss: 19.692551, Train_MMSE: 0.023369, NMMSE: 0.024999, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:47:44] Epoch 8/240, Loss: 19.539364, Train_MMSE: 0.023314, NMMSE: 0.024965, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:49:05] Epoch 9/240, Loss: 19.629379, Train_MMSE: 0.023273, NMMSE: 0.024786, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:50:35] Epoch 10/240, Loss: 19.444000, Train_MMSE: 0.02326, NMMSE: 0.024741, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:52:04] Epoch 11/240, Loss: 19.616896, Train_MMSE: 0.02319, NMMSE: 0.0248, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:53:33] Epoch 12/240, Loss: 19.428032, Train_MMSE: 0.023176, NMMSE: 0.024671, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:55:01] Epoch 13/240, Loss: 19.498032, Train_MMSE: 0.023193, NMMSE: 0.024737, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:56:30] Epoch 14/240, Loss: 19.412746, Train_MMSE: 0.02313, NMMSE: 0.024627, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:57:59] Epoch 15/240, Loss: 19.586960, Train_MMSE: 0.023114, NMMSE: 0.024574, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 14:59:27] Epoch 16/240, Loss: 19.555281, Train_MMSE: 0.023098, NMMSE: 0.024654, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:00:55] Epoch 17/240, Loss: 19.389231, Train_MMSE: 0.023101, NMMSE: 0.024654, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:02:24] Epoch 18/240, Loss: 19.579582, Train_MMSE: 0.023083, NMMSE: 0.024705, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:03:52] Epoch 19/240, Loss: 19.419970, Train_MMSE: 0.023073, NMMSE: 0.024634, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:05:19] Epoch 20/240, Loss: 19.592302, Train_MMSE: 0.023057, NMMSE: 0.024582, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:06:46] Epoch 21/240, Loss: 19.415751, Train_MMSE: 0.023049, NMMSE: 0.024618, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:08:14] Epoch 22/240, Loss: 19.527525, Train_MMSE: 0.023053, NMMSE: 0.024795, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:09:42] Epoch 23/240, Loss: 19.384317, Train_MMSE: 0.023026, NMMSE: 0.024471, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:11:11] Epoch 24/240, Loss: 19.517399, Train_MMSE: 0.023029, NMMSE: 0.024533, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:12:39] Epoch 25/240, Loss: 19.430922, Train_MMSE: 0.023026, NMMSE: 0.024523, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:14:08] Epoch 26/240, Loss: 19.582756, Train_MMSE: 0.023027, NMMSE: 0.024508, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:15:36] Epoch 27/240, Loss: 19.425131, Train_MMSE: 0.023005, NMMSE: 0.02453, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:17:05] Epoch 28/240, Loss: 19.453823, Train_MMSE: 0.023, NMMSE: 0.024447, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:18:33] Epoch 29/240, Loss: 19.391844, Train_MMSE: 0.022983, NMMSE: 0.024464, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:20:02] Epoch 30/240, Loss: 19.449400, Train_MMSE: 0.022982, NMMSE: 0.024519, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:21:30] Epoch 31/240, Loss: 19.521370, Train_MMSE: 0.022995, NMMSE: 0.024466, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:22:58] Epoch 32/240, Loss: 19.488791, Train_MMSE: 0.02298, NMMSE: 0.024483, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:24:27] Epoch 33/240, Loss: 19.466381, Train_MMSE: 0.022952, NMMSE: 0.024506, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:25:55] Epoch 34/240, Loss: 19.274572, Train_MMSE: 0.02296, NMMSE: 0.024523, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:27:24] Epoch 35/240, Loss: 19.359581, Train_MMSE: 0.022946, NMMSE: 0.024413, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:28:52] Epoch 36/240, Loss: 19.349869, Train_MMSE: 0.022943, NMMSE: 0.024481, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:30:21] Epoch 37/240, Loss: 19.271862, Train_MMSE: 0.022947, NMMSE: 0.024443, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:31:49] Epoch 38/240, Loss: 19.608168, Train_MMSE: 0.022948, NMMSE: 0.024519, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:33:17] Epoch 39/240, Loss: 19.432556, Train_MMSE: 0.022937, NMMSE: 0.024508, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:34:46] Epoch 40/240, Loss: 19.332510, Train_MMSE: 0.022934, NMMSE: 0.024366, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:36:14] Epoch 41/240, Loss: 19.433517, Train_MMSE: 0.02293, NMMSE: 0.024577, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:37:42] Epoch 42/240, Loss: 19.493172, Train_MMSE: 0.02292, NMMSE: 0.024391, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:39:11] Epoch 43/240, Loss: 19.576086, Train_MMSE: 0.022929, NMMSE: 0.024699, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:40:40] Epoch 44/240, Loss: 19.526558, Train_MMSE: 0.022912, NMMSE: 0.024419, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:42:09] Epoch 45/240, Loss: 19.422743, Train_MMSE: 0.022895, NMMSE: 0.024438, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:43:39] Epoch 46/240, Loss: 19.402910, Train_MMSE: 0.022909, NMMSE: 0.024544, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:45:08] Epoch 47/240, Loss: 19.340389, Train_MMSE: 0.022907, NMMSE: 0.024536, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:46:39] Epoch 48/240, Loss: 19.362579, Train_MMSE: 0.022905, NMMSE: 0.02446, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:48:08] Epoch 49/240, Loss: 19.341932, Train_MMSE: 0.022908, NMMSE: 0.02443, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:51:41] Epoch 50/240, Loss: 19.593428, Train_MMSE: 0.022909, NMMSE: 0.024403, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:55:26] Epoch 51/240, Loss: 19.635002, Train_MMSE: 0.022889, NMMSE: 0.024453, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:56:56] Epoch 52/240, Loss: 19.360472, Train_MMSE: 0.022872, NMMSE: 0.024408, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 15:58:38] Epoch 53/240, Loss: 19.368090, Train_MMSE: 0.02289, NMMSE: 0.024469, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:00:15] Epoch 54/240, Loss: 19.202169, Train_MMSE: 0.022901, NMMSE: 0.024357, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:01:56] Epoch 55/240, Loss: 19.469431, Train_MMSE: 0.022896, NMMSE: 0.024455, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:03:34] Epoch 56/240, Loss: 19.414310, Train_MMSE: 0.022892, NMMSE: 0.024547, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:05:09] Epoch 57/240, Loss: 19.331009, Train_MMSE: 0.022871, NMMSE: 0.024497, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:06:44] Epoch 58/240, Loss: 19.453655, Train_MMSE: 0.022883, NMMSE: 0.024559, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:08:21] Epoch 59/240, Loss: 19.595308, Train_MMSE: 0.022863, NMMSE: 0.024563, LS_NMSE: 0.030273, Lr: 0.001
[2025-02-24 16:09:58] Epoch 60/240, Loss: 19.413021, Train_MMSE: 0.022877, NMMSE: 0.024419, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:11:33] Epoch 61/240, Loss: 19.433567, Train_MMSE: 0.022661, NMMSE: 0.024109, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:13:10] Epoch 62/240, Loss: 19.314684, Train_MMSE: 0.022622, NMMSE: 0.024103, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:14:46] Epoch 63/240, Loss: 19.288710, Train_MMSE: 0.022625, NMMSE: 0.024113, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:16:22] Epoch 64/240, Loss: 19.198954, Train_MMSE: 0.022628, NMMSE: 0.024106, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:17:57] Epoch 65/240, Loss: 19.083717, Train_MMSE: 0.022609, NMMSE: 0.024109, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:19:32] Epoch 66/240, Loss: 19.146774, Train_MMSE: 0.022625, NMMSE: 0.024147, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:21:10] Epoch 67/240, Loss: 19.145723, Train_MMSE: 0.022615, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:22:45] Epoch 68/240, Loss: 19.306236, Train_MMSE: 0.02263, NMMSE: 0.024111, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:24:20] Epoch 69/240, Loss: 19.194574, Train_MMSE: 0.022626, NMMSE: 0.024111, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:25:56] Epoch 70/240, Loss: 19.259329, Train_MMSE: 0.022603, NMMSE: 0.024126, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:27:32] Epoch 71/240, Loss: 19.350290, Train_MMSE: 0.022602, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:29:08] Epoch 72/240, Loss: 19.260878, Train_MMSE: 0.022609, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:30:43] Epoch 73/240, Loss: 19.337986, Train_MMSE: 0.022609, NMMSE: 0.024118, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:32:18] Epoch 74/240, Loss: 19.299463, Train_MMSE: 0.0226, NMMSE: 0.024122, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:33:54] Epoch 75/240, Loss: 19.154442, Train_MMSE: 0.022608, NMMSE: 0.024121, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:35:29] Epoch 76/240, Loss: 19.231178, Train_MMSE: 0.022613, NMMSE: 0.024114, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:37:03] Epoch 77/240, Loss: 19.219995, Train_MMSE: 0.0226, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:38:37] Epoch 78/240, Loss: 19.337063, Train_MMSE: 0.022594, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:40:12] Epoch 79/240, Loss: 19.181725, Train_MMSE: 0.022611, NMMSE: 0.024138, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:41:48] Epoch 80/240, Loss: 19.253790, Train_MMSE: 0.022597, NMMSE: 0.024125, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:43:23] Epoch 81/240, Loss: 19.212444, Train_MMSE: 0.022598, NMMSE: 0.024122, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:44:58] Epoch 82/240, Loss: 19.098898, Train_MMSE: 0.022598, NMMSE: 0.024131, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:46:33] Epoch 83/240, Loss: 19.235504, Train_MMSE: 0.022588, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:48:08] Epoch 84/240, Loss: 19.303001, Train_MMSE: 0.022591, NMMSE: 0.024138, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:49:43] Epoch 85/240, Loss: 19.439875, Train_MMSE: 0.022597, NMMSE: 0.024143, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:51:17] Epoch 86/240, Loss: 19.390537, Train_MMSE: 0.022581, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:52:52] Epoch 87/240, Loss: 19.219398, Train_MMSE: 0.022591, NMMSE: 0.024118, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:54:27] Epoch 88/240, Loss: 19.291901, Train_MMSE: 0.02258, NMMSE: 0.024147, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:56:02] Epoch 89/240, Loss: 19.182314, Train_MMSE: 0.022578, NMMSE: 0.024127, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:57:38] Epoch 90/240, Loss: 19.145864, Train_MMSE: 0.022584, NMMSE: 0.024148, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 16:59:13] Epoch 91/240, Loss: 19.240858, Train_MMSE: 0.022571, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:00:49] Epoch 92/240, Loss: 19.317968, Train_MMSE: 0.022583, NMMSE: 0.024148, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:02:23] Epoch 93/240, Loss: 19.305260, Train_MMSE: 0.02258, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:03:59] Epoch 94/240, Loss: 19.290701, Train_MMSE: 0.022597, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:05:34] Epoch 95/240, Loss: 19.381632, Train_MMSE: 0.022586, NMMSE: 0.024149, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:07:09] Epoch 96/240, Loss: 19.257080, Train_MMSE: 0.022576, NMMSE: 0.024178, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:08:43] Epoch 97/240, Loss: 19.368130, Train_MMSE: 0.022577, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:10:18] Epoch 98/240, Loss: 19.193775, Train_MMSE: 0.022577, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:11:52] Epoch 99/240, Loss: 19.310122, Train_MMSE: 0.022576, NMMSE: 0.024152, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:13:26] Epoch 100/240, Loss: 19.270969, Train_MMSE: 0.022571, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:15:01] Epoch 101/240, Loss: 19.316233, Train_MMSE: 0.022578, NMMSE: 0.024149, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:16:36] Epoch 102/240, Loss: 19.346378, Train_MMSE: 0.022592, NMMSE: 0.024149, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:18:11] Epoch 103/240, Loss: 19.282938, Train_MMSE: 0.022574, NMMSE: 0.024154, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:19:47] Epoch 104/240, Loss: 19.244408, Train_MMSE: 0.022564, NMMSE: 0.024144, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:21:22] Epoch 105/240, Loss: 19.246834, Train_MMSE: 0.022576, NMMSE: 0.024147, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:22:56] Epoch 106/240, Loss: 19.356693, Train_MMSE: 0.022571, NMMSE: 0.024148, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:24:31] Epoch 107/240, Loss: 19.391842, Train_MMSE: 0.02256, NMMSE: 0.024167, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:26:06] Epoch 108/240, Loss: 19.232592, Train_MMSE: 0.022573, NMMSE: 0.02415, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:27:41] Epoch 109/240, Loss: 19.198214, Train_MMSE: 0.02257, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:29:15] Epoch 110/240, Loss: 19.432159, Train_MMSE: 0.022577, NMMSE: 0.024155, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:30:50] Epoch 111/240, Loss: 19.278919, Train_MMSE: 0.022565, NMMSE: 0.024154, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:32:25] Epoch 112/240, Loss: 19.461695, Train_MMSE: 0.022581, NMMSE: 0.024155, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:34:00] Epoch 113/240, Loss: 19.261818, Train_MMSE: 0.022581, NMMSE: 0.024145, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:35:35] Epoch 114/240, Loss: 19.139469, Train_MMSE: 0.022579, NMMSE: 0.02416, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:37:12] Epoch 115/240, Loss: 19.280993, Train_MMSE: 0.022561, NMMSE: 0.024145, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:38:52] Epoch 116/240, Loss: 19.103693, Train_MMSE: 0.022574, NMMSE: 0.024156, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:40:26] Epoch 117/240, Loss: 19.253597, Train_MMSE: 0.022566, NMMSE: 0.024146, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:42:02] Epoch 118/240, Loss: 19.199514, Train_MMSE: 0.022571, NMMSE: 0.024154, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:43:37] Epoch 119/240, Loss: 19.288040, Train_MMSE: 0.02256, NMMSE: 0.024165, LS_NMSE: 0.030273, Lr: 0.0001
[2025-02-24 17:45:11] Epoch 120/240, Loss: 19.646332, Train_MMSE: 0.022571, NMMSE: 0.024163, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:46:46] Epoch 121/240, Loss: 19.234091, Train_MMSE: 0.022511, NMMSE: 0.024123, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:48:20] Epoch 122/240, Loss: 19.202681, Train_MMSE: 0.022519, NMMSE: 0.024151, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:49:53] Epoch 123/240, Loss: 19.229954, Train_MMSE: 0.022529, NMMSE: 0.024122, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:51:28] Epoch 124/240, Loss: 19.187540, Train_MMSE: 0.022515, NMMSE: 0.024122, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:53:03] Epoch 125/240, Loss: 19.314249, Train_MMSE: 0.022519, NMMSE: 0.024123, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:54:38] Epoch 126/240, Loss: 19.315434, Train_MMSE: 0.022523, NMMSE: 0.024125, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:56:13] Epoch 127/240, Loss: 19.276190, Train_MMSE: 0.022511, NMMSE: 0.024124, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:57:48] Epoch 128/240, Loss: 19.180943, Train_MMSE: 0.022528, NMMSE: 0.024126, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 17:59:25] Epoch 129/240, Loss: 19.263712, Train_MMSE: 0.02251, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:01:01] Epoch 130/240, Loss: 19.326349, Train_MMSE: 0.022533, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:02:37] Epoch 131/240, Loss: 19.223120, Train_MMSE: 0.022516, NMMSE: 0.024125, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:04:12] Epoch 132/240, Loss: 19.198677, Train_MMSE: 0.022511, NMMSE: 0.024125, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:05:47] Epoch 133/240, Loss: 19.351904, Train_MMSE: 0.02251, NMMSE: 0.024127, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:07:21] Epoch 134/240, Loss: 19.265528, Train_MMSE: 0.022506, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:08:57] Epoch 135/240, Loss: 19.115747, Train_MMSE: 0.022507, NMMSE: 0.024129, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:10:32] Epoch 136/240, Loss: 19.262613, Train_MMSE: 0.022518, NMMSE: 0.02413, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:12:11] Epoch 137/240, Loss: 19.185490, Train_MMSE: 0.02251, NMMSE: 0.024125, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:13:47] Epoch 138/240, Loss: 19.233313, Train_MMSE: 0.022501, NMMSE: 0.024129, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:15:22] Epoch 139/240, Loss: 19.372845, Train_MMSE: 0.022499, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:16:57] Epoch 140/240, Loss: 19.249531, Train_MMSE: 0.022521, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:18:32] Epoch 141/240, Loss: 19.423126, Train_MMSE: 0.022513, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:20:06] Epoch 142/240, Loss: 19.221338, Train_MMSE: 0.022515, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:21:41] Epoch 143/240, Loss: 19.318172, Train_MMSE: 0.022522, NMMSE: 0.024127, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:23:16] Epoch 144/240, Loss: 19.247036, Train_MMSE: 0.022505, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:24:50] Epoch 145/240, Loss: 19.216362, Train_MMSE: 0.022517, NMMSE: 0.02413, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:26:25] Epoch 146/240, Loss: 19.120043, Train_MMSE: 0.022519, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:28:00] Epoch 147/240, Loss: 19.069990, Train_MMSE: 0.022518, NMMSE: 0.024146, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:29:36] Epoch 148/240, Loss: 19.423655, Train_MMSE: 0.022521, NMMSE: 0.024131, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:31:11] Epoch 149/240, Loss: 19.160000, Train_MMSE: 0.022508, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:32:47] Epoch 150/240, Loss: 19.245079, Train_MMSE: 0.022519, NMMSE: 0.024154, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:34:22] Epoch 151/240, Loss: 19.256184, Train_MMSE: 0.022514, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:35:58] Epoch 152/240, Loss: 19.302597, Train_MMSE: 0.022518, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:37:33] Epoch 153/240, Loss: 19.225878, Train_MMSE: 0.02251, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:39:08] Epoch 154/240, Loss: 19.175364, Train_MMSE: 0.022514, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:40:43] Epoch 155/240, Loss: 19.204556, Train_MMSE: 0.022509, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:42:18] Epoch 156/240, Loss: 19.160217, Train_MMSE: 0.022512, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:43:53] Epoch 157/240, Loss: 19.111635, Train_MMSE: 0.022511, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:45:28] Epoch 158/240, Loss: 19.140440, Train_MMSE: 0.022513, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:47:03] Epoch 159/240, Loss: 19.081146, Train_MMSE: 0.022515, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:48:38] Epoch 160/240, Loss: 19.134510, Train_MMSE: 0.022523, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:50:14] Epoch 161/240, Loss: 19.294674, Train_MMSE: 0.022501, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:51:44] Epoch 162/240, Loss: 19.137014, Train_MMSE: 0.022501, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:52:57] Epoch 163/240, Loss: 19.244495, Train_MMSE: 0.022509, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:54:11] Epoch 164/240, Loss: 19.218103, Train_MMSE: 0.022521, NMMSE: 0.024141, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:55:26] Epoch 165/240, Loss: 19.149000, Train_MMSE: 0.022509, NMMSE: 0.024131, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:56:41] Epoch 166/240, Loss: 19.188606, Train_MMSE: 0.022518, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:57:55] Epoch 167/240, Loss: 19.180506, Train_MMSE: 0.022519, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 18:59:11] Epoch 168/240, Loss: 19.174356, Train_MMSE: 0.022512, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:00:26] Epoch 169/240, Loss: 19.146677, Train_MMSE: 0.022512, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:01:42] Epoch 170/240, Loss: 19.127327, Train_MMSE: 0.022504, NMMSE: 0.024151, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:02:57] Epoch 171/240, Loss: 19.168026, Train_MMSE: 0.022493, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:04:13] Epoch 172/240, Loss: 19.181030, Train_MMSE: 0.022502, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:05:30] Epoch 173/240, Loss: 19.405294, Train_MMSE: 0.022514, NMMSE: 0.02416, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:06:45] Epoch 174/240, Loss: 19.178005, Train_MMSE: 0.022507, NMMSE: 0.024146, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:08:01] Epoch 175/240, Loss: 19.211288, Train_MMSE: 0.022496, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:09:16] Epoch 176/240, Loss: 19.148882, Train_MMSE: 0.022507, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:10:31] Epoch 177/240, Loss: 19.208675, Train_MMSE: 0.022499, NMMSE: 0.024138, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:11:46] Epoch 178/240, Loss: 19.236280, Train_MMSE: 0.022493, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:13:01] Epoch 179/240, Loss: 19.262384, Train_MMSE: 0.022497, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 1e-05
[2025-02-24 19:14:17] Epoch 180/240, Loss: 19.065405, Train_MMSE: 0.022502, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:15:33] Epoch 181/240, Loss: 19.212734, Train_MMSE: 0.022489, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:16:48] Epoch 182/240, Loss: 19.122860, Train_MMSE: 0.022503, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:18:04] Epoch 183/240, Loss: 19.200905, Train_MMSE: 0.0225, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:19:20] Epoch 184/240, Loss: 19.196056, Train_MMSE: 0.0225, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:20:36] Epoch 185/240, Loss: 19.175379, Train_MMSE: 0.022498, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:21:50] Epoch 186/240, Loss: 19.230562, Train_MMSE: 0.022496, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:23:05] Epoch 187/240, Loss: 19.326820, Train_MMSE: 0.022499, NMMSE: 0.024151, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:24:20] Epoch 188/240, Loss: 19.223154, Train_MMSE: 0.022487, NMMSE: 0.024133, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:25:33] Epoch 189/240, Loss: 19.149696, Train_MMSE: 0.02249, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:26:48] Epoch 190/240, Loss: 19.266376, Train_MMSE: 0.0225, NMMSE: 0.024141, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:28:04] Epoch 191/240, Loss: 19.243841, Train_MMSE: 0.022495, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:29:19] Epoch 192/240, Loss: 19.310476, Train_MMSE: 0.022499, NMMSE: 0.024142, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:30:34] Epoch 193/240, Loss: 19.281631, Train_MMSE: 0.022496, NMMSE: 0.024132, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:31:49] Epoch 194/240, Loss: 19.221970, Train_MMSE: 0.022496, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:33:04] Epoch 195/240, Loss: 19.374638, Train_MMSE: 0.022505, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:34:19] Epoch 196/240, Loss: 19.210661, Train_MMSE: 0.022495, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:35:33] Epoch 197/240, Loss: 19.150032, Train_MMSE: 0.022496, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:36:48] Epoch 198/240, Loss: 19.219263, Train_MMSE: 0.022504, NMMSE: 0.024141, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:38:03] Epoch 199/240, Loss: 19.516935, Train_MMSE: 0.022505, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:39:17] Epoch 200/240, Loss: 19.338814, Train_MMSE: 0.022501, NMMSE: 0.024138, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:40:31] Epoch 201/240, Loss: 19.218189, Train_MMSE: 0.022493, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:41:47] Epoch 202/240, Loss: 19.567150, Train_MMSE: 0.022506, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:43:03] Epoch 203/240, Loss: 19.327431, Train_MMSE: 0.022505, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:44:17] Epoch 204/240, Loss: 19.278185, Train_MMSE: 0.022502, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:45:32] Epoch 205/240, Loss: 19.183861, Train_MMSE: 0.022505, NMMSE: 0.024145, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:46:47] Epoch 206/240, Loss: 19.123230, Train_MMSE: 0.02251, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:48:03] Epoch 207/240, Loss: 19.006130, Train_MMSE: 0.022504, NMMSE: 0.024144, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:49:18] Epoch 208/240, Loss: 19.410990, Train_MMSE: 0.022498, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:50:34] Epoch 209/240, Loss: 19.275812, Train_MMSE: 0.022491, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:51:48] Epoch 210/240, Loss: 19.228529, Train_MMSE: 0.022517, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:53:03] Epoch 211/240, Loss: 19.176086, Train_MMSE: 0.022496, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:54:18] Epoch 212/240, Loss: 19.170670, Train_MMSE: 0.022496, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:55:33] Epoch 213/240, Loss: 19.181454, Train_MMSE: 0.022485, NMMSE: 0.024144, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:56:47] Epoch 214/240, Loss: 19.194622, Train_MMSE: 0.022486, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:58:03] Epoch 215/240, Loss: 19.141386, Train_MMSE: 0.022502, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 19:59:18] Epoch 216/240, Loss: 19.248154, Train_MMSE: 0.022483, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:00:33] Epoch 217/240, Loss: 19.151340, Train_MMSE: 0.022498, NMMSE: 0.024146, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:01:49] Epoch 218/240, Loss: 19.197094, Train_MMSE: 0.022508, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:03:04] Epoch 219/240, Loss: 19.338690, Train_MMSE: 0.022495, NMMSE: 0.024153, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:04:19] Epoch 220/240, Loss: 19.310736, Train_MMSE: 0.022498, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:05:34] Epoch 221/240, Loss: 19.207275, Train_MMSE: 0.022508, NMMSE: 0.024141, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:06:50] Epoch 222/240, Loss: 19.319901, Train_MMSE: 0.022501, NMMSE: 0.024136, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:08:04] Epoch 223/240, Loss: 19.255136, Train_MMSE: 0.022494, NMMSE: 0.02414, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:09:07] Epoch 224/240, Loss: 19.067942, Train_MMSE: 0.022509, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:10:02] Epoch 225/240, Loss: 19.451698, Train_MMSE: 0.022496, NMMSE: 0.024147, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:10:57] Epoch 226/240, Loss: 19.176004, Train_MMSE: 0.022505, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:11:51] Epoch 227/240, Loss: 19.282669, Train_MMSE: 0.02249, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:12:46] Epoch 228/240, Loss: 19.435083, Train_MMSE: 0.022516, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:13:41] Epoch 229/240, Loss: 19.033213, Train_MMSE: 0.022496, NMMSE: 0.024139, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:14:36] Epoch 230/240, Loss: 19.204355, Train_MMSE: 0.022511, NMMSE: 0.024144, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:15:31] Epoch 231/240, Loss: 19.157379, Train_MMSE: 0.022487, NMMSE: 0.024137, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:16:28] Epoch 232/240, Loss: 19.224361, Train_MMSE: 0.022492, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:17:24] Epoch 233/240, Loss: 19.174063, Train_MMSE: 0.022489, NMMSE: 0.024134, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:18:17] Epoch 234/240, Loss: 19.164642, Train_MMSE: 0.022501, NMMSE: 0.024138, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:18:55] Epoch 235/240, Loss: 19.301479, Train_MMSE: 0.022488, NMMSE: 0.024144, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:19:31] Epoch 236/240, Loss: 19.165005, Train_MMSE: 0.022492, NMMSE: 0.024181, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:20:08] Epoch 237/240, Loss: 19.058367, Train_MMSE: 0.022501, NMMSE: 0.024148, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:20:45] Epoch 238/240, Loss: 19.223019, Train_MMSE: 0.022501, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:21:21] Epoch 239/240, Loss: 19.186558, Train_MMSE: 0.022498, NMMSE: 0.024135, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-06
[2025-02-24 20:21:57] Epoch 240/240, Loss: 19.254648, Train_MMSE: 0.022506, NMMSE: 0.024158, LS_NMSE: 0.030273, Lr: 1.0000000000000002e-07
