H shape: (50000, 4, 36) (50000, 4, 36)
NMMSE of valid dataset:: 0.1349612742577396
num samples :: 500000
num valid: 50000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_train_Dataset_dB-25_N36_K4_L4_S10_Setup500_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_test_Dataset_dB-25_N36_K4_L4_S10_Setup50_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 256, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(480, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 3.09 MB
loss function:: L1Loss()
[2025-02-18 18:38:01] Epoch 1/300, Loss: 52.956684, Train_MMSE: 0.205963, NMMSE: 0.161271, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:39:34] Epoch 2/300, Loss: 51.726223, Train_MMSE: 0.162515, NMMSE: 0.156986, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:41:04] Epoch 3/300, Loss: 51.934124, Train_MMSE: 0.160255, NMMSE: 0.155501, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:42:32] Epoch 4/300, Loss: 52.423267, Train_MMSE: 0.161077, NMMSE: 0.155673, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:44:03] Epoch 5/300, Loss: 51.822353, Train_MMSE: 0.158698, NMMSE: 0.15432, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:45:35] Epoch 6/300, Loss: 51.693359, Train_MMSE: 0.159176, NMMSE: 0.15783, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:47:06] Epoch 7/300, Loss: 51.362675, Train_MMSE: 0.158043, NMMSE: 0.157042, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:48:41] Epoch 8/300, Loss: 51.464329, Train_MMSE: 0.157859, NMMSE: 0.155631, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:50:13] Epoch 9/300, Loss: 51.481995, Train_MMSE: 0.158512, NMMSE: 0.155833, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:51:48] Epoch 10/300, Loss: 51.092457, Train_MMSE: 0.157476, NMMSE: 0.153579, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:53:19] Epoch 11/300, Loss: 51.381290, Train_MMSE: 0.157349, NMMSE: 0.154872, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:54:54] Epoch 12/300, Loss: 51.634056, Train_MMSE: 0.157269, NMMSE: 0.156032, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:56:28] Epoch 13/300, Loss: 51.686825, Train_MMSE: 0.158844, NMMSE: 0.278437, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:57:58] Epoch 14/300, Loss: 51.460995, Train_MMSE: 0.156894, NMMSE: 0.154221, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 18:59:30] Epoch 15/300, Loss: 51.205086, Train_MMSE: 0.156943, NMMSE: 0.153866, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:00:59] Epoch 16/300, Loss: 51.353931, Train_MMSE: 0.156862, NMMSE: 0.15233, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:02:36] Epoch 17/300, Loss: 51.683445, Train_MMSE: 0.156848, NMMSE: 0.15378, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:04:10] Epoch 18/300, Loss: 51.622513, Train_MMSE: 0.157963, NMMSE: 0.153399, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:05:41] Epoch 19/300, Loss: 51.562111, Train_MMSE: 0.158139, NMMSE: 0.154043, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:07:15] Epoch 20/300, Loss: 51.308834, Train_MMSE: 0.157413, NMMSE: 0.152954, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:08:47] Epoch 21/300, Loss: 51.667595, Train_MMSE: 0.156586, NMMSE: 0.154305, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:10:20] Epoch 22/300, Loss: 51.447578, Train_MMSE: 0.156592, NMMSE: 0.155729, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:11:50] Epoch 23/300, Loss: 51.375286, Train_MMSE: 0.156529, NMMSE: 0.154663, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:13:23] Epoch 24/300, Loss: 51.299507, Train_MMSE: 0.158946, NMMSE: 0.154323, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:14:57] Epoch 25/300, Loss: 50.630325, Train_MMSE: 0.157278, NMMSE: 0.152572, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:16:28] Epoch 26/300, Loss: 51.774872, Train_MMSE: 0.156357, NMMSE: 0.152566, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:18:02] Epoch 27/300, Loss: 51.323574, Train_MMSE: 0.156383, NMMSE: 0.152912, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:19:36] Epoch 28/300, Loss: 58.613785, Train_MMSE: 0.162232, NMMSE: 0.246977, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:21:09] Epoch 29/300, Loss: 51.907749, Train_MMSE: 0.163837, NMMSE: 0.155844, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:22:43] Epoch 30/300, Loss: 51.104481, Train_MMSE: 0.158946, NMMSE: 0.154678, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:24:13] Epoch 31/300, Loss: 51.672478, Train_MMSE: 0.157293, NMMSE: 0.156829, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:25:44] Epoch 32/300, Loss: 51.262928, Train_MMSE: 0.156943, NMMSE: 0.156648, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:27:19] Epoch 33/300, Loss: 51.276077, Train_MMSE: 0.157177, NMMSE: 0.157227, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:28:49] Epoch 34/300, Loss: 51.578968, Train_MMSE: 0.156812, NMMSE: 0.157606, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:30:21] Epoch 35/300, Loss: 50.914387, Train_MMSE: 0.156617, NMMSE: 0.152839, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:31:54] Epoch 36/300, Loss: 51.358097, Train_MMSE: 0.15648, NMMSE: 0.154928, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:33:25] Epoch 37/300, Loss: 51.428860, Train_MMSE: 0.158424, NMMSE: 0.163762, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:34:56] Epoch 38/300, Loss: 50.980610, Train_MMSE: 0.157479, NMMSE: 0.156452, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:36:27] Epoch 39/300, Loss: 51.006798, Train_MMSE: 0.156297, NMMSE: 0.153975, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:37:59] Epoch 40/300, Loss: 51.350685, Train_MMSE: 0.156239, NMMSE: 0.157759, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:39:31] Epoch 41/300, Loss: 51.136280, Train_MMSE: 0.156398, NMMSE: 0.156034, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:41:05] Epoch 42/300, Loss: 50.772415, Train_MMSE: 0.156259, NMMSE: 0.156601, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:42:39] Epoch 43/300, Loss: 50.988609, Train_MMSE: 0.156148, NMMSE: 0.156942, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:44:10] Epoch 44/300, Loss: 51.415478, Train_MMSE: 0.156191, NMMSE: 0.157997, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:45:40] Epoch 45/300, Loss: 50.922005, Train_MMSE: 0.158596, NMMSE: 0.156724, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:47:10] Epoch 46/300, Loss: 50.907471, Train_MMSE: 0.156049, NMMSE: 0.152915, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:48:40] Epoch 47/300, Loss: 51.449177, Train_MMSE: 0.156115, NMMSE: 0.155422, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:50:14] Epoch 48/300, Loss: 51.688473, Train_MMSE: 0.156113, NMMSE: 0.15261, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:51:46] Epoch 49/300, Loss: 50.934273, Train_MMSE: 0.156086, NMMSE: 0.155633, LS_NMSE: 0.466596, Lr: 0.01
[2025-02-18 19:53:14] Epoch 50/300, Loss: 50.948196, Train_MMSE: 0.158538, NMMSE: 0.159431, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 19:54:48] Epoch 51/300, Loss: 50.448593, Train_MMSE: 0.15184, NMMSE: 0.146845, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 19:56:20] Epoch 52/300, Loss: 50.774197, Train_MMSE: 0.151662, NMMSE: 0.146456, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 19:57:54] Epoch 53/300, Loss: 50.819561, Train_MMSE: 0.151612, NMMSE: 0.146447, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 19:59:27] Epoch 54/300, Loss: 51.163658, Train_MMSE: 0.151584, NMMSE: 0.146469, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:01:06] Epoch 55/300, Loss: 50.710228, Train_MMSE: 0.151562, NMMSE: 0.146729, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:02:57] Epoch 56/300, Loss: 49.892994, Train_MMSE: 0.151514, NMMSE: 0.14633, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:04:56] Epoch 57/300, Loss: 50.402760, Train_MMSE: 0.151494, NMMSE: 0.146547, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:07:05] Epoch 58/300, Loss: 50.380856, Train_MMSE: 0.151482, NMMSE: 0.146735, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:09:21] Epoch 59/300, Loss: 50.439190, Train_MMSE: 0.151468, NMMSE: 0.147371, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:11:38] Epoch 60/300, Loss: 50.559376, Train_MMSE: 0.151471, NMMSE: 0.147619, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:13:54] Epoch 61/300, Loss: 49.950623, Train_MMSE: 0.151458, NMMSE: 0.149735, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:16:12] Epoch 62/300, Loss: 50.731419, Train_MMSE: 0.151463, NMMSE: 0.147025, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:18:24] Epoch 63/300, Loss: 50.086880, Train_MMSE: 0.151452, NMMSE: 0.147891, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:20:41] Epoch 64/300, Loss: 50.230900, Train_MMSE: 0.15144, NMMSE: 0.155429, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:22:56] Epoch 65/300, Loss: 51.104816, Train_MMSE: 0.151443, NMMSE: 0.147093, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:25:14] Epoch 66/300, Loss: 50.854691, Train_MMSE: 0.151445, NMMSE: 0.153077, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:27:35] Epoch 67/300, Loss: 50.577503, Train_MMSE: 0.151436, NMMSE: 0.14729, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:29:56] Epoch 68/300, Loss: 50.311928, Train_MMSE: 0.151445, NMMSE: 0.14777, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:32:08] Epoch 69/300, Loss: 50.612679, Train_MMSE: 0.151416, NMMSE: 0.148075, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:34:28] Epoch 70/300, Loss: 50.635700, Train_MMSE: 0.151424, NMMSE: 0.14703, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:36:50] Epoch 71/300, Loss: 50.509750, Train_MMSE: 0.151426, NMMSE: 0.146688, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:39:10] Epoch 72/300, Loss: 50.504513, Train_MMSE: 0.151418, NMMSE: 0.146571, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:41:26] Epoch 73/300, Loss: 50.233589, Train_MMSE: 0.151442, NMMSE: 0.149109, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:43:05] Epoch 74/300, Loss: 50.510330, Train_MMSE: 0.151441, NMMSE: 0.147422, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:44:40] Epoch 75/300, Loss: 50.026775, Train_MMSE: 0.151441, NMMSE: 0.146365, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:46:15] Epoch 76/300, Loss: 50.183475, Train_MMSE: 0.151445, NMMSE: 0.14683, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:47:47] Epoch 77/300, Loss: 50.526871, Train_MMSE: 0.151449, NMMSE: 0.156666, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:49:17] Epoch 78/300, Loss: 50.604710, Train_MMSE: 0.151441, NMMSE: 0.146574, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:50:53] Epoch 79/300, Loss: 50.418545, Train_MMSE: 0.151415, NMMSE: 0.14828, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:52:22] Epoch 80/300, Loss: 50.705902, Train_MMSE: 0.15143, NMMSE: 0.147161, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:53:51] Epoch 81/300, Loss: 50.260098, Train_MMSE: 0.151444, NMMSE: 0.149351, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:55:22] Epoch 82/300, Loss: 50.844837, Train_MMSE: 0.151443, NMMSE: 0.161469, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:56:54] Epoch 83/300, Loss: 50.764851, Train_MMSE: 0.151433, NMMSE: 0.14661, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:58:24] Epoch 84/300, Loss: 50.936279, Train_MMSE: 0.151438, NMMSE: 0.146536, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 20:59:57] Epoch 85/300, Loss: 50.340977, Train_MMSE: 0.151425, NMMSE: 0.147013, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:01:32] Epoch 86/300, Loss: 50.665447, Train_MMSE: 0.151438, NMMSE: 0.1476, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:03:10] Epoch 87/300, Loss: 50.396313, Train_MMSE: 0.151426, NMMSE: 0.146922, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:04:42] Epoch 88/300, Loss: 50.386368, Train_MMSE: 0.151426, NMMSE: 0.159566, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:06:17] Epoch 89/300, Loss: 50.718777, Train_MMSE: 0.151439, NMMSE: 0.158384, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:07:49] Epoch 90/300, Loss: 50.583237, Train_MMSE: 0.151441, NMMSE: 0.147995, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:09:23] Epoch 91/300, Loss: 50.042019, Train_MMSE: 0.151426, NMMSE: 0.14767, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:10:57] Epoch 92/300, Loss: 50.682182, Train_MMSE: 0.151422, NMMSE: 0.146948, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:12:33] Epoch 93/300, Loss: 50.169205, Train_MMSE: 0.151434, NMMSE: 0.147069, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:14:05] Epoch 94/300, Loss: 50.284851, Train_MMSE: 0.151452, NMMSE: 0.146932, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:15:42] Epoch 95/300, Loss: 50.655636, Train_MMSE: 0.151436, NMMSE: 0.146818, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:17:12] Epoch 96/300, Loss: 50.108143, Train_MMSE: 0.15143, NMMSE: 0.148734, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:18:45] Epoch 97/300, Loss: 50.692287, Train_MMSE: 0.151429, NMMSE: 0.146886, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:20:16] Epoch 98/300, Loss: 50.657314, Train_MMSE: 0.151421, NMMSE: 0.146741, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:21:51] Epoch 99/300, Loss: 50.387070, Train_MMSE: 0.151448, NMMSE: 0.147994, LS_NMSE: 0.466596, Lr: 0.001
[2025-02-18 21:23:25] Epoch 100/300, Loss: 50.914425, Train_MMSE: 0.151441, NMMSE: 0.146951, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:24:56] Epoch 101/300, Loss: 50.349426, Train_MMSE: 0.15044, NMMSE: 0.144821, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:26:29] Epoch 102/300, Loss: 50.474220, Train_MMSE: 0.150383, NMMSE: 0.144755, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:27:58] Epoch 103/300, Loss: 50.604439, Train_MMSE: 0.150354, NMMSE: 0.144802, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:29:31] Epoch 104/300, Loss: 50.498707, Train_MMSE: 0.150365, NMMSE: 0.144798, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:31:03] Epoch 105/300, Loss: 50.350822, Train_MMSE: 0.150337, NMMSE: 0.144788, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:32:39] Epoch 106/300, Loss: 50.149406, Train_MMSE: 0.15033, NMMSE: 0.144929, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:34:12] Epoch 107/300, Loss: 50.284134, Train_MMSE: 0.150324, NMMSE: 0.144751, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:35:41] Epoch 108/300, Loss: 49.795002, Train_MMSE: 0.150317, NMMSE: 0.144813, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:37:15] Epoch 109/300, Loss: 50.876221, Train_MMSE: 0.150318, NMMSE: 0.144872, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:38:47] Epoch 110/300, Loss: 50.358086, Train_MMSE: 0.150311, NMMSE: 0.144756, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:40:14] Epoch 111/300, Loss: 50.058170, Train_MMSE: 0.150307, NMMSE: 0.14476, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:41:52] Epoch 112/300, Loss: 50.183979, Train_MMSE: 0.150292, NMMSE: 0.144736, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:43:26] Epoch 113/300, Loss: 49.953541, Train_MMSE: 0.150294, NMMSE: 0.144845, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:44:57] Epoch 114/300, Loss: 49.986980, Train_MMSE: 0.150284, NMMSE: 0.144746, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:46:32] Epoch 115/300, Loss: 50.449036, Train_MMSE: 0.150281, NMMSE: 0.144685, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:48:08] Epoch 116/300, Loss: 49.911457, Train_MMSE: 0.150281, NMMSE: 0.144735, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:49:41] Epoch 117/300, Loss: 50.697739, Train_MMSE: 0.150282, NMMSE: 0.144713, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:51:13] Epoch 118/300, Loss: 50.426670, Train_MMSE: 0.150272, NMMSE: 0.144777, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:52:43] Epoch 119/300, Loss: 49.923782, Train_MMSE: 0.150273, NMMSE: 0.14484, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:54:15] Epoch 120/300, Loss: 50.087086, Train_MMSE: 0.150274, NMMSE: 0.14469, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:55:49] Epoch 121/300, Loss: 50.249233, Train_MMSE: 0.150263, NMMSE: 0.144973, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:57:19] Epoch 122/300, Loss: 50.456284, Train_MMSE: 0.150267, NMMSE: 0.144717, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 21:58:52] Epoch 123/300, Loss: 50.328079, Train_MMSE: 0.150264, NMMSE: 0.144695, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:00:24] Epoch 124/300, Loss: 50.144245, Train_MMSE: 0.15026, NMMSE: 0.14471, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:01:47] Epoch 125/300, Loss: 50.420010, Train_MMSE: 0.150253, NMMSE: 0.144759, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:03:19] Epoch 126/300, Loss: 50.397015, Train_MMSE: 0.150256, NMMSE: 0.144727, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:04:48] Epoch 127/300, Loss: 50.232727, Train_MMSE: 0.150254, NMMSE: 0.144669, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:06:16] Epoch 128/300, Loss: 50.088116, Train_MMSE: 0.150249, NMMSE: 0.144781, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:07:44] Epoch 129/300, Loss: 50.196560, Train_MMSE: 0.150245, NMMSE: 0.144731, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:09:16] Epoch 130/300, Loss: 50.215210, Train_MMSE: 0.150252, NMMSE: 0.144619, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:10:44] Epoch 131/300, Loss: 50.680622, Train_MMSE: 0.150242, NMMSE: 0.144675, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:12:11] Epoch 132/300, Loss: 50.542809, Train_MMSE: 0.150239, NMMSE: 0.144715, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:13:40] Epoch 133/300, Loss: 50.578728, Train_MMSE: 0.15024, NMMSE: 0.144738, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:15:09] Epoch 134/300, Loss: 50.268406, Train_MMSE: 0.150237, NMMSE: 0.144929, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:16:38] Epoch 135/300, Loss: 50.371277, Train_MMSE: 0.150237, NMMSE: 0.144783, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:18:08] Epoch 136/300, Loss: 51.031658, Train_MMSE: 0.150229, NMMSE: 0.144707, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:19:39] Epoch 137/300, Loss: 50.365742, Train_MMSE: 0.150232, NMMSE: 0.144665, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:21:13] Epoch 138/300, Loss: 50.150513, Train_MMSE: 0.150219, NMMSE: 0.144731, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:22:43] Epoch 139/300, Loss: 49.977543, Train_MMSE: 0.150231, NMMSE: 0.144664, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:24:11] Epoch 140/300, Loss: 50.387653, Train_MMSE: 0.150222, NMMSE: 0.144682, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:25:41] Epoch 141/300, Loss: 50.104183, Train_MMSE: 0.150232, NMMSE: 0.144737, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:27:10] Epoch 142/300, Loss: 50.206963, Train_MMSE: 0.150231, NMMSE: 0.144685, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:28:40] Epoch 143/300, Loss: 50.363628, Train_MMSE: 0.150226, NMMSE: 0.144682, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:30:12] Epoch 144/300, Loss: 50.528389, Train_MMSE: 0.150234, NMMSE: 0.144775, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:31:42] Epoch 145/300, Loss: 50.733543, Train_MMSE: 0.150221, NMMSE: 0.144736, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:33:11] Epoch 146/300, Loss: 50.214409, Train_MMSE: 0.150219, NMMSE: 0.144718, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:34:39] Epoch 147/300, Loss: 49.906967, Train_MMSE: 0.150208, NMMSE: 0.144695, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:36:11] Epoch 148/300, Loss: 50.273769, Train_MMSE: 0.150212, NMMSE: 0.145007, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:37:41] Epoch 149/300, Loss: 50.220676, Train_MMSE: 0.15021, NMMSE: 0.144959, LS_NMSE: 0.466596, Lr: 0.0001
[2025-02-18 22:39:13] Epoch 150/300, Loss: 50.246334, Train_MMSE: 0.150209, NMMSE: 0.144658, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:40:44] Epoch 151/300, Loss: 50.176228, Train_MMSE: 0.150045, NMMSE: 0.144475, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:42:15] Epoch 152/300, Loss: 50.048939, Train_MMSE: 0.15003, NMMSE: 0.144445, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:43:48] Epoch 153/300, Loss: 50.090744, Train_MMSE: 0.15004, NMMSE: 0.144457, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:45:23] Epoch 154/300, Loss: 50.131695, Train_MMSE: 0.150028, NMMSE: 0.144434, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:46:52] Epoch 155/300, Loss: 50.734951, Train_MMSE: 0.150032, NMMSE: 0.144442, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:48:25] Epoch 156/300, Loss: 50.357998, Train_MMSE: 0.150042, NMMSE: 0.144457, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:49:51] Epoch 157/300, Loss: 50.627056, Train_MMSE: 0.150042, NMMSE: 0.14447, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:51:22] Epoch 158/300, Loss: 50.520630, Train_MMSE: 0.150033, NMMSE: 0.144478, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:52:48] Epoch 159/300, Loss: 49.893291, Train_MMSE: 0.15003, NMMSE: 0.144453, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:54:19] Epoch 160/300, Loss: 50.083504, Train_MMSE: 0.150033, NMMSE: 0.144439, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:55:48] Epoch 161/300, Loss: 50.072502, Train_MMSE: 0.150041, NMMSE: 0.14447, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:57:19] Epoch 162/300, Loss: 50.317940, Train_MMSE: 0.150025, NMMSE: 0.14444, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 22:58:44] Epoch 163/300, Loss: 50.635231, Train_MMSE: 0.150029, NMMSE: 0.144455, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:00:11] Epoch 164/300, Loss: 50.568924, Train_MMSE: 0.150034, NMMSE: 0.144432, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:01:40] Epoch 165/300, Loss: 50.951691, Train_MMSE: 0.150028, NMMSE: 0.144452, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:03:11] Epoch 166/300, Loss: 50.527344, Train_MMSE: 0.150034, NMMSE: 0.144462, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:04:38] Epoch 167/300, Loss: 50.292622, Train_MMSE: 0.150031, NMMSE: 0.144465, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:06:06] Epoch 168/300, Loss: 50.270931, Train_MMSE: 0.150026, NMMSE: 0.144436, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:07:37] Epoch 169/300, Loss: 50.080051, Train_MMSE: 0.150033, NMMSE: 0.144439, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:09:08] Epoch 170/300, Loss: 50.395679, Train_MMSE: 0.150023, NMMSE: 0.144452, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:10:38] Epoch 171/300, Loss: 50.065273, Train_MMSE: 0.150026, NMMSE: 0.144438, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:12:08] Epoch 172/300, Loss: 50.069553, Train_MMSE: 0.150034, NMMSE: 0.144429, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:13:35] Epoch 173/300, Loss: 50.330021, Train_MMSE: 0.150023, NMMSE: 0.144429, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:15:04] Epoch 174/300, Loss: 50.087940, Train_MMSE: 0.150021, NMMSE: 0.144435, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:16:34] Epoch 175/300, Loss: 50.388641, Train_MMSE: 0.150026, NMMSE: 0.144433, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:18:05] Epoch 176/300, Loss: 49.909691, Train_MMSE: 0.150021, NMMSE: 0.144437, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:19:38] Epoch 177/300, Loss: 50.264423, Train_MMSE: 0.150023, NMMSE: 0.144426, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:21:12] Epoch 178/300, Loss: 50.323456, Train_MMSE: 0.150026, NMMSE: 0.144481, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:22:43] Epoch 179/300, Loss: 50.102482, Train_MMSE: 0.150024, NMMSE: 0.144428, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:24:14] Epoch 180/300, Loss: 49.768059, Train_MMSE: 0.150021, NMMSE: 0.144421, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:25:43] Epoch 181/300, Loss: 50.215473, Train_MMSE: 0.150021, NMMSE: 0.144427, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:27:14] Epoch 182/300, Loss: 50.237904, Train_MMSE: 0.150017, NMMSE: 0.144451, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:28:45] Epoch 183/300, Loss: 49.916702, Train_MMSE: 0.150024, NMMSE: 0.144425, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:30:15] Epoch 184/300, Loss: 50.071384, Train_MMSE: 0.150013, NMMSE: 0.144424, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:31:38] Epoch 185/300, Loss: 50.050098, Train_MMSE: 0.150018, NMMSE: 0.144432, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:33:06] Epoch 186/300, Loss: 50.433556, Train_MMSE: 0.150019, NMMSE: 0.144447, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:34:36] Epoch 187/300, Loss: 49.913261, Train_MMSE: 0.150015, NMMSE: 0.144433, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:36:08] Epoch 188/300, Loss: 50.316204, Train_MMSE: 0.150021, NMMSE: 0.144429, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:37:39] Epoch 189/300, Loss: 50.022755, Train_MMSE: 0.150012, NMMSE: 0.144437, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:39:07] Epoch 190/300, Loss: 50.140854, Train_MMSE: 0.150016, NMMSE: 0.14443, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:40:36] Epoch 191/300, Loss: 50.891796, Train_MMSE: 0.150028, NMMSE: 0.144421, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:42:04] Epoch 192/300, Loss: 49.716846, Train_MMSE: 0.150016, NMMSE: 0.144459, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:43:33] Epoch 193/300, Loss: 50.007038, Train_MMSE: 0.150019, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:45:05] Epoch 194/300, Loss: 50.150108, Train_MMSE: 0.150015, NMMSE: 0.144418, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:46:37] Epoch 195/300, Loss: 49.929085, Train_MMSE: 0.150021, NMMSE: 0.144411, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:48:07] Epoch 196/300, Loss: 50.344231, Train_MMSE: 0.150014, NMMSE: 0.144431, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:49:37] Epoch 197/300, Loss: 49.686050, Train_MMSE: 0.150025, NMMSE: 0.144444, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:51:08] Epoch 198/300, Loss: 50.098419, Train_MMSE: 0.150019, NMMSE: 0.144425, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:52:35] Epoch 199/300, Loss: 50.410625, Train_MMSE: 0.150017, NMMSE: 0.144421, LS_NMSE: 0.466596, Lr: 1e-05
[2025-02-18 23:54:03] Epoch 200/300, Loss: 49.980915, Train_MMSE: 0.150014, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-18 23:55:32] Epoch 201/300, Loss: 50.636326, Train_MMSE: 0.149994, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-18 23:57:03] Epoch 202/300, Loss: 50.057194, Train_MMSE: 0.149997, NMMSE: 0.144428, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-18 23:58:32] Epoch 203/300, Loss: 49.986599, Train_MMSE: 0.149988, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:00:02] Epoch 204/300, Loss: 49.900520, Train_MMSE: 0.15, NMMSE: 0.14441, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:01:29] Epoch 205/300, Loss: 50.190521, Train_MMSE: 0.149989, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:02:58] Epoch 206/300, Loss: 50.126209, Train_MMSE: 0.149993, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:04:27] Epoch 207/300, Loss: 50.725842, Train_MMSE: 0.149993, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:05:59] Epoch 208/300, Loss: 50.200027, Train_MMSE: 0.149985, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:07:28] Epoch 209/300, Loss: 50.330387, Train_MMSE: 0.149992, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:08:57] Epoch 210/300, Loss: 50.244022, Train_MMSE: 0.149984, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:10:23] Epoch 211/300, Loss: 49.928211, Train_MMSE: 0.149988, NMMSE: 0.144412, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:11:58] Epoch 212/300, Loss: 50.286339, Train_MMSE: 0.149985, NMMSE: 0.14441, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:13:31] Epoch 213/300, Loss: 51.312149, Train_MMSE: 0.149984, NMMSE: 0.144427, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:15:03] Epoch 214/300, Loss: 50.116287, Train_MMSE: 0.149992, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:16:34] Epoch 215/300, Loss: 49.860100, Train_MMSE: 0.149985, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:18:06] Epoch 216/300, Loss: 50.007793, Train_MMSE: 0.149983, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:19:35] Epoch 217/300, Loss: 50.419308, Train_MMSE: 0.149986, NMMSE: 0.144409, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:21:09] Epoch 218/300, Loss: 49.976074, Train_MMSE: 0.149995, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:22:39] Epoch 219/300, Loss: 50.348946, Train_MMSE: 0.149983, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:24:10] Epoch 220/300, Loss: 50.105949, Train_MMSE: 0.14999, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:25:41] Epoch 221/300, Loss: 49.907452, Train_MMSE: 0.149997, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:27:13] Epoch 222/300, Loss: 50.466331, Train_MMSE: 0.149992, NMMSE: 0.144435, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:28:46] Epoch 223/300, Loss: 50.429920, Train_MMSE: 0.14999, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:30:19] Epoch 224/300, Loss: 50.333958, Train_MMSE: 0.149992, NMMSE: 0.14441, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:31:52] Epoch 225/300, Loss: 49.920765, Train_MMSE: 0.149994, NMMSE: 0.144423, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:33:20] Epoch 226/300, Loss: 50.152420, Train_MMSE: 0.149994, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:34:50] Epoch 227/300, Loss: 50.075550, Train_MMSE: 0.149997, NMMSE: 0.144418, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:36:18] Epoch 228/300, Loss: 50.083118, Train_MMSE: 0.149988, NMMSE: 0.144421, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:37:50] Epoch 229/300, Loss: 49.771156, Train_MMSE: 0.149985, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:39:21] Epoch 230/300, Loss: 50.423374, Train_MMSE: 0.149988, NMMSE: 0.144432, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:40:52] Epoch 231/300, Loss: 50.411549, Train_MMSE: 0.150005, NMMSE: 0.144481, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:42:24] Epoch 232/300, Loss: 50.297047, Train_MMSE: 0.149989, NMMSE: 0.144407, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:43:55] Epoch 233/300, Loss: 50.398304, Train_MMSE: 0.149993, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:45:26] Epoch 234/300, Loss: 50.358051, Train_MMSE: 0.149987, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:46:56] Epoch 235/300, Loss: 49.907757, Train_MMSE: 0.149992, NMMSE: 0.144414, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:48:27] Epoch 236/300, Loss: 50.111526, Train_MMSE: 0.149996, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:49:59] Epoch 237/300, Loss: 50.594685, Train_MMSE: 0.14999, NMMSE: 0.144454, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:51:30] Epoch 238/300, Loss: 49.990852, Train_MMSE: 0.149988, NMMSE: 0.144404, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:53:02] Epoch 239/300, Loss: 50.256672, Train_MMSE: 0.14999, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:54:33] Epoch 240/300, Loss: 50.350922, Train_MMSE: 0.149996, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:56:02] Epoch 241/300, Loss: 49.791237, Train_MMSE: 0.149989, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:57:31] Epoch 242/300, Loss: 50.545403, Train_MMSE: 0.14999, NMMSE: 0.144436, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 00:59:03] Epoch 243/300, Loss: 50.274265, Train_MMSE: 0.149981, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:00:34] Epoch 244/300, Loss: 49.987411, Train_MMSE: 0.149983, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:02:03] Epoch 245/300, Loss: 50.332623, Train_MMSE: 0.149989, NMMSE: 0.144403, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:03:36] Epoch 246/300, Loss: 50.068558, Train_MMSE: 0.149985, NMMSE: 0.14441, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:05:03] Epoch 247/300, Loss: 50.028839, Train_MMSE: 0.149994, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:06:34] Epoch 248/300, Loss: 50.132885, Train_MMSE: 0.149989, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:08:09] Epoch 249/300, Loss: 49.893353, Train_MMSE: 0.149999, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-06
[2025-02-19 01:09:43] Epoch 250/300, Loss: 50.020611, Train_MMSE: 0.149987, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:11:16] Epoch 251/300, Loss: 50.045135, Train_MMSE: 0.149979, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:12:53] Epoch 252/300, Loss: 50.128063, Train_MMSE: 0.149984, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:14:31] Epoch 253/300, Loss: 50.590576, Train_MMSE: 0.149992, NMMSE: 0.144408, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:16:07] Epoch 254/300, Loss: 50.057362, Train_MMSE: 0.149983, NMMSE: 0.144407, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:17:44] Epoch 255/300, Loss: 50.542881, Train_MMSE: 0.149984, NMMSE: 0.144426, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:19:17] Epoch 256/300, Loss: 50.571362, Train_MMSE: 0.149988, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:20:54] Epoch 257/300, Loss: 50.486710, Train_MMSE: 0.14999, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:22:31] Epoch 258/300, Loss: 50.339039, Train_MMSE: 0.149982, NMMSE: 0.144427, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:24:08] Epoch 259/300, Loss: 50.054234, Train_MMSE: 0.149989, NMMSE: 0.144411, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:25:43] Epoch 260/300, Loss: 50.386879, Train_MMSE: 0.149984, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:27:19] Epoch 261/300, Loss: 49.256023, Train_MMSE: 0.149985, NMMSE: 0.144421, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:28:51] Epoch 262/300, Loss: 50.244766, Train_MMSE: 0.149995, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:30:23] Epoch 263/300, Loss: 50.449970, Train_MMSE: 0.149979, NMMSE: 0.144404, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:31:50] Epoch 264/300, Loss: 50.599121, Train_MMSE: 0.149991, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:33:20] Epoch 265/300, Loss: 50.523777, Train_MMSE: 0.14999, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:34:53] Epoch 266/300, Loss: 50.619453, Train_MMSE: 0.149986, NMMSE: 0.144404, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:36:24] Epoch 267/300, Loss: 49.836414, Train_MMSE: 0.149983, NMMSE: 0.144416, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:37:56] Epoch 268/300, Loss: 49.862705, Train_MMSE: 0.14998, NMMSE: 0.144398, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:39:27] Epoch 269/300, Loss: 50.451054, Train_MMSE: 0.149979, NMMSE: 0.144438, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:40:58] Epoch 270/300, Loss: 50.516197, Train_MMSE: 0.149979, NMMSE: 0.144415, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:42:25] Epoch 271/300, Loss: 50.492950, Train_MMSE: 0.149978, NMMSE: 0.144412, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:43:55] Epoch 272/300, Loss: 50.021465, Train_MMSE: 0.149985, NMMSE: 0.144411, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:45:27] Epoch 273/300, Loss: 50.285519, Train_MMSE: 0.149981, NMMSE: 0.144411, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:46:57] Epoch 274/300, Loss: 50.535698, Train_MMSE: 0.149993, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:48:29] Epoch 275/300, Loss: 50.193516, Train_MMSE: 0.14999, NMMSE: 0.144402, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:50:03] Epoch 276/300, Loss: 50.339378, Train_MMSE: 0.149987, NMMSE: 0.144417, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:51:34] Epoch 277/300, Loss: 50.251709, Train_MMSE: 0.14999, NMMSE: 0.144425, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:53:07] Epoch 278/300, Loss: 50.518452, Train_MMSE: 0.149977, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:54:40] Epoch 279/300, Loss: 50.574448, Train_MMSE: 0.149993, NMMSE: 0.144408, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:56:13] Epoch 280/300, Loss: 50.288326, Train_MMSE: 0.149981, NMMSE: 0.144414, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:57:47] Epoch 281/300, Loss: 50.136410, Train_MMSE: 0.149988, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 01:59:19] Epoch 282/300, Loss: 50.159771, Train_MMSE: 0.149991, NMMSE: 0.144419, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:00:47] Epoch 283/300, Loss: 49.957981, Train_MMSE: 0.149991, NMMSE: 0.144406, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:02:22] Epoch 284/300, Loss: 50.519207, Train_MMSE: 0.149984, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:03:54] Epoch 285/300, Loss: 50.096554, Train_MMSE: 0.149988, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:05:27] Epoch 286/300, Loss: 50.564274, Train_MMSE: 0.149992, NMMSE: 0.144405, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:06:56] Epoch 287/300, Loss: 50.886524, Train_MMSE: 0.149988, NMMSE: 0.144401, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:08:24] Epoch 288/300, Loss: 50.133053, Train_MMSE: 0.149981, NMMSE: 0.1444, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:09:57] Epoch 289/300, Loss: 50.222889, Train_MMSE: 0.149978, NMMSE: 0.144408, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:11:27] Epoch 290/300, Loss: 49.929764, Train_MMSE: 0.149988, NMMSE: 0.144413, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:12:59] Epoch 291/300, Loss: 50.083363, Train_MMSE: 0.149992, NMMSE: 0.144412, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:14:30] Epoch 292/300, Loss: 50.350937, Train_MMSE: 0.149987, NMMSE: 0.144435, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:16:04] Epoch 293/300, Loss: 50.476562, Train_MMSE: 0.149983, NMMSE: 0.144418, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:17:35] Epoch 294/300, Loss: 50.318867, Train_MMSE: 0.149987, NMMSE: 0.144397, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:19:08] Epoch 295/300, Loss: 50.019165, Train_MMSE: 0.149989, NMMSE: 0.144415, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:20:42] Epoch 296/300, Loss: 50.237766, Train_MMSE: 0.149981, NMMSE: 0.14441, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:22:12] Epoch 297/300, Loss: 50.048679, Train_MMSE: 0.149984, NMMSE: 0.144408, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:23:44] Epoch 298/300, Loss: 50.088165, Train_MMSE: 0.149985, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:25:17] Epoch 299/300, Loss: 50.452320, Train_MMSE: 0.149988, NMMSE: 0.144446, LS_NMSE: 0.466596, Lr: 1.0000000000000002e-07
[2025-02-19 02:26:50] Epoch 300/300, Loss: 50.461636, Train_MMSE: 0.149984, NMMSE: 0.144399, LS_NMSE: 0.466596, Lr: 1.0000000000000004e-08
