H shape: (50000, 4, 36) (50000, 4, 36)
NMMSE of valid dataset:: 0.09186652170994043
num samples :: 500000
num valid: 50000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_train_Dataset_dB-25_N36_K4_L4_S12_Setup500_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_test_Dataset_dB-25_N36_K4_L4_S12_Setup50_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 256, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(480, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 3.09 MB
loss function:: L1Loss()
[2025-02-18 20:08:16] Epoch 1/300, Loss: 40.887806, Train_MMSE: 0.141982, NMMSE: 0.101196, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:10:41] Epoch 2/300, Loss: 40.713703, Train_MMSE: 0.101741, NMMSE: 0.09943, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:13:05] Epoch 3/300, Loss: 41.757069, Train_MMSE: 0.106427, NMMSE: 0.104046, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:15:29] Epoch 4/300, Loss: 41.434345, Train_MMSE: 0.103739, NMMSE: 0.105804, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:17:55] Epoch 5/300, Loss: 41.168640, Train_MMSE: 0.102765, NMMSE: 0.102294, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:20:19] Epoch 6/300, Loss: 40.851685, Train_MMSE: 0.102207, NMMSE: 0.104369, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:22:43] Epoch 7/300, Loss: 40.921680, Train_MMSE: 0.102012, NMMSE: 0.103853, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:25:11] Epoch 8/300, Loss: 40.882942, Train_MMSE: 0.101798, NMMSE: 0.10047, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:27:37] Epoch 9/300, Loss: 40.538139, Train_MMSE: 0.101619, NMMSE: 0.100921, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:30:02] Epoch 10/300, Loss: 40.988213, Train_MMSE: 0.101496, NMMSE: 0.100339, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:32:27] Epoch 11/300, Loss: 40.959431, Train_MMSE: 0.101389, NMMSE: 0.09943, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:34:51] Epoch 12/300, Loss: 40.894596, Train_MMSE: 0.101264, NMMSE: 0.1005, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:37:06] Epoch 13/300, Loss: 41.041008, Train_MMSE: 0.101198, NMMSE: 0.101193, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:39:28] Epoch 14/300, Loss: 41.071484, Train_MMSE: 0.101159, NMMSE: 0.100218, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:41:49] Epoch 15/300, Loss: 40.709885, Train_MMSE: 0.101172, NMMSE: 0.101815, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:43:27] Epoch 16/300, Loss: 40.810562, Train_MMSE: 0.101097, NMMSE: 0.102993, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:44:59] Epoch 17/300, Loss: 40.633541, Train_MMSE: 0.101108, NMMSE: 0.100497, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:46:34] Epoch 18/300, Loss: 40.704094, Train_MMSE: 0.101011, NMMSE: 0.101298, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:48:11] Epoch 19/300, Loss: 40.472412, Train_MMSE: 0.101041, NMMSE: 0.104237, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:49:49] Epoch 20/300, Loss: 41.292957, Train_MMSE: 0.100973, NMMSE: 0.103104, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:51:25] Epoch 21/300, Loss: 40.762074, Train_MMSE: 0.100918, NMMSE: 0.103141, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:53:05] Epoch 22/300, Loss: 40.974373, Train_MMSE: 0.100918, NMMSE: 0.101132, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:54:44] Epoch 23/300, Loss: 40.935303, Train_MMSE: 0.10104, NMMSE: 0.099437, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:56:23] Epoch 24/300, Loss: 40.668587, Train_MMSE: 0.100885, NMMSE: 0.100012, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:57:58] Epoch 25/300, Loss: 40.924175, Train_MMSE: 0.100865, NMMSE: 0.101173, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 20:59:36] Epoch 26/300, Loss: 40.763138, Train_MMSE: 0.100844, NMMSE: 0.101759, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:01:14] Epoch 27/300, Loss: 40.630825, Train_MMSE: 0.100859, NMMSE: 0.104419, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:02:43] Epoch 28/300, Loss: 40.879612, Train_MMSE: 0.100856, NMMSE: 0.102256, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:04:17] Epoch 29/300, Loss: 40.830330, Train_MMSE: 0.100779, NMMSE: 0.100251, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:05:50] Epoch 30/300, Loss: 40.900757, Train_MMSE: 0.100813, NMMSE: 0.100683, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:07:25] Epoch 31/300, Loss: 40.531384, Train_MMSE: 0.100791, NMMSE: 0.100496, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:08:59] Epoch 32/300, Loss: 40.595684, Train_MMSE: 0.101286, NMMSE: 0.129206, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:10:35] Epoch 33/300, Loss: 40.372192, Train_MMSE: 0.099405, NMMSE: 0.099378, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:12:11] Epoch 34/300, Loss: 40.491398, Train_MMSE: 0.099495, NMMSE: 0.105374, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:13:50] Epoch 35/300, Loss: 40.229202, Train_MMSE: 0.10001, NMMSE: 0.099496, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:15:23] Epoch 36/300, Loss: 40.434280, Train_MMSE: 0.099263, NMMSE: 0.102634, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:17:04] Epoch 37/300, Loss: 40.597797, Train_MMSE: 0.099299, NMMSE: 0.098399, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:18:40] Epoch 38/300, Loss: 40.684055, Train_MMSE: 0.099232, NMMSE: 0.100132, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:20:17] Epoch 39/300, Loss: 40.442730, Train_MMSE: 0.101613, NMMSE: 0.100988, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:21:52] Epoch 40/300, Loss: 40.602970, Train_MMSE: 0.099673, NMMSE: 0.099507, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:23:27] Epoch 41/300, Loss: 42.082073, Train_MMSE: 0.099945, NMMSE: 0.108212, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:25:07] Epoch 42/300, Loss: 40.278706, Train_MMSE: 0.100002, NMMSE: 0.10263, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:26:42] Epoch 43/300, Loss: 40.528397, Train_MMSE: 0.099124, NMMSE: 0.103378, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:28:20] Epoch 44/300, Loss: 40.353840, Train_MMSE: 0.099849, NMMSE: 0.102125, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:29:56] Epoch 45/300, Loss: 40.386250, Train_MMSE: 0.09912, NMMSE: 0.114772, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:31:32] Epoch 46/300, Loss: 40.713985, Train_MMSE: 0.101388, NMMSE: 0.102753, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:33:09] Epoch 47/300, Loss: 41.276447, Train_MMSE: 0.09953, NMMSE: 0.103196, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:34:45] Epoch 48/300, Loss: 40.963497, Train_MMSE: 0.099472, NMMSE: 0.151554, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:36:24] Epoch 49/300, Loss: 40.451874, Train_MMSE: 0.099159, NMMSE: 0.100904, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:38:01] Epoch 50/300, Loss: 40.478832, Train_MMSE: 0.099091, NMMSE: 0.097759, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:39:40] Epoch 51/300, Loss: 40.430580, Train_MMSE: 0.099095, NMMSE: 0.099037, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:41:22] Epoch 52/300, Loss: 40.669060, Train_MMSE: 0.099062, NMMSE: 0.101091, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:43:04] Epoch 53/300, Loss: 46.898487, Train_MMSE: 0.100324, NMMSE: 0.861944, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:44:43] Epoch 54/300, Loss: 40.580593, Train_MMSE: 0.100918, NMMSE: 0.098346, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:46:18] Epoch 55/300, Loss: 40.373703, Train_MMSE: 0.099469, NMMSE: 0.100954, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:47:52] Epoch 56/300, Loss: 40.620903, Train_MMSE: 0.099532, NMMSE: 0.155567, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:49:30] Epoch 57/300, Loss: 40.660492, Train_MMSE: 0.099158, NMMSE: 0.101838, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:51:06] Epoch 58/300, Loss: 40.914726, Train_MMSE: 0.099115, NMMSE: 0.102119, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:52:44] Epoch 59/300, Loss: 40.294647, Train_MMSE: 0.100075, NMMSE: 0.100378, LS_NMSE: 0.177624, Lr: 0.01
[2025-02-18 21:54:22] Epoch 60/300, Loss: 40.259205, Train_MMSE: 0.099018, NMMSE: 0.099826, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 21:56:00] Epoch 61/300, Loss: 39.859306, Train_MMSE: 0.096954, NMMSE: 0.094641, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 21:57:43] Epoch 62/300, Loss: 40.158314, Train_MMSE: 0.096838, NMMSE: 0.094362, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 21:59:21] Epoch 63/300, Loss: 39.805927, Train_MMSE: 0.09683, NMMSE: 0.094541, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:01:01] Epoch 64/300, Loss: 39.669018, Train_MMSE: 0.096806, NMMSE: 0.094398, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:02:39] Epoch 65/300, Loss: 39.928692, Train_MMSE: 0.096782, NMMSE: 0.09459, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:04:11] Epoch 66/300, Loss: 39.775993, Train_MMSE: 0.096767, NMMSE: 0.094347, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:05:47] Epoch 67/300, Loss: 40.024876, Train_MMSE: 0.096807, NMMSE: 0.094331, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:07:24] Epoch 68/300, Loss: 40.062191, Train_MMSE: 0.09675, NMMSE: 0.094278, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:08:55] Epoch 69/300, Loss: 40.020393, Train_MMSE: 0.096748, NMMSE: 0.094689, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:10:29] Epoch 70/300, Loss: 40.361057, Train_MMSE: 0.096745, NMMSE: 0.094377, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:12:05] Epoch 71/300, Loss: 39.844219, Train_MMSE: 0.096742, NMMSE: 0.094454, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:13:39] Epoch 72/300, Loss: 40.013298, Train_MMSE: 0.096734, NMMSE: 0.094469, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:15:14] Epoch 73/300, Loss: 40.047226, Train_MMSE: 0.096719, NMMSE: 0.09436, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:16:47] Epoch 74/300, Loss: 39.806690, Train_MMSE: 0.096927, NMMSE: 0.123705, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:18:20] Epoch 75/300, Loss: 39.717690, Train_MMSE: 0.096725, NMMSE: 0.094762, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:19:54] Epoch 76/300, Loss: 39.852661, Train_MMSE: 0.096716, NMMSE: 0.094289, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:21:22] Epoch 77/300, Loss: 39.739414, Train_MMSE: 0.096711, NMMSE: 0.09432, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:22:56] Epoch 78/300, Loss: 39.665752, Train_MMSE: 0.096702, NMMSE: 0.094185, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:24:30] Epoch 79/300, Loss: 39.693295, Train_MMSE: 0.096701, NMMSE: 0.094447, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:26:02] Epoch 80/300, Loss: 39.912804, Train_MMSE: 0.096816, NMMSE: 0.094537, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:27:34] Epoch 81/300, Loss: 40.016724, Train_MMSE: 0.096707, NMMSE: 0.094659, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:29:05] Epoch 82/300, Loss: 39.894951, Train_MMSE: 0.096703, NMMSE: 0.094176, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:30:41] Epoch 83/300, Loss: 39.715679, Train_MMSE: 0.0967, NMMSE: 0.095013, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:32:14] Epoch 84/300, Loss: 39.746902, Train_MMSE: 0.096691, NMMSE: 0.094647, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:33:47] Epoch 85/300, Loss: 40.021130, Train_MMSE: 0.096694, NMMSE: 0.094321, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:35:23] Epoch 86/300, Loss: 40.189995, Train_MMSE: 0.096682, NMMSE: 0.094222, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:36:57] Epoch 87/300, Loss: 39.815407, Train_MMSE: 0.096699, NMMSE: 0.094066, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:38:30] Epoch 88/300, Loss: 40.370995, Train_MMSE: 0.102321, NMMSE: 0.096168, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:39:59] Epoch 89/300, Loss: 39.875477, Train_MMSE: 0.096895, NMMSE: 0.095615, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:41:31] Epoch 90/300, Loss: 40.092602, Train_MMSE: 0.096786, NMMSE: 0.096985, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:43:02] Epoch 91/300, Loss: 39.910725, Train_MMSE: 0.096759, NMMSE: 0.136408, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:44:36] Epoch 92/300, Loss: 40.112484, Train_MMSE: 0.096742, NMMSE: 0.190809, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:46:09] Epoch 93/300, Loss: 39.876137, Train_MMSE: 0.096735, NMMSE: 0.108059, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:47:42] Epoch 94/300, Loss: 40.232910, Train_MMSE: 0.096733, NMMSE: 0.097509, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:49:16] Epoch 95/300, Loss: 39.712837, Train_MMSE: 0.096722, NMMSE: 0.09949, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:50:50] Epoch 96/300, Loss: 40.168327, Train_MMSE: 0.096718, NMMSE: 0.142773, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:52:26] Epoch 97/300, Loss: 40.238750, Train_MMSE: 0.09671, NMMSE: 0.114108, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:53:58] Epoch 98/300, Loss: 39.762123, Train_MMSE: 0.096712, NMMSE: 0.106929, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:55:27] Epoch 99/300, Loss: 40.036774, Train_MMSE: 0.096702, NMMSE: 0.097687, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:57:01] Epoch 100/300, Loss: 39.827385, Train_MMSE: 0.096687, NMMSE: 0.113969, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 22:58:37] Epoch 101/300, Loss: 39.862259, Train_MMSE: 0.096685, NMMSE: 0.094535, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:00:10] Epoch 102/300, Loss: 40.092243, Train_MMSE: 0.096711, NMMSE: 0.094644, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:01:41] Epoch 103/300, Loss: 40.015129, Train_MMSE: 0.096692, NMMSE: 0.098525, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:03:13] Epoch 104/300, Loss: 40.268780, Train_MMSE: 0.096681, NMMSE: 0.094524, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:04:49] Epoch 105/300, Loss: 39.846550, Train_MMSE: 0.096793, NMMSE: 0.102599, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:06:22] Epoch 106/300, Loss: 39.627991, Train_MMSE: 0.096698, NMMSE: 0.137702, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:07:54] Epoch 107/300, Loss: 39.689716, Train_MMSE: 0.096675, NMMSE: 0.097253, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:09:24] Epoch 108/300, Loss: 40.141434, Train_MMSE: 0.096665, NMMSE: 0.169191, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:10:55] Epoch 109/300, Loss: 39.958199, Train_MMSE: 0.096683, NMMSE: 0.098862, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:12:27] Epoch 110/300, Loss: 40.101185, Train_MMSE: 0.096672, NMMSE: 0.131006, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:14:00] Epoch 111/300, Loss: 39.728546, Train_MMSE: 0.096673, NMMSE: 0.128812, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:15:37] Epoch 112/300, Loss: 39.800125, Train_MMSE: 0.09668, NMMSE: 0.094802, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:17:08] Epoch 113/300, Loss: 40.001335, Train_MMSE: 0.096666, NMMSE: 0.094709, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:18:41] Epoch 114/300, Loss: 39.800320, Train_MMSE: 0.096677, NMMSE: 0.094421, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:20:10] Epoch 115/300, Loss: 40.136562, Train_MMSE: 0.09667, NMMSE: 0.098234, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:21:44] Epoch 116/300, Loss: 39.684578, Train_MMSE: 0.096664, NMMSE: 0.094798, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:23:14] Epoch 117/300, Loss: 39.991413, Train_MMSE: 0.096662, NMMSE: 0.094283, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:24:48] Epoch 118/300, Loss: 40.109180, Train_MMSE: 0.096672, NMMSE: 0.094097, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:26:22] Epoch 119/300, Loss: 39.937946, Train_MMSE: 0.096661, NMMSE: 0.095513, LS_NMSE: 0.177624, Lr: 0.001
[2025-02-18 23:27:52] Epoch 120/300, Loss: 40.140247, Train_MMSE: 0.09666, NMMSE: 0.094524, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:29:24] Epoch 121/300, Loss: 39.645264, Train_MMSE: 0.096112, NMMSE: 0.093393, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:31:01] Epoch 122/300, Loss: 39.824490, Train_MMSE: 0.096074, NMMSE: 0.093381, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:32:35] Epoch 123/300, Loss: 39.983257, Train_MMSE: 0.096083, NMMSE: 0.093383, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:34:09] Epoch 124/300, Loss: 39.685989, Train_MMSE: 0.096054, NMMSE: 0.093377, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:35:39] Epoch 125/300, Loss: 39.751717, Train_MMSE: 0.096057, NMMSE: 0.093324, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:37:11] Epoch 126/300, Loss: 39.575054, Train_MMSE: 0.096058, NMMSE: 0.093326, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:38:47] Epoch 127/300, Loss: 39.749092, Train_MMSE: 0.09604, NMMSE: 0.093336, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:40:22] Epoch 128/300, Loss: 39.711086, Train_MMSE: 0.096035, NMMSE: 0.09333, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:41:55] Epoch 129/300, Loss: 39.607353, Train_MMSE: 0.096044, NMMSE: 0.093354, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:43:28] Epoch 130/300, Loss: 39.683895, Train_MMSE: 0.096033, NMMSE: 0.093328, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:44:59] Epoch 131/300, Loss: 39.692661, Train_MMSE: 0.096028, NMMSE: 0.093312, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:46:38] Epoch 132/300, Loss: 39.798347, Train_MMSE: 0.096031, NMMSE: 0.093371, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:48:12] Epoch 133/300, Loss: 39.447891, Train_MMSE: 0.096023, NMMSE: 0.093329, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:49:46] Epoch 134/300, Loss: 39.618309, Train_MMSE: 0.096014, NMMSE: 0.093286, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:51:18] Epoch 135/300, Loss: 39.419624, Train_MMSE: 0.09601, NMMSE: 0.093306, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:52:50] Epoch 136/300, Loss: 40.001209, Train_MMSE: 0.096009, NMMSE: 0.093324, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:54:25] Epoch 137/300, Loss: 39.844387, Train_MMSE: 0.09599, NMMSE: 0.093316, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:55:58] Epoch 138/300, Loss: 39.796772, Train_MMSE: 0.096007, NMMSE: 0.093312, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:57:28] Epoch 139/300, Loss: 39.827892, Train_MMSE: 0.096011, NMMSE: 0.09335, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-18 23:59:01] Epoch 140/300, Loss: 39.879749, Train_MMSE: 0.095997, NMMSE: 0.093333, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:00:34] Epoch 141/300, Loss: 39.781460, Train_MMSE: 0.095998, NMMSE: 0.093329, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:02:09] Epoch 142/300, Loss: 40.078934, Train_MMSE: 0.095997, NMMSE: 0.093307, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:03:43] Epoch 143/300, Loss: 39.443600, Train_MMSE: 0.095993, NMMSE: 0.093276, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:05:13] Epoch 144/300, Loss: 39.758301, Train_MMSE: 0.095987, NMMSE: 0.09332, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:06:43] Epoch 145/300, Loss: 39.739681, Train_MMSE: 0.095993, NMMSE: 0.093336, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:08:16] Epoch 146/300, Loss: 39.877522, Train_MMSE: 0.095989, NMMSE: 0.093327, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:09:53] Epoch 147/300, Loss: 40.079693, Train_MMSE: 0.095985, NMMSE: 0.093278, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:11:26] Epoch 148/300, Loss: 40.095684, Train_MMSE: 0.095989, NMMSE: 0.093325, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:12:57] Epoch 149/300, Loss: 39.616917, Train_MMSE: 0.095977, NMMSE: 0.09326, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:14:32] Epoch 150/300, Loss: 39.974480, Train_MMSE: 0.095985, NMMSE: 0.093279, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:16:04] Epoch 151/300, Loss: 39.640930, Train_MMSE: 0.095981, NMMSE: 0.093271, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:17:37] Epoch 152/300, Loss: 39.818569, Train_MMSE: 0.095968, NMMSE: 0.093316, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:19:12] Epoch 153/300, Loss: 39.570290, Train_MMSE: 0.095978, NMMSE: 0.093334, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:20:44] Epoch 154/300, Loss: 39.846272, Train_MMSE: 0.095969, NMMSE: 0.093307, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:22:19] Epoch 155/300, Loss: 39.755665, Train_MMSE: 0.095974, NMMSE: 0.093294, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:23:55] Epoch 156/300, Loss: 39.653709, Train_MMSE: 0.095958, NMMSE: 0.093283, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:25:27] Epoch 157/300, Loss: 39.523506, Train_MMSE: 0.095978, NMMSE: 0.093293, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:27:00] Epoch 158/300, Loss: 39.589832, Train_MMSE: 0.095961, NMMSE: 0.093278, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:28:32] Epoch 159/300, Loss: 39.878529, Train_MMSE: 0.095963, NMMSE: 0.093262, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:30:03] Epoch 160/300, Loss: 39.602306, Train_MMSE: 0.095971, NMMSE: 0.093268, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:31:34] Epoch 161/300, Loss: 39.783901, Train_MMSE: 0.09596, NMMSE: 0.093307, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:33:10] Epoch 162/300, Loss: 39.634209, Train_MMSE: 0.095959, NMMSE: 0.093339, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:34:45] Epoch 163/300, Loss: 39.875790, Train_MMSE: 0.095957, NMMSE: 0.093306, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:36:20] Epoch 164/300, Loss: 39.707832, Train_MMSE: 0.095974, NMMSE: 0.093351, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:37:53] Epoch 165/300, Loss: 39.665833, Train_MMSE: 0.095967, NMMSE: 0.093304, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:39:27] Epoch 166/300, Loss: 40.176987, Train_MMSE: 0.095961, NMMSE: 0.093291, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:41:02] Epoch 167/300, Loss: 40.008053, Train_MMSE: 0.095947, NMMSE: 0.093227, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:42:35] Epoch 168/300, Loss: 39.803223, Train_MMSE: 0.095964, NMMSE: 0.093288, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:44:11] Epoch 169/300, Loss: 39.612480, Train_MMSE: 0.095955, NMMSE: 0.093262, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:45:45] Epoch 170/300, Loss: 40.052376, Train_MMSE: 0.095948, NMMSE: 0.093288, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:47:19] Epoch 171/300, Loss: 39.683662, Train_MMSE: 0.095944, NMMSE: 0.093211, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:48:53] Epoch 172/300, Loss: 39.565701, Train_MMSE: 0.095949, NMMSE: 0.093277, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:50:31] Epoch 173/300, Loss: 39.703316, Train_MMSE: 0.095946, NMMSE: 0.093251, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:52:08] Epoch 174/300, Loss: 39.723248, Train_MMSE: 0.095948, NMMSE: 0.093306, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:53:37] Epoch 175/300, Loss: 40.070415, Train_MMSE: 0.095943, NMMSE: 0.093393, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:55:11] Epoch 176/300, Loss: 39.743694, Train_MMSE: 0.095951, NMMSE: 0.093276, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:56:44] Epoch 177/300, Loss: 39.787098, Train_MMSE: 0.095951, NMMSE: 0.093317, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:58:16] Epoch 178/300, Loss: 39.477039, Train_MMSE: 0.095947, NMMSE: 0.093292, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 00:59:56] Epoch 179/300, Loss: 39.956890, Train_MMSE: 0.09594, NMMSE: 0.094936, LS_NMSE: 0.177624, Lr: 0.0001
[2025-02-19 01:01:29] Epoch 180/300, Loss: 39.874592, Train_MMSE: 0.09594, NMMSE: 0.093257, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:03:04] Epoch 181/300, Loss: 39.481647, Train_MMSE: 0.095841, NMMSE: 0.093124, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:04:41] Epoch 182/300, Loss: 39.718880, Train_MMSE: 0.095839, NMMSE: 0.093122, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:06:20] Epoch 183/300, Loss: 39.704784, Train_MMSE: 0.095838, NMMSE: 0.09313, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:07:56] Epoch 184/300, Loss: 39.719517, Train_MMSE: 0.095834, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:09:33] Epoch 185/300, Loss: 39.624645, Train_MMSE: 0.09584, NMMSE: 0.093121, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:11:06] Epoch 186/300, Loss: 39.486622, Train_MMSE: 0.095836, NMMSE: 0.093125, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:12:41] Epoch 187/300, Loss: 39.680168, Train_MMSE: 0.095838, NMMSE: 0.093117, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:14:23] Epoch 188/300, Loss: 39.729115, Train_MMSE: 0.095827, NMMSE: 0.093129, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:16:02] Epoch 189/300, Loss: 39.629978, Train_MMSE: 0.095842, NMMSE: 0.093117, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:17:39] Epoch 190/300, Loss: 39.939747, Train_MMSE: 0.095833, NMMSE: 0.09313, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:19:15] Epoch 191/300, Loss: 39.441792, Train_MMSE: 0.09584, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:20:53] Epoch 192/300, Loss: 39.736172, Train_MMSE: 0.095834, NMMSE: 0.093115, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:22:29] Epoch 193/300, Loss: 39.642063, Train_MMSE: 0.095843, NMMSE: 0.093125, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:24:04] Epoch 194/300, Loss: 39.715885, Train_MMSE: 0.095836, NMMSE: 0.093117, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:25:41] Epoch 195/300, Loss: 39.617126, Train_MMSE: 0.095832, NMMSE: 0.093115, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:27:17] Epoch 196/300, Loss: 39.702259, Train_MMSE: 0.095832, NMMSE: 0.093112, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:28:53] Epoch 197/300, Loss: 39.827538, Train_MMSE: 0.095831, NMMSE: 0.093124, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:30:27] Epoch 198/300, Loss: 40.292236, Train_MMSE: 0.095836, NMMSE: 0.093112, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:32:04] Epoch 199/300, Loss: 39.978981, Train_MMSE: 0.09583, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:33:39] Epoch 200/300, Loss: 39.807316, Train_MMSE: 0.095831, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:35:16] Epoch 201/300, Loss: 39.576633, Train_MMSE: 0.095827, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:36:51] Epoch 202/300, Loss: 39.638668, Train_MMSE: 0.095832, NMMSE: 0.093118, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:38:23] Epoch 203/300, Loss: 39.554317, Train_MMSE: 0.095835, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:40:01] Epoch 204/300, Loss: 39.816936, Train_MMSE: 0.095825, NMMSE: 0.093115, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:41:36] Epoch 205/300, Loss: 39.695526, Train_MMSE: 0.095825, NMMSE: 0.093109, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:43:10] Epoch 206/300, Loss: 39.846817, Train_MMSE: 0.095831, NMMSE: 0.093119, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:44:41] Epoch 207/300, Loss: 39.548599, Train_MMSE: 0.095829, NMMSE: 0.093114, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:46:18] Epoch 208/300, Loss: 39.811722, Train_MMSE: 0.09584, NMMSE: 0.093112, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:47:55] Epoch 209/300, Loss: 39.602146, Train_MMSE: 0.095832, NMMSE: 0.093126, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:49:27] Epoch 210/300, Loss: 39.600178, Train_MMSE: 0.095823, NMMSE: 0.09311, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:51:02] Epoch 211/300, Loss: 39.788235, Train_MMSE: 0.095825, NMMSE: 0.09311, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:52:37] Epoch 212/300, Loss: 39.812580, Train_MMSE: 0.095831, NMMSE: 0.093122, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:54:11] Epoch 213/300, Loss: 39.755241, Train_MMSE: 0.095825, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:55:46] Epoch 214/300, Loss: 39.621628, Train_MMSE: 0.095827, NMMSE: 0.093121, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:57:20] Epoch 215/300, Loss: 39.721874, Train_MMSE: 0.095835, NMMSE: 0.09312, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 01:58:55] Epoch 216/300, Loss: 40.062672, Train_MMSE: 0.095825, NMMSE: 0.093126, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:00:34] Epoch 217/300, Loss: 39.295929, Train_MMSE: 0.095823, NMMSE: 0.09312, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:02:08] Epoch 218/300, Loss: 39.583454, Train_MMSE: 0.095823, NMMSE: 0.093129, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:03:42] Epoch 219/300, Loss: 39.681053, Train_MMSE: 0.095834, NMMSE: 0.093111, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:05:16] Epoch 220/300, Loss: 40.072220, Train_MMSE: 0.095826, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:06:48] Epoch 221/300, Loss: 39.673725, Train_MMSE: 0.095831, NMMSE: 0.09311, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:08:21] Epoch 222/300, Loss: 39.957939, Train_MMSE: 0.095824, NMMSE: 0.093126, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:09:56] Epoch 223/300, Loss: 39.895126, Train_MMSE: 0.095822, NMMSE: 0.093107, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:11:28] Epoch 224/300, Loss: 39.454529, Train_MMSE: 0.095828, NMMSE: 0.093104, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:12:59] Epoch 225/300, Loss: 39.501595, Train_MMSE: 0.095824, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:14:34] Epoch 226/300, Loss: 39.671326, Train_MMSE: 0.095829, NMMSE: 0.093104, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:16:07] Epoch 227/300, Loss: 39.841599, Train_MMSE: 0.095829, NMMSE: 0.093114, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:17:42] Epoch 228/300, Loss: 40.110073, Train_MMSE: 0.095828, NMMSE: 0.093125, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:19:13] Epoch 229/300, Loss: 39.957298, Train_MMSE: 0.095831, NMMSE: 0.09311, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:20:48] Epoch 230/300, Loss: 39.717670, Train_MMSE: 0.095829, NMMSE: 0.093119, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:22:24] Epoch 231/300, Loss: 39.654102, Train_MMSE: 0.095815, NMMSE: 0.09311, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:24:00] Epoch 232/300, Loss: 40.162758, Train_MMSE: 0.095817, NMMSE: 0.093103, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:25:34] Epoch 233/300, Loss: 39.633709, Train_MMSE: 0.09582, NMMSE: 0.093108, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:26:58] Epoch 234/300, Loss: 40.001022, Train_MMSE: 0.095825, NMMSE: 0.093121, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:27:44] Epoch 235/300, Loss: 39.871643, Train_MMSE: 0.09582, NMMSE: 0.093116, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:28:30] Epoch 236/300, Loss: 39.690861, Train_MMSE: 0.095818, NMMSE: 0.093124, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:29:15] Epoch 237/300, Loss: 39.428425, Train_MMSE: 0.095825, NMMSE: 0.093104, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:30:01] Epoch 238/300, Loss: 39.559753, Train_MMSE: 0.095814, NMMSE: 0.093106, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:30:47] Epoch 239/300, Loss: 39.667549, Train_MMSE: 0.095813, NMMSE: 0.093146, LS_NMSE: 0.177624, Lr: 1e-05
[2025-02-19 02:31:32] Epoch 240/300, Loss: 39.899689, Train_MMSE: 0.095816, NMMSE: 0.093111, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:32:18] Epoch 241/300, Loss: 39.764111, Train_MMSE: 0.095797, NMMSE: 0.093106, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:33:05] Epoch 242/300, Loss: 39.484669, Train_MMSE: 0.095803, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:33:51] Epoch 243/300, Loss: 39.503963, Train_MMSE: 0.095801, NMMSE: 0.093107, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:34:37] Epoch 244/300, Loss: 39.811871, Train_MMSE: 0.095815, NMMSE: 0.093104, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:35:23] Epoch 245/300, Loss: 40.139820, Train_MMSE: 0.095808, NMMSE: 0.093142, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:36:09] Epoch 246/300, Loss: 39.866123, Train_MMSE: 0.09581, NMMSE: 0.093102, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:36:55] Epoch 247/300, Loss: 39.681362, Train_MMSE: 0.095805, NMMSE: 0.093104, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:37:40] Epoch 248/300, Loss: 39.718494, Train_MMSE: 0.095797, NMMSE: 0.093098, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:38:26] Epoch 249/300, Loss: 39.574368, Train_MMSE: 0.095805, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:39:12] Epoch 250/300, Loss: 39.516968, Train_MMSE: 0.095808, NMMSE: 0.093096, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:39:59] Epoch 251/300, Loss: 39.643391, Train_MMSE: 0.095801, NMMSE: 0.093108, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:40:45] Epoch 252/300, Loss: 39.885963, Train_MMSE: 0.095808, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:41:31] Epoch 253/300, Loss: 39.694218, Train_MMSE: 0.095806, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:42:17] Epoch 254/300, Loss: 39.917362, Train_MMSE: 0.095807, NMMSE: 0.093102, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:43:03] Epoch 255/300, Loss: 39.856533, Train_MMSE: 0.09581, NMMSE: 0.093096, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:43:49] Epoch 256/300, Loss: 39.622223, Train_MMSE: 0.095803, NMMSE: 0.093106, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:44:36] Epoch 257/300, Loss: 39.960701, Train_MMSE: 0.095802, NMMSE: 0.093105, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:45:22] Epoch 258/300, Loss: 39.621914, Train_MMSE: 0.09581, NMMSE: 0.093098, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:46:08] Epoch 259/300, Loss: 39.741226, Train_MMSE: 0.0958, NMMSE: 0.093095, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:46:55] Epoch 260/300, Loss: 39.629204, Train_MMSE: 0.095805, NMMSE: 0.09309, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:47:41] Epoch 261/300, Loss: 39.697235, Train_MMSE: 0.095805, NMMSE: 0.093109, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:48:27] Epoch 262/300, Loss: 39.589447, Train_MMSE: 0.095804, NMMSE: 0.093092, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:49:13] Epoch 263/300, Loss: 39.593067, Train_MMSE: 0.095817, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:49:59] Epoch 264/300, Loss: 39.624992, Train_MMSE: 0.095809, NMMSE: 0.093142, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:50:45] Epoch 265/300, Loss: 39.765198, Train_MMSE: 0.095802, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:51:30] Epoch 266/300, Loss: 39.614296, Train_MMSE: 0.095808, NMMSE: 0.093098, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:52:15] Epoch 267/300, Loss: 39.873463, Train_MMSE: 0.095818, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:53:00] Epoch 268/300, Loss: 39.637241, Train_MMSE: 0.095804, NMMSE: 0.093129, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:53:45] Epoch 269/300, Loss: 39.628208, Train_MMSE: 0.095806, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:54:29] Epoch 270/300, Loss: 39.873161, Train_MMSE: 0.095793, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:55:14] Epoch 271/300, Loss: 39.832695, Train_MMSE: 0.095808, NMMSE: 0.093106, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:55:59] Epoch 272/300, Loss: 40.054241, Train_MMSE: 0.095813, NMMSE: 0.093095, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:56:44] Epoch 273/300, Loss: 39.671509, Train_MMSE: 0.09581, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:57:29] Epoch 274/300, Loss: 39.625507, Train_MMSE: 0.095809, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:58:13] Epoch 275/300, Loss: 39.848598, Train_MMSE: 0.095807, NMMSE: 0.093102, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:58:58] Epoch 276/300, Loss: 39.754082, Train_MMSE: 0.095807, NMMSE: 0.09309, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 02:59:43] Epoch 277/300, Loss: 39.839310, Train_MMSE: 0.095809, NMMSE: 0.093092, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:00:27] Epoch 278/300, Loss: 39.705559, Train_MMSE: 0.095806, NMMSE: 0.093096, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:01:12] Epoch 279/300, Loss: 39.413319, Train_MMSE: 0.095807, NMMSE: 0.093105, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:01:57] Epoch 280/300, Loss: 39.649212, Train_MMSE: 0.095795, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:02:42] Epoch 281/300, Loss: 39.532207, Train_MMSE: 0.095803, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:03:27] Epoch 282/300, Loss: 39.606548, Train_MMSE: 0.095811, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:04:11] Epoch 283/300, Loss: 39.794895, Train_MMSE: 0.095802, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:04:56] Epoch 284/300, Loss: 39.790920, Train_MMSE: 0.095809, NMMSE: 0.093095, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:05:41] Epoch 285/300, Loss: 39.816410, Train_MMSE: 0.095806, NMMSE: 0.093102, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:06:26] Epoch 286/300, Loss: 40.050129, Train_MMSE: 0.095803, NMMSE: 0.093113, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:07:11] Epoch 287/300, Loss: 39.575020, Train_MMSE: 0.095812, NMMSE: 0.093095, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:07:56] Epoch 288/300, Loss: 39.937149, Train_MMSE: 0.0958, NMMSE: 0.093103, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:08:55] Epoch 289/300, Loss: 39.472225, Train_MMSE: 0.095806, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:09:39] Epoch 290/300, Loss: 39.758942, Train_MMSE: 0.095798, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:10:24] Epoch 291/300, Loss: 39.879436, Train_MMSE: 0.095805, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:11:09] Epoch 292/300, Loss: 39.764011, Train_MMSE: 0.095803, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:11:55] Epoch 293/300, Loss: 39.901703, Train_MMSE: 0.095801, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:12:39] Epoch 294/300, Loss: 39.690414, Train_MMSE: 0.095804, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:13:24] Epoch 295/300, Loss: 39.688862, Train_MMSE: 0.095796, NMMSE: 0.093106, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:14:09] Epoch 296/300, Loss: 39.502338, Train_MMSE: 0.095802, NMMSE: 0.093091, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:14:55] Epoch 297/300, Loss: 39.733276, Train_MMSE: 0.095804, NMMSE: 0.093097, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:15:40] Epoch 298/300, Loss: 39.901161, Train_MMSE: 0.095808, NMMSE: 0.093102, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:16:25] Epoch 299/300, Loss: 39.755299, Train_MMSE: 0.095807, NMMSE: 0.093094, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-06
[2025-02-19 03:17:11] Epoch 300/300, Loss: 39.501881, Train_MMSE: 0.095803, NMMSE: 0.093093, LS_NMSE: 0.177624, Lr: 1.0000000000000002e-07
