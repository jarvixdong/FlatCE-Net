H shape: (50000, 4, 36) (50000, 4, 36)
NMMSE of valid dataset:: 0.10944907202708025
num samples :: 500000
num valid: 50000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_train_Dataset_dB-25_N36_K4_L4_S11_Setup500_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/v1_test_Dataset_dB-25_N36_K4_L4_S11_Setup50_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}}
model:: DiaUNet1D(
  (encoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(2, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (pools): ModuleList(
    (0-2): 3 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (bottleneck): BasicUnetBlock(
    (blocks): ModuleList(
      (0): Sequential(
        (0): Stdconv1D(
          (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (1): DynamicDilatedConv(
        (layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (1): Sequential(
            (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (2): Sequential(
            (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
            (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
          (3): Sequential(
            (0): Conv1d(32, 256, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
            (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
          )
        )
        (projection): Conv1d(480, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (upconvs): ModuleList(
    (0): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
    (1): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))
    (2): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
  )
  (decoders): ModuleList(
    (0): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(16, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(240, 128, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(120, 64, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BasicUnetBlock(
      (blocks): ModuleList(
        (0): Sequential(
          (0): Stdconv1D(
            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))
            (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU(inplace=True)
          )
        )
        (1): DynamicDilatedConv(
          (layers): ModuleList(
            (0): Sequential(
              (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))
              (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (1): Sequential(
              (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
              (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (2): Sequential(
              (0): Conv1d(8, 4, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
              (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
            (3): Sequential(
              (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))
              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
            )
          )
          (projection): Conv1d(60, 32, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (final_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
)
Estimated model size: 3.09 MB
loss function:: L1Loss()
[2025-02-18 18:53:19] Epoch 1/300, Loss: 47.146374, Train_MMSE: 0.174702, NMMSE: 0.129248, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 18:55:06] Epoch 2/300, Loss: 46.323147, Train_MMSE: 0.130593, NMMSE: 0.12801, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 18:56:57] Epoch 3/300, Loss: 46.389637, Train_MMSE: 0.128588, NMMSE: 0.122772, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 18:58:45] Epoch 4/300, Loss: 46.712227, Train_MMSE: 0.128334, NMMSE: 0.126439, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:00:32] Epoch 5/300, Loss: 46.664261, Train_MMSE: 0.132563, NMMSE: 0.127124, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:02:16] Epoch 6/300, Loss: 46.165695, Train_MMSE: 0.12824, NMMSE: 0.125453, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:04:03] Epoch 7/300, Loss: 46.295280, Train_MMSE: 0.128277, NMMSE: 0.145159, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:05:51] Epoch 8/300, Loss: 46.167660, Train_MMSE: 0.128218, NMMSE: 0.123687, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:07:38] Epoch 9/300, Loss: 46.335022, Train_MMSE: 0.126756, NMMSE: 0.142406, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:09:25] Epoch 10/300, Loss: 46.190250, Train_MMSE: 0.126634, NMMSE: 0.122537, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:11:14] Epoch 11/300, Loss: 46.251221, Train_MMSE: 0.127517, NMMSE: 0.123157, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:13:12] Epoch 12/300, Loss: 46.164703, Train_MMSE: 0.127509, NMMSE: 0.128189, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:15:29] Epoch 13/300, Loss: 46.167164, Train_MMSE: 0.12637, NMMSE: 0.122352, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:17:42] Epoch 14/300, Loss: 46.476387, Train_MMSE: 0.126264, NMMSE: 0.121178, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:20:09] Epoch 15/300, Loss: 46.077374, Train_MMSE: 0.126261, NMMSE: 0.124032, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:22:35] Epoch 16/300, Loss: 45.797771, Train_MMSE: 0.1277, NMMSE: 0.121114, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:25:04] Epoch 17/300, Loss: 45.996704, Train_MMSE: 0.126315, NMMSE: 0.124622, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:27:31] Epoch 18/300, Loss: 46.652584, Train_MMSE: 0.12717, NMMSE: 0.123875, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:29:56] Epoch 19/300, Loss: 46.031158, Train_MMSE: 0.126222, NMMSE: 0.122462, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:32:23] Epoch 20/300, Loss: 46.334278, Train_MMSE: 0.126855, NMMSE: 0.127516, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:34:46] Epoch 21/300, Loss: 46.145229, Train_MMSE: 0.127286, NMMSE: 0.124217, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:37:12] Epoch 22/300, Loss: 45.935509, Train_MMSE: 0.126849, NMMSE: 0.175014, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:39:36] Epoch 23/300, Loss: 45.903816, Train_MMSE: 0.126145, NMMSE: 0.124196, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:42:02] Epoch 24/300, Loss: 46.279331, Train_MMSE: 0.128066, NMMSE: 0.12145, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:44:29] Epoch 25/300, Loss: 45.739449, Train_MMSE: 0.126058, NMMSE: 0.12312, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:46:54] Epoch 26/300, Loss: 45.799870, Train_MMSE: 0.127551, NMMSE: 0.127861, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:49:20] Epoch 27/300, Loss: 45.680267, Train_MMSE: 0.125979, NMMSE: 0.121558, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:51:45] Epoch 28/300, Loss: 46.486877, Train_MMSE: 0.125971, NMMSE: 0.124168, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:54:10] Epoch 29/300, Loss: 45.796803, Train_MMSE: 0.125898, NMMSE: 0.126575, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:56:37] Epoch 30/300, Loss: 45.894447, Train_MMSE: 0.127061, NMMSE: 0.126518, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 19:59:03] Epoch 31/300, Loss: 46.295216, Train_MMSE: 0.125817, NMMSE: 0.12186, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:01:41] Epoch 32/300, Loss: 46.203239, Train_MMSE: 0.125913, NMMSE: 0.123499, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:04:24] Epoch 33/300, Loss: 45.970013, Train_MMSE: 0.125845, NMMSE: 0.123587, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:07:06] Epoch 34/300, Loss: 46.180733, Train_MMSE: 0.125851, NMMSE: 0.126358, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:09:50] Epoch 35/300, Loss: 46.253681, Train_MMSE: 0.12747, NMMSE: 0.12554, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:12:35] Epoch 36/300, Loss: 46.159237, Train_MMSE: 0.126078, NMMSE: 0.133682, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:15:19] Epoch 37/300, Loss: 45.586617, Train_MMSE: 0.126051, NMMSE: 0.122169, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:18:02] Epoch 38/300, Loss: 46.066536, Train_MMSE: 0.127606, NMMSE: 0.128825, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:20:49] Epoch 39/300, Loss: 45.642540, Train_MMSE: 0.125873, NMMSE: 0.123934, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:23:32] Epoch 40/300, Loss: 45.940113, Train_MMSE: 0.12591, NMMSE: 0.14691, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:26:14] Epoch 41/300, Loss: 45.773273, Train_MMSE: 0.126312, NMMSE: 0.291671, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:28:54] Epoch 42/300, Loss: 45.694042, Train_MMSE: 0.125902, NMMSE: 0.137129, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:31:40] Epoch 43/300, Loss: 45.933075, Train_MMSE: 0.12645, NMMSE: 0.150323, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:34:20] Epoch 44/300, Loss: 45.787014, Train_MMSE: 0.125683, NMMSE: 0.136509, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:37:00] Epoch 45/300, Loss: 45.805790, Train_MMSE: 0.12609, NMMSE: 0.125658, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:39:35] Epoch 46/300, Loss: 45.645851, Train_MMSE: 0.125695, NMMSE: 0.128972, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:42:15] Epoch 47/300, Loss: 45.738007, Train_MMSE: 0.125696, NMMSE: 0.148682, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:45:00] Epoch 48/300, Loss: 46.182755, Train_MMSE: 0.134377, NMMSE: 0.123871, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:47:46] Epoch 49/300, Loss: 46.110550, Train_MMSE: 0.126542, NMMSE: 0.125502, LS_NMSE: 0.27073, Lr: 0.01
[2025-02-18 20:50:33] Epoch 50/300, Loss: 45.958603, Train_MMSE: 0.126281, NMMSE: 0.134575, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 20:53:15] Epoch 51/300, Loss: 45.344727, Train_MMSE: 0.122768, NMMSE: 0.117035, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 20:56:05] Epoch 52/300, Loss: 45.261116, Train_MMSE: 0.122595, NMMSE: 0.11717, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 20:58:53] Epoch 53/300, Loss: 45.172577, Train_MMSE: 0.122536, NMMSE: 0.11672, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:01:38] Epoch 54/300, Loss: 45.801952, Train_MMSE: 0.122488, NMMSE: 0.116701, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:04:26] Epoch 55/300, Loss: 45.472458, Train_MMSE: 0.122444, NMMSE: 0.11644, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:07:14] Epoch 56/300, Loss: 45.375126, Train_MMSE: 0.122395, NMMSE: 0.116603, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:10:05] Epoch 57/300, Loss: 45.174809, Train_MMSE: 0.122388, NMMSE: 0.116344, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:13:03] Epoch 58/300, Loss: 45.259232, Train_MMSE: 0.122367, NMMSE: 0.118074, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:15:52] Epoch 59/300, Loss: 45.159046, Train_MMSE: 0.122334, NMMSE: 0.116376, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:18:49] Epoch 60/300, Loss: 45.584526, Train_MMSE: 0.122343, NMMSE: 0.119263, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:21:39] Epoch 61/300, Loss: 45.444889, Train_MMSE: 0.122316, NMMSE: 0.116766, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:24:36] Epoch 62/300, Loss: 44.947670, Train_MMSE: 0.122302, NMMSE: 0.116569, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:27:26] Epoch 63/300, Loss: 45.112854, Train_MMSE: 0.122288, NMMSE: 0.116432, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:30:23] Epoch 64/300, Loss: 45.485809, Train_MMSE: 0.122278, NMMSE: 0.116733, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:33:22] Epoch 65/300, Loss: 45.387745, Train_MMSE: 0.122267, NMMSE: 0.116443, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:36:23] Epoch 66/300, Loss: 45.425625, Train_MMSE: 0.122281, NMMSE: 0.119997, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:39:18] Epoch 67/300, Loss: 45.232437, Train_MMSE: 0.122243, NMMSE: 0.117197, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:42:13] Epoch 68/300, Loss: 45.212955, Train_MMSE: 0.122245, NMMSE: 0.117355, LS_NMSE: 0.27073, Lr: 0.001
[2025-02-18 21:45:04] Epoch 69/300, Loss: 45.726608, Train_MMSE: 0.122222, NMMSE: 0.116146, LS_NMSE: 0.27073, Lr: 0.001
