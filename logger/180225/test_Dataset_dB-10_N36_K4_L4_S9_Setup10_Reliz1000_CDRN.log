H shape: (10000, 4, 36) (10000, 4, 36)
NMMSE of valid dataset:: 0.024458686477191807
num samples :: 100000
num valid: 10000
config_path: conf/config_multisetup.yml
cfg: {'seed': 10, 'dataset': {'train_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/train_Dataset_dB-10_N36_K4_L4_S9_Setup100_Reliz1000.mat', 'valid_path': '/mnt/parscratch/users/elq20xd/channel_estimation/cc_data/dataset10_N36K4_3types/test_Dataset_dB-10_N36_K4_L4_S9_Setup10_Reliz1000.mat', 'with_Vpinv': True}, 'dataloader': {'shuffle': True, 'batch_size': 512, 'num_workers': 1}, 'model': {'channel_index': 32, 'num_layers': 3}, 'logger': {'path': None}}
model:: DnCNN_MultiBlock_ds(
  (layers): ModuleList(
    (0-2): 3 x Sequential(
      (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
      (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (11): ReLU(inplace=True)
      (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (14): ReLU(inplace=True)
      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (17): ReLU(inplace=True)
      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (20): ReLU(inplace=True)
      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (23): ReLU(inplace=True)
      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (26): ReLU(inplace=True)
      (27): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (28): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (29): ReLU(inplace=True)
      (30): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (31): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (32): ReLU(inplace=True)
      (33): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (34): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (35): ReLU(inplace=True)
      (36): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (37): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (38): ReLU(inplace=True)
      (39): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (40): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (41): ReLU(inplace=True)
      (42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (43): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (44): ReLU(inplace=True)
      (45): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
Estimated model size: 5.95 MB
loss function:: L1Loss()
[2025-02-18 13:30:07] Epoch 1/50, Loss: 26.924301, Train_MMSE: 0.044128, NMMSE: 0.036784, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:31:17] Epoch 2/50, Loss: 25.859488, Train_MMSE: 0.041224, NMMSE: 0.034789, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:32:31] Epoch 3/50, Loss: 25.367748, Train_MMSE: 0.038832, NMMSE: 0.03341, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:33:57] Epoch 4/50, Loss: 24.923105, Train_MMSE: 0.037445, NMMSE: 0.032678, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:35:24] Epoch 5/50, Loss: 24.647552, Train_MMSE: 0.03664, NMMSE: 0.032163, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:36:52] Epoch 6/50, Loss: 24.496477, Train_MMSE: 0.036068, NMMSE: 0.031915, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:38:21] Epoch 7/50, Loss: 24.265049, Train_MMSE: 0.035544, NMMSE: 0.031411, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:39:49] Epoch 8/50, Loss: 24.237146, Train_MMSE: 0.035092, NMMSE: 0.031168, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:41:14] Epoch 9/50, Loss: 24.144146, Train_MMSE: 0.03467, NMMSE: 0.031057, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:42:43] Epoch 10/50, Loss: 24.016764, Train_MMSE: 0.034399, NMMSE: 0.030793, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:44:15] Epoch 11/50, Loss: 24.015623, Train_MMSE: 0.0342, NMMSE: 0.030715, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:45:49] Epoch 12/50, Loss: 23.818005, Train_MMSE: 0.03404, NMMSE: 0.030549, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:47:18] Epoch 13/50, Loss: 23.667305, Train_MMSE: 0.033893, NMMSE: 0.030453, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:48:43] Epoch 14/50, Loss: 23.702629, Train_MMSE: 0.03376, NMMSE: 0.030364, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:50:13] Epoch 15/50, Loss: 23.729210, Train_MMSE: 0.033644, NMMSE: 0.030418, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:51:40] Epoch 16/50, Loss: 23.705240, Train_MMSE: 0.033512, NMMSE: 0.030418, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:53:14] Epoch 17/50, Loss: 23.604881, Train_MMSE: 0.033421, NMMSE: 0.030338, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:54:42] Epoch 18/50, Loss: 23.534359, Train_MMSE: 0.033329, NMMSE: 0.030325, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:56:08] Epoch 19/50, Loss: 23.524261, Train_MMSE: 0.033245, NMMSE: 0.030277, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:57:38] Epoch 20/50, Loss: 23.746094, Train_MMSE: 0.033154, NMMSE: 0.030264, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 13:59:06] Epoch 21/50, Loss: 23.631420, Train_MMSE: 0.033065, NMMSE: 0.030205, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:00:36] Epoch 22/50, Loss: 23.579643, Train_MMSE: 0.033015, NMMSE: 0.03021, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:02:05] Epoch 23/50, Loss: 23.451754, Train_MMSE: 0.032942, NMMSE: 0.030476, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:03:33] Epoch 24/50, Loss: 23.387953, Train_MMSE: 0.032882, NMMSE: 0.03035, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:05:03] Epoch 25/50, Loss: 23.628231, Train_MMSE: 0.032818, NMMSE: 0.030184, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:06:30] Epoch 26/50, Loss: 23.414158, Train_MMSE: 0.032729, NMMSE: 0.030201, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:07:59] Epoch 27/50, Loss: 23.277365, Train_MMSE: 0.032681, NMMSE: 0.030192, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:09:28] Epoch 28/50, Loss: 23.336451, Train_MMSE: 0.032628, NMMSE: 0.030212, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:10:56] Epoch 29/50, Loss: 23.405600, Train_MMSE: 0.03256, NMMSE: 0.030301, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:12:25] Epoch 30/50, Loss: 23.109922, Train_MMSE: 0.032496, NMMSE: 0.030331, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:13:49] Epoch 31/50, Loss: 23.100060, Train_MMSE: 0.032453, NMMSE: 0.030437, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:15:22] Epoch 32/50, Loss: 23.132700, Train_MMSE: 0.032406, NMMSE: 0.030348, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:16:51] Epoch 33/50, Loss: 23.413507, Train_MMSE: 0.032332, NMMSE: 0.030374, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:18:20] Epoch 34/50, Loss: 23.151423, Train_MMSE: 0.032269, NMMSE: 0.030426, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:19:50] Epoch 35/50, Loss: 23.162020, Train_MMSE: 0.03222, NMMSE: 0.030331, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:21:15] Epoch 36/50, Loss: 23.242847, Train_MMSE: 0.032161, NMMSE: 0.030436, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:22:41] Epoch 37/50, Loss: 23.306208, Train_MMSE: 0.032093, NMMSE: 0.030357, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:24:10] Epoch 38/50, Loss: 23.319790, Train_MMSE: 0.032056, NMMSE: 0.030466, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:25:39] Epoch 39/50, Loss: 23.189259, Train_MMSE: 0.032009, NMMSE: 0.030383, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:27:08] Epoch 40/50, Loss: 23.044434, Train_MMSE: 0.031939, NMMSE: 0.030507, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:28:37] Epoch 41/50, Loss: 22.819071, Train_MMSE: 0.031904, NMMSE: 0.030523, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:30:03] Epoch 42/50, Loss: 23.260946, Train_MMSE: 0.031838, NMMSE: 0.030545, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:31:29] Epoch 43/50, Loss: 23.127836, Train_MMSE: 0.031772, NMMSE: 0.030672, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:32:47] Epoch 44/50, Loss: 23.343672, Train_MMSE: 0.031738, NMMSE: 0.03062, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:34:01] Epoch 45/50, Loss: 22.993052, Train_MMSE: 0.031681, NMMSE: 0.03057, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:35:17] Epoch 46/50, Loss: 23.084080, Train_MMSE: 0.031651, NMMSE: 0.030781, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:36:33] Epoch 47/50, Loss: 22.993870, Train_MMSE: 0.031591, NMMSE: 0.030716, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:37:47] Epoch 48/50, Loss: 22.790113, Train_MMSE: 0.031533, NMMSE: 0.030794, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:39:02] Epoch 49/50, Loss: 22.923512, Train_MMSE: 0.03149, NMMSE: 0.0307, LS_NMSE: 0.040619, Lr: 0.001
[2025-02-18 14:40:20] Epoch 50/50, Loss: 22.691389, Train_MMSE: 0.031429, NMMSE: 0.030888, LS_NMSE: 0.040619, Lr: 0.0001
